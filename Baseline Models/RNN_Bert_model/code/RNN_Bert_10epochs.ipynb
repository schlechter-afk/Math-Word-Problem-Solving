{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9525411,"sourceType":"datasetVersion","datasetId":5800301},{"sourceId":9530204,"sourceType":"datasetVersion","datasetId":5803771}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch_pretrained_bert","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:45:46.887229Z","iopub.execute_input":"2024-11-13T07:45:46.888054Z","iopub.status.idle":"2024-11-13T07:45:58.652596Z","shell.execute_reply.started":"2024-11-13T07:45:46.888012Z","shell.execute_reply":"2024-11-13T07:45:58.651366Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch_pretrained_bert in /opt/conda/lib/python3.10/site-packages (0.6.2)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from pytorch_pretrained_bert) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch_pretrained_bert) (1.26.4)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from pytorch_pretrained_bert) (1.26.100)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pytorch_pretrained_bert) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch_pretrained_bert) (4.66.4)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from pytorch_pretrained_bert) (2024.5.15)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2024.6.1)\nRequirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch_pretrained_bert) (1.29.165)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch_pretrained_bert) (0.6.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch_pretrained_bert) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch_pretrained_bert) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch_pretrained_bert) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch_pretrained_bert) (2024.8.30)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->pytorch_pretrained_bert) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3->pytorch_pretrained_bert) (1.16.0)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport math\nimport logging\nimport pdb\nimport json\nimport random\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport collections\nimport argparse\nfrom glob import glob\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom tensorboardX import SummaryWriter\nfrom gensim import models\nfrom collections import OrderedDict\nfrom transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer, AdamW\n\nfrom sympy import Eq, solve\nfrom sympy.parsing.sympy_parser import parse_expr\nimport sympy as sp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# from attrdict import AttrDict\nimport unicodedata\ntry:\n\timport cPickle as pickle\nexcept ImportError:\n\timport pickle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:45:58.654615Z","iopub.execute_input":"2024-11-13T07:45:58.654954Z","iopub.status.idle":"2024-11-13T07:45:58.665527Z","shell.execute_reply.started":"2024-11-13T07:45:58.654919Z","shell.execute_reply":"2024-11-13T07:45:58.664462Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Components","metadata":{}},{"cell_type":"code","source":"##################################################\n# Attention.py #\n##################################################\n\n# Luong attention layer\nclass Attn(nn.Module):\n\tdef __init__(self, method, hidden_size):\n\t\tsuper(Attn, self).__init__()\n\t\tself.method = method\n\t\tif self.method not in ['dot', 'general', 'concat']:\n\t\t\traise ValueError(self.method, \"is not an appropriate attention method.\")\n\t\tself.hidden_size = hidden_size\n\t\tif self.method == 'general':\n\t\t\tself.attn = nn.Linear(self.hidden_size, hidden_size)\n\t\telif self.method == 'concat':\n\t\t\tself.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n\t\t\tself.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n\n\tdef dot_score(self, hidden, encoder_outputs):\n\t\treturn torch.sum(hidden * encoder_outputs, dim=2)\n\n\tdef general_score(self, hidden, encoder_outputs):\n\t\tenergy = self.attn(encoder_outputs)\n\t\treturn torch.sum(hidden * energy, dim=2)\n\n\tdef concat_score(self, hidden, encoder_outputs):\n\t\tenergy = self.attn(torch.cat((hidden.expand(encoder_outputs.size(0), -1, -1), encoder_outputs), 2)).tanh()\n\t\treturn torch.sum(self.v * energy, dim=2)\n\n\tdef forward(self, hidden, encoder_outputs):\n\t\t# Calculate the attention weights (energies) based on the given method\n\t\tif self.method == 'general':\n\t\t\tattn_energies = self.general_score(hidden, encoder_outputs)\n\t\telif self.method == 'concat':\n\t\t\tattn_energies = self.concat_score(hidden, encoder_outputs)\n\t\telif self.method == 'dot':\n\t\t\tattn_energies = self.dot_score(hidden, encoder_outputs)\n\n\t\t# Transpose max_length and batch_size dimensions\n\t\tattn_energies = attn_energies.t()\n\n\t\t# Return the softmax normalized probability scores (with added dimension)\n\t\treturn F.softmax(attn_energies, dim=1).unsqueeze(1)\n\nclass LuongAttnDecoderRNN(nn.Module):\n\tdef __init__(self, attn_model, embedding, cell_type, hidden_size, output_size, nlayers=1, dropout=0.1):\n\t\tsuper(LuongAttnDecoderRNN, self).__init__()\n\n\t\t# Keep for reference\n\t\tself.attn_model \t= attn_model\n\t\tself.hidden_size \t= hidden_size\n\t\tself.output_size \t= output_size\n\t\tself.nlayers \t\t= nlayers\n\t\tself.dropout \t\t= dropout\n\t\tself.cell_type \t\t= cell_type\n\n\t\t# Define layers\n\t\tself.embedding = embedding\n\t\tself.embedding_size  = self.embedding.embedding_dim\n\t\tself.embedding_dropout = nn.Dropout(self.dropout)\n\t\tif self.cell_type == 'gru':\n\t\t\tself.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.nlayers, dropout=(0 if self.nlayers == 1 else self.dropout))\n\t\telse:\n\t\t\tself.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.nlayers, dropout=(0 if self.nlayers == 1 else self.dropout))\n\t\tself.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n\t\tself.out = nn.Linear(self.hidden_size, self.output_size)\n\n\t\tself.attn = Attn(self.attn_model, self.hidden_size)\n\n\tdef forward(self, input_step, last_hidden, encoder_outputs):\n\t\t# Note: we run this one step (word) at a time\n\t\t# Get embedding of current input word\n\t\tembedded = self.embedding(input_step)\n\t\tembedded = self.embedding_dropout(embedded)\n\n\t\ttry:\n\t\t\tembedded = embedded.view(1, input_step.size(0), self.embedding_size)\n\t\texcept:\n\t\t\tembedded = embedded.view(1, 1, self.embedding_size)\n\n\t\trnn_output, hidden = self.rnn(embedded, last_hidden)\n\t\t# Calculate attention weights from the current GRU output\n\t\tattn_weights = self.attn(rnn_output, encoder_outputs)\n\t\t# Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n\t\tcontext = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n\t\t# Concatenate weighted context vector and GRU output using Luong eq. 5\n\t\trnn_output = rnn_output.squeeze(0)\n\t\tcontext = context.squeeze(1)\n\t\tconcat_input = torch.cat((rnn_output, context), 1)\n\t\tconcat_output = F.relu(self.concat(concat_input))\n\t\trepresentation = concat_output\n\t\t# Predict next word using Luong eq. 6\n\t\toutput = self.out(concat_output)\n\t\toutput = F.log_softmax(output, dim=1)\n\t\t# Return output and final hidden state\n\t\treturn output, hidden, attn_weights, representation\n    \n    \n##################################################\n# Contextual embeddings.py #\n##################################################\n\nclass BertEncoder(nn.Module):\n\tdef __init__(self, bert_model = 'bert-base-uncased',device = 'cuda:0 ', freeze_bert = False):\n\t\tsuper(BertEncoder, self).__init__()\n\t\tself.bert_layer = BertModel.from_pretrained(bert_model)\n\t\tself.bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n\t\tself.device = device\n\t\t\n\t\tif freeze_bert:\n\t\t\tfor p in self.bert_layer.parameters():\n\t\t\t\tp.requires_grad = False\n\t\t\n\tdef bertify_input(self, sentences):\n\t\t'''\n\t\tPreprocess the input sentences using bert tokenizer and converts them to a torch tensor containing token ids\n\n\t\t'''\n\t\t#Tokenize the input sentences for feeding into BERT\n\t\tall_tokens  = [['[CLS]'] + self.bert_tokenizer.tokenize(sentence) + ['[SEP]'] for sentence in sentences]\n\t\t\n\t\t#Pad all the sentences to a maximum length\n\t\tinput_lengths = [len(tokens) for tokens in all_tokens]\n\t\tmax_length    = max(input_lengths)\n\t\tpadded_tokens = [tokens + ['[PAD]' for _ in range(max_length - len(tokens))] for tokens in all_tokens]\n\n\t\t#Convert tokens to token ids\n\t\ttoken_ids = torch.tensor([self.bert_tokenizer.convert_tokens_to_ids(tokens) for tokens in padded_tokens]).to(self.device)\n\n\t\t#Obtain attention masks\n\t\tpad_token = self.bert_tokenizer.convert_tokens_to_ids('[PAD]')\n\t\tattn_masks = (token_ids != pad_token).long()\n\n\t\treturn token_ids, attn_masks, input_lengths\n\n\t# def forward(self, sentences):\n\t# \t'''\n\t# \tFeed the batch of sentences to a BERT encoder to obtain contextualized representations of each token\n\t# \t'''\n\t# \t#Preprocess sentences\n\t# \ttoken_ids, attn_masks, input_lengths = self.bertify_input(sentences)\n\n\t# \t#Feed through bert\n\t# \tcont_reps, _ = self.bert_layer(token_ids, attention_mask = attn_masks)\n\n\t# \treturn cont_reps, input_lengths\n\n\tdef forward(self, sentences):\n\t\t'''\n\t\tFeed the batch of sentences to a BERT encoder to obtain contextualized representations of each token\n\t\t'''\n\t\t#Preprocess sentences\n\t\ttoken_ids, attn_masks, input_lengths = self.bertify_input(sentences)\n\n\t\t#Feed through bert\n\t\t# cont_reps, _ = self.bert_layer(token_ids, attention_mask = attn_masks)\n\t\toutput = self.bert_layer(token_ids, attention_mask = attn_masks)\n\t\tcont_reps = output.last_hidden_state\n\n\t\treturn cont_reps, input_lengths\n\nclass RobertaEncoder(nn.Module):\n\tdef __init__(self, roberta_model = 'roberta-base', device = 'cuda:0 ', freeze_roberta = False):\n\t\tsuper(RobertaEncoder, self).__init__()\n\t\tself.roberta_layer = RobertaModel.from_pretrained(roberta_model)\n\t\tself.roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model)\n\t\tself.device = device\n\t\t\n\t\tif freeze_roberta:\n\t\t\tfor p in self.roberta_layer.parameters():\n\t\t\t\tp.requires_grad = False\n\t\t\n\tdef robertify_input(self, sentences):\n\t\t'''\n\t\tPreprocess the input sentences using roberta tokenizer and converts them to a torch tensor containing token ids\n\n\t\t'''\n\t\t# Tokenize the input sentences for feeding into RoBERTa\n\t\tall_tokens  = [['<s>'] + self.roberta_tokenizer.tokenize(sentence) + ['</s>'] for sentence in sentences]\n\t\t\n\t\t# Pad all the sentences to a maximum length\n\t\tinput_lengths = [len(tokens) for tokens in all_tokens]\n\t\tmax_length    = max(input_lengths)\n\t\tpadded_tokens = [tokens + ['<pad>' for _ in range(max_length - len(tokens))] for tokens in all_tokens]\n\n\t\t# Convert tokens to token ids\n\t\ttoken_ids = torch.tensor([self.roberta_tokenizer.convert_tokens_to_ids(tokens) for tokens in padded_tokens]).to(self.device)\n\n\t\t# Obtain attention masks\n\t\tpad_token = self.roberta_tokenizer.convert_tokens_to_ids('<pad>')\n\t\tattn_masks = (token_ids != pad_token).long()\n\n\t\treturn token_ids, attn_masks, input_lengths\n\n\tdef forward(self, sentences):\n\t\t'''\n\t\tFeed the batch of sentences to a RoBERTa encoder to obtain contextualized representations of each token\n\t\t'''\n\t\t# Preprocess sentences\n\t\ttoken_ids, attn_masks, input_lengths = self.robertify_input(sentences)\n\n\t\t# Feed through RoBERTa\n\t\toutput = self.roberta_layer(token_ids, attention_mask = attn_masks)\n        \n\t\tcont_reps = output.last_hidden_state\n\n\t\treturn cont_reps, input_lengths\n    \n##################################################\n# Decoder.py #\n##################################################\n\nclass DecoderRNN(nn.Module):\n\t'''\n\tTo DO\n\tEncoder helps in building the sentence encoding module for a batched version\n\tof data that is sent in [T x B] having corresponding input lengths in [1 x B]\n\n\tArgs:\n\t\t\thidden_size: Hidden size of the RNN cell\n\t\t\tembedding: Embeddings matrix [vocab_size, embedding_dim]\n\t\t\tcell_type: Type of RNN cell to be used : LSTM, GRU\n\t\t\tnlayers: Number of layers of LSTM (default = 1)\n\t\t\tdropout: Dropout Rate (default = 0.1)\n\t\t\tbidirectional: Bidirectional model to be formed (default: False)\n\t'''\n\tdef __init__(self, embedding, cell_type, hidden_size, output_size, nlayers=1, dropout=0.2):\n\t\tsuper(DecoderRNN, self).__init__()\n\t\tself.hidden_size        = hidden_size\n\t\tself.cell_type          = cell_type\n\t\tself.embedding          = embedding\n\t\tself.embedding_size     = self.embedding.embedding_dim\n\t\tself.embedding_dropout = nn.Dropout(dropout)\n\t\tself.nlayers            = nlayers\n\t\tself.output_size        = output_size\n\n\t\tif self.cell_type == 'lstm':\n\t\t\tself.rnn = nn.LSTM(self.embedding_size, self.hidden_size, num_layers=self.nlayers, dropout=(0 if nlayers == 1 else dropout))\n\t\telse:\n\t\t\tself.rnn = nn.GRU(self.embedding_size, self.hidden_size, num_layers=self.nlayers, dropout=(0 if nlayers == 1 else dropout))\n\n\t\tself.out     = nn.Linear(self.hidden_size, self.output_size)\n\n\n\tdef forward(self, input_step, last_hidden):\n\t\t'''\n\t\tTo Do\n\t\t\tArgs:\n\t\t\t\tinput_seqs (tensor) : input tensor | size : [Seq_len X Batch_size]\n\t\t\t\tinput_lengths (list/tensor) : length of each input sentence | size : [Batch_size] \n\t\t\t\tdevice (gpu) : Used for sorting the sentences and putting it to device\n\n\t\t\tReturns:\n\t\t\t\toutput (tensor) : Last State representations of RNN [Seq_len X Batch_size X hidden_size]\n\t\t\t\thidden (tuple)\t: Hidden states and (cell states) of recurrent networks\n\t\t'''\n\t\toutput              = self.embedding(input_step)\n\t\toutput              = self.embedding_dropout(output)\n\t\toutput              = output.view(1, input_step.size(0), self.embedding_size)\n\t\toutput              = F.relu(output)\n\t\toutput, last_hidden = self.rnn(output, last_hidden)\n\t\toutput              = output.squeeze(0)\n\t\toutput              = self.out(output)\n\t\toutput              = F.log_softmax(output, dim=1)\n\n\t\treturn output, last_hidden\n\n##################################################\n# Encoder.py #\n##################################################\n\nclass Encoder(nn.Module):\n\t'''\n\tEncoder helps in building the sentence encoding module for a batched version\n\tof data that is sent in [T x B] having corresponding input lengths in [1 x B]\n\n\tArgs:\n\t\t\thidden_size: Hidden size of the RNN cell\n\t\t\tembedding: Embeddings matrix [vocab_size, embedding_dim]\n\t\t\tcell_type: Type of RNN cell to be used : LSTM, GRU\n\t\t\tnlayers: Number of layers of LSTM (default = 1)\n\t\t\tdropout: Dropout Rate (default = 0.1)\n\t\t\tbidirectional: Bidirectional model to be formed (default: False)\n\t'''\n\n\tdef __init__(self, hidden_size=512,embedding_size = 768, cell_type='lstm', nlayers=1, dropout=0.1, bidirectional=True):\n\t\tsuper(Encoder, self).__init__()\n\t\tself.hidden_size = hidden_size\n\t\tself.nlayers = nlayers\n\t\tself.dropout = dropout\n\t\tself.cell_type = cell_type\n\t\tself.embedding_size = embedding_size\n\t\t# self.embedding_size = self.embedding.embedding_dim\n\t\tself.bidirectional = bidirectional\n\n\t\tif self.cell_type == 'lstm':\n\t\t\tself.rnn = nn.LSTM(self.embedding_size, self.hidden_size,\n\t\t\t\t\t\t\t   num_layers=self.nlayers,\n\t\t\t\t\t\t\t   dropout=(0 if self.nlayers == 1 else dropout),\n\t\t\t\t\t\t\t   bidirectional=bidirectional)\n\t\telif self.cell_type == 'gru':\n\t\t\tself.rnn = nn.GRU(self.embedding_size, self.hidden_size,\n\t\t\t\t\t\t\t  num_layers=self.nlayers,\n\t\t\t\t\t\t\t  dropout=(0 if self.nlayers == 1 else dropout),\n\t\t\t\t\t\t\t  bidirectional=bidirectional)\n\t\telse:\n\t\t\tself.rnn = nn.RNN(self.embedding_size, self.hidden_size,\n\t\t\t\t\t\t\t  num_layers=self.nlayers,\n\t\t\t\t\t\t\t  nonlinearity='tanh',\t\t\t\t\t\t\t# ['relu', 'tanh']\n\t\t\t\t\t\t\t  dropout=(0 if self.nlayers == 1 else dropout),\n\t\t\t\t\t\t\t  bidirectional=bidirectional)\n\n\tdef forward(self, sorted_seqs, sorted_len, orig_idx, device=None, hidden=None):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tinput_seqs (tensor) : input tensor | size : [Seq_len X Batch_size]\n\t\t\t\tinput_lengths (list/tensor) : length of each input sentence | size : [Batch_size] \n\t\t\t\tdevice (gpu) : Used for sorting the sentences and putting it to device\n\n\t\t\tReturns:\n\t\t\t\toutput (tensor) : Last State representations of RNN [Seq_len X Batch_size X hidden_size]\n\t\t\t\thidden (tuple)\t: Hidden states and (cell states) of recurrent networks\n\t\t'''\n\n\t\t# sorted_seqs, sorted_len, orig_idx = sort_by_len(input_seqs, input_lengths, device)\n\t\t# pdb.set_trace()\n\n\t\t#embedded = self.embedding(sorted_seqs)  ### NO MORE IDS\n\t\tpacked = torch.nn.utils.rnn.pack_padded_sequence(\n\t\t\tsorted_seqs, sorted_len)\n\t\toutputs, hidden = self.rnn(packed, hidden)\n\t\toutputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(\n\t\t\toutputs)  # unpack (back to padded)\n\n\t\toutputs = outputs.index_select(1, orig_idx)\n\n\t\tif self.bidirectional:\n\t\t\toutputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n\n\t\treturn outputs, hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:49:27.637299Z","iopub.execute_input":"2024-11-13T07:49:27.637720Z","iopub.status.idle":"2024-11-13T07:49:27.688796Z","shell.execute_reply.started":"2024-11-13T07:49:27.637681Z","shell.execute_reply":"2024-11-13T07:49:27.687926Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## args.py","metadata":{}},{"cell_type":"code","source":"def build_parser():\n\t# Data loading parameters\n\tparser = argparse.ArgumentParser(description='Run Single sequence model')\n\n\t# Mode specifications\n\tparser.add_argument('-mode', type=str, default='train', choices=['train', 'test', 'conf'], help='Modes: train, test, conf')\n\tparser.add_argument('-debug', dest='debug', action='store_true', help='Operate in debug mode')\n\tparser.add_argument('-no-debug', dest='debug', action='store_false', help='Operate in normal mode')\n\tparser.set_defaults(debug=False)\n\n\t# Run Config\n\tparser.add_argument('-run_name', type=str, default='debug', help='run name for logs')\n\tparser.add_argument('-dataset', type=str, default='mawps', help='Dataset')\n\tparser.add_argument('-display_freq', type=int, default= 10000, help='number of batches after which to display samples')\n\tparser.add_argument('-outputs', dest='outputs', action='store_true', help='Show full validation outputs')\n\tparser.add_argument('-no-outputs', dest='outputs', action='store_false', help='Do not show full validation outputs')\n\tparser.set_defaults(outputs=True)\n\tparser.add_argument('-results', dest='results', action='store_true', help='Store results')\n\tparser.add_argument('-no-results', dest='results', action='store_false', help='Do not store results')\n\tparser.set_defaults(results=True)\n\n\t# Meta Attributes\n\tparser.add_argument('-vocab_size', type=int, default=30000, help='Vocabulary size to consider')\n\tparser.add_argument('-histogram', dest='histogram', action='store_true', help='Operate in debug mode')\n\tparser.add_argument('-no-histogram', dest='histogram', action='store_false', help='Operate in normal mode')\n\tparser.set_defaults(histogram=True)\n\tparser.add_argument('-save_writer', dest='save_writer',action='store_true', help='To write tensorboard')\n\tparser.add_argument('-no-save_writer', dest='save_writer', action='store_false', help='Dont write tensorboard')\n\tparser.set_defaults(save_writer=False)\n\n\t# Device Configuration\n\tparser.add_argument('-gpu', type=int, default=2, help='Specify the gpu to use')\n\tparser.add_argument('-early_stopping', type=int, default=50, help='Early Stopping after n epoch')\n\tparser.add_argument('-seed', type=int, default=6174, help='Default seed to set')\n\tparser.add_argument('-logging', type=int, default=1, help='Set to 0 if you do not require logging')\n\tparser.add_argument('-ckpt', type=str, default='model', help='Checkpoint file name')\n\tparser.add_argument('-save_model', dest='save_model',action='store_true', help='To save the model')\n\tparser.add_argument('-no-save_model', dest='save_model', action='store_false', help='Dont save the model')\n\tparser.set_defaults(save_model=False)\n\t# parser.add_argument('-log_fmt', type=str, default='%(asctime)s | %(levelname)s | %(name)s | %(message)s', help='Specify format of the logger')\n\n\t# LSTM parameters\n\tparser.add_argument('-emb2_size', type=int, default=16, help='Embedding dimensions of inputs')\n\tparser.add_argument('-cell_type', type=str, default='lstm', help='RNN cell for encoder and decoder, default: lstm')\n\n\tparser.add_argument('-use_attn', dest='use_attn',action='store_true', help='To use attention mechanism?')\n\tparser.add_argument('-no-attn', dest='use_attn', action='store_false', help='Not to use attention mechanism?')\n\tparser.set_defaults(use_attn=True)\n\n\tparser.add_argument('-attn_type', type=str, default='general', help='Attention mechanism: (general, concat), default: general')\n\tparser.add_argument('-hidden_size', type=int, default=256, help='Number of hidden units in each layer')\n\tparser.add_argument('-depth', type=int, default=1, help='Number of layers in each encoder and decoder')\n\tparser.add_argument('-dropout', type=float, default=0.1, help= 'Dropout probability for input/output/state units (0.0: no dropout)')\n\tparser.add_argument('-max_length', type=int, default=100, help='Specify max decode steps: Max length string to output')\n\tparser.add_argument('-init_range', type=float, default=0.08, help='Initialization range for seq2seq model')\n\tparser.add_argument('-bidirectional', dest='bidirectional', action='store_true', help='Bidirectionality in LSTMs')\n\tparser.add_argument('-no-bidirectional', dest='bidirectional', action='store_false', help='Bidirectionality in LSTMs')\n\tparser.set_defaults(bidirectional=True)\n\tparser.add_argument('-lr', type=float, default=0.0005, help='Learning rate')\n\t# parser.add_argument('-bert_lr', type=float, default=5e-5, help='Larning rate to train BERT embeddings')\n\tparser.add_argument('-warmup', type=float, default=0.1, help='Proportion of training to perform linear learning rate warmup for')\n\tparser.add_argument('-max_grad_norm', type=float, default=0.25, help='Clip gradients to this norm')\n\tparser.add_argument('-batch_size', type=int, default=8, help='Batch size')\n\tparser.add_argument('-epochs', type=int, default=50, help='Maximum # of training epochs')\n\tparser.add_argument('-opt', type=str, default='adam', choices=['adam', 'adadelta', 'sgd', 'asgd'], help='Optimizer for training')\n\tparser.add_argument('-separate_opt', dest='separate_opt', action='store_true', help='Separate Optimizers for Embedding and model - AdamW for emb and Adam for model')\n\tparser.add_argument('-no-separate_opt', dest='separate_opt', action='store_false', help='Common optimizer for Embedding and model')\n\tparser.set_defaults(separate_opt=False)\n\tparser.add_argument('-teacher_forcing_ratio', type=float, default=0.9, help='Teacher forcing ratio')\n\n\t# Embeddings\n\tparser.add_argument('-embedding', type=str, default='roberta', choices=['bert', 'roberta', 'word2vec', 'random'], help='Embeddings')\n\t# parser.add_argument('-use_word2vec', dest='use_word2vec', action='store_true', help='use word2vec')\n\t# parser.add_argument('-no-use_word2vec', dest='use_word2vec', action='store_false', help='Do not use word2vec')\n\t# parser.set_defaults(use_word2vec=False)\n\t# parser.add_argument('-word2vec_bin', type=str, default='/datadrive/satwik/global_data/glove.840B.300d.txt', help='Binary file of word2vec')\n\tparser.add_argument('-word2vec_bin', type=str, default='/datadrive/global_files/GoogleNews-vectors-negative300.bin', help='Binary file of word2vec')\n\t# parser.add_argument('-train_word2vec', dest='train_word2vec', action='store_true', help='train word2vec')\n\t# parser.add_argument('-no-train_word2vec', dest='train_word2vec', action='store_false', help='Do not train word2vec')\n\t# parser.set_defaults(train_word2vec=True)\n\tparser.add_argument('-emb1_size', type=int, default=768, help='Embedding dimensions of inputs')\n\tparser.add_argument('-emb_name', type=str, default='roberta-base', choices=['bert-base-uncased', 'roberta-base'], help='Which pre-trained model')\n\t# parser.add_argument('-bert_size', type=int, default = 768, help = 'Size of BERT\\'s last layer representations')\n\tparser.add_argument('-emb_lr', type=float, default=1e-5, help='Larning rate to train embeddings')\n\tparser.add_argument('-freeze_emb', dest='freeze_emb', action='store_true', help='Freeze embedding weights')\n\tparser.add_argument('-no-freeze_emb', dest='freeze_emb', action='store_false', help='Train embedding weights')\n\tparser.set_defaults(freeze_emb=False)\n\n\tparser.add_argument('-grade_disp', dest='grade_disp', action='store_true', help='Display grade information in validation outputs')\n\tparser.add_argument('-no-grade_disp', dest='grade_disp', action='store_false', help='Don\\'t display grade information')\n\tparser.set_defaults(grade_disp=False)\n\tparser.add_argument('-type_disp', dest='type_disp', action='store_true', help='Display Type information in validation outputs')\n\tparser.add_argument('-no-type_disp', dest='type_disp', action='store_false', help='Don\\'t display Type information')\n\tparser.set_defaults(type_disp=False)\n\tparser.add_argument('-challenge_disp', dest='challenge_disp', action='store_true', help='Display information in validation outputs')\n\tparser.add_argument('-no-challenge_disp', dest='challenge_disp', action='store_false', help='Don\\'t display information')\n\tparser.set_defaults(challenge_disp=False)\n\tparser.add_argument('-nums_disp', dest='nums_disp', action='store_true', help='Display number of numbers information in validation outputs')\n\tparser.add_argument('-no-nums_disp', dest='nums_disp', action='store_false', help='Don\\'t display number of numbers information')\n\tparser.set_defaults(nums_disp=True)\n\tparser.add_argument('-more_nums', dest='more_nums', action='store_true', help='More numbers in Voc2')\n\tparser.add_argument('-no-more_nums', dest='more_nums', action='store_false', help='Usual numbers in Voc2')\n\tparser.set_defaults(more_nums=False)\n\tparser.add_argument('-mawps_vocab', dest='mawps_vocab', action='store_true', help='Custom Numbers in Voc2')\n\tparser.add_argument('-no-mawps_vocab', dest='mawps_vocab', action='store_false', help='No Custom Numbers in Voc2')\n\tparser.set_defaults(mawps_vocab=False)\n\n\tparser.add_argument('-show_train_acc', dest='show_train_acc', action='store_true', help='Calculate the train accuracy')\n\tparser.add_argument('-no-show_train_acc', dest='show_train_acc', action='store_false', help='Don\\'t calculate the train accuracy')\n\tparser.set_defaults(show_train_acc=True)\n\n\tparser.add_argument('-full_cv', dest='full_cv', action='store_true', help='5-fold CV')\n\tparser.add_argument('-no-full_cv', dest='full_cv', action='store_false', help='No 5-fold CV')\n\tparser.set_defaults(full_cv=False)\n\n\t#Conf parameters\n\tparser.add_argument('-conf', type = str, default = 'posterior', choices = [\"posterior\", \"similarity\"], help = 'Confidence estimation criteria to use, [\"posterior\", \"similarity\"]')\n\tparser.add_argument('-sim_criteria', type = str, default = 'bleu', choices = ['bert_score', 'bleu_score'], help = 'Only applicable if similarity based criteria is selected for confidence.')\n\tparser.add_argument('-adv', action = 'store_true', help = 'If dealing with out of distribution examples')\n\t\n\treturn parser\n\ndef parse_arguments(arg_dict=None):\n    parser = build_parser()\n    if arg_dict:\n        # Override default values with provided dictionary values\n        args = parser.parse_args([])\n        for key, value in arg_dict.items():\n            setattr(args, key, value)\n        return args\n    else:\n        return parser.parse_args()  # If no dictionary is provided, use default command line arguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:49:27.690905Z","iopub.execute_input":"2024-11-13T07:49:27.691253Z","iopub.status.idle":"2024-11-13T07:49:27.729415Z","shell.execute_reply.started":"2024-11-13T07:49:27.691216Z","shell.execute_reply":"2024-11-13T07:49:27.728567Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Utilities Folder","metadata":{}},{"cell_type":"code","source":"##################################################\n# Bleu.py #\n##################################################\n\"\"\"Python implementation of BLEU and smooth-BLEU.\nThis module provides a Python implementation of BLEU and smooth-BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\nChin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\nevaluation metrics for machine translation. COLING 2004.\n\"\"\"\n\ndef _get_ngrams(segment, max_order):\n    ngram_counts = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i+order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 smooth=False):\n \n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n    reference_length = 0\n    translation_length = 0\n    for (references, translation) in zip(reference_corpus,\n                                       translation_corpus):\n        reference_length += min(len(r) for r in references)\n        translation_length += len(translation)\n\n        merged_ref_ngram_counts = collections.Counter()\n        for reference in references:\n            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n        translation_ngram_counts = _get_ngrams(translation, max_order)\n        overlap = translation_ngram_counts & merged_ref_ngram_counts\n        for ngram in overlap:\n            matches_by_order[len(ngram)-1] += overlap[ngram]\n        for order in range(1, max_order+1):\n            possible_matches = len(translation) - order + 1\n            if possible_matches > 0:\n                possible_matches_by_order[order-1] += possible_matches\n\n    precisions = [0] * max_order\n    for i in range(0, max_order):\n        if smooth:\n            precisions[i] = ((matches_by_order[i] + 1.) /\n                           (possible_matches_by_order[i] + 1.))\n        else:\n            if possible_matches_by_order[i] > 0:\n                precisions[i] = (float(matches_by_order[i]) /\n                             possible_matches_by_order[i])\n            else:\n                precisions[i] = 0.0\n\n    if min(precisions) > 0:\n        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n        geo_mean = math.exp(p_log_sum)\n    else:\n        geo_mean = 0\n\n    ratio = float(translation_length) / reference_length\n\n    if ratio > 1.0:\n        bp = 1.\n    else:\n        if ratio > 1E-1:\n            bp = math.exp(1 - 1. / ratio)\n        else:\n            bp = 1E-2\n\n    bleu = geo_mean * bp\n\n    return (bleu, precisions, bp, ratio, translation_length, reference_length)\n\n##################################################\n# eq_preprocessing.py #\n##################################################\n\n\nOPS = ['+', '-', '*', '/']\n\nclass Node():\n    def __init__(self, val):\n        self.val    = val\n        self.left   = None\n        self.right  = None\n\n\ndef preorder(node, prefix = ''):\n    if node is None:\n        return prefix\n    val = node.val\n    prefix += val +' '\n    prefix = preorder(node.left, prefix)\n    prefix = preorder(node.right, prefix)\n    return prefix\n\ndef expr2tree(string):\n    tokens = string.split()\n    if len(tokens) == 1:\n        return Node(tokens[0])\n    i = 0\n    while i < len(tokens):\n        if tokens[i] in OPS:\n            break\n        i += 1\n\n    node = Node(tokens[i])\n    node.left  = expr2tree(' '.join(tokens[:i]))\n    node.right = expr2tree(' '.join(tokens[i+1:])) \n    return node\n\ndef infix2prefix(equation):\n    tree_root = expr2tree(equation)\n    prefix = preorder(tree_root, '')\n    return prefix\n\n# if __name__ == \"__main__\":\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('-eqn', required=True, type = str)\n#     args = parser.parse_args()\n\n#     print(infix2prefix(args.eqn))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:49:27.730803Z","iopub.execute_input":"2024-11-13T07:49:27.731117Z","iopub.status.idle":"2024-11-13T07:49:27.751241Z","shell.execute_reply.started":"2024-11-13T07:49:27.731083Z","shell.execute_reply":"2024-11-13T07:49:27.750306Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"##################################################\n# Evaluate.py #\n##################################################\n\n\"\"\"\nEXAMPLE:\nprefix1 = '* n0 + + n1 n2 n3'\nlist_num1 = [13, 9, 10, 3] n0->13, n1->9, n2->10, n3->3\nprint(ans_evaluator(prefix1, list_num1))\nanswer: 286\n\"\"\"\n\ndef format_eq(eq):\n\tfin_eq = \"\"\n\tls = ['0','1','2','3','4','5','6','7','8','9','.']\n\ttemp_num = \"\"\n\tflag = 0\n\tfor i in eq:\n\t\tif flag > 0:\n\t\t\tfin_eq = fin_eq + i\n\t\t\tflag = flag-1\n\t\telif i == 'n':\n\t\t\tflag = 6\n\t\t\tif fin_eq == \"\":\n\t\t\t\tfin_eq = fin_eq + i\n\t\t\telse:\n\t\t\t\tfin_eq = fin_eq + ' ' + i\n\t\telif i in ls:\n\t\t\ttemp_num = temp_num + i\n\t\telif i == ' ':\n\t\t\tif temp_num == \"\":\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tif fin_eq == \"\":\n\t\t\t\t\tfin_eq = fin_eq + temp_num\n\t\t\t\telse:\n\t\t\t\t\tfin_eq = fin_eq + ' ' + temp_num\n\t\t\ttemp_num = \"\"\n\t\telse:\n\t\t\tif fin_eq == \"\":\n\t\t\t\tif temp_num == \"\":\n\t\t\t\t\tfin_eq = fin_eq + i\n\t\t\t\telse:\n\t\t\t\t\tfin_eq = fin_eq + temp_num + ' ' + i\n\t\t\telse:\n\t\t\t\tif temp_num == \"\":\n\t\t\t\t\tfin_eq = fin_eq + ' ' + i\n\t\t\t\telse:\n\t\t\t\t\tfin_eq = fin_eq + ' ' + temp_num + ' ' + i\n\t\t\ttemp_num = \"\"\n\tif temp_num != \"\":\n\t\tfin_eq = fin_eq + ' ' + temp_num\n\treturn fin_eq\n\ndef prefix_to_infix(prefix):\n\toperators = ['+', '-', '*', '/']\n\tstack = []\n\telements = format_eq(prefix).split()\n\tfor i in range(len(elements)-1, -1, -1):\n\t\tif elements[i] in operators and len(stack)>1:\n\t\t\top1 = stack.pop(-1)\n\t\t\top2 = stack.pop(-1)\n\t\t\tfin_operand = '(' + ' ' + op1 + ' ' + elements[i] + ' ' + op2 + ' ' + ')'\n\t\t\tstack.append(fin_operand)\n\t\telse:\n\t\t\tstack.append(elements[i])\n\ttry:\n\t\treturn stack[0]\n\texcept:\n\t\treturn \"\"\n\ndef stack_to_string(stack):\n\top = \"\"\n\tfor i in stack:\n\t\tif op == \"\":\n\t\t\top = op + i\n\t\telse:\n\t\t\top = op + ' ' + i\n\treturn op\n\ndef back_align(eq, list_num):\n\telements = eq.split()\n\tfor i in range(len(elements)):\n\t\tif elements[i][0] == 'n':\n\t\t\tindex = int(elements[i][6])\n\t\t\ttry:\n\t\t\t\tnumber = str(list_num[index])\n\t\t\texcept:\n\t\t\t\treturn '-1000.112'\n\t\t\telements[i] = number\n\treturn stack_to_string(elements)    \n\ndef ans_evaluator(eq, list_num):\n\t#pdb.set_trace()\n\tinfix = prefix_to_infix(eq)\n\taligned = back_align(infix, list_num)\n\ttry:\n\t\tfinal_ans = parse_expr(aligned, evaluate = True)\n\texcept:\n\t\tfinal_ans = -1000.112\n\treturn final_ans\n\ndef cal_score(outputs, nums, ans, eqns):\n\tcorr = 0\n\ttot = 0\n\tdisp_corr = []\n\tfor i in range(len(outputs)):\n\t\top = stack_to_string(outputs[i])\n\t\tif 'NONE' in op:\n\t\t\tif op == eqns[i]:\n\t\t\t\tcorr+=1\n\t\t\t\ttot+=1\n\t\t\t\tdisp_corr.append(1)\n\t\t\telse:\n\t\t\t\ttot+=1\n\t\t\t\tdisp_corr.append(0)\n\t\telse:\n\t\t\tnum = nums[i].split()\n\t\t\tnum = [float(nu) for nu in num]\n\t\t\tanswer = ans[i].item()\n\n\t\t\tpred = ans_evaluator(op, num)\n\n\t\t\tif abs(pred - answer) <= 0.1:\n\t\t\t\tcorr+=1\n\t\t\t\ttot+=1\n\t\t\t\tdisp_corr.append(1)\n\t\t\telse:\n\t\t\t\ttot+=1\n\t\t\t\tdisp_corr.append(0)\n\n\treturn corr, tot, disp_corr\n\ndef get_infix_eq(outputs, nums):\n\teqs = []\n\tfor i in range(len(outputs)):\n\t\top = stack_to_string(outputs[i])\n\t\tnum = nums[i].split()\n\t\tnum = [float(nu) for nu in num]\n\n\t\tinfix = prefix_to_infix(op)\n\t\teqs.append(infix)\n\n\treturn eqs\n\n\n##################################################\n# Helper.py #\n##################################################\n\ndef gpu_init_pytorch(gpu_num):\n\t'''\n\t\tInitialize GPU\n\t'''\n\ttorch.cuda.set_device(int(gpu_num))\n\tdevice = torch.device(\"cuda:{}\".format(\n\t\tgpu_num) if torch.cuda.is_available() else \"cpu\")\n\treturn device\n\ndef create_save_directories(path):\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\ndef save_checkpoint(state, epoch, logger, model_path, ckpt):\n\t'''\n\t\tSaves the model state along with epoch number. The name format is important for \n\t\tthe load functions. Don't mess with it.\n\n\t\tArgs:\n\t\t\tmodel state\n\t\t\tepoch number\n\t\t\tlogger variable\n\t\t\tdirectory to save models\n\t\t\tcheckpoint name\n\t'''\n\tckpt_path = os.path.join(model_path, '{}.pt'.format(ckpt))\n\tlogger.info('Saving Checkpoint at : {}'.format(ckpt_path))\n\ttorch.save(state, ckpt_path)\n\ndef get_latest_checkpoint(model_path, logger):\n\t'''\n\t\tLooks for the checkpoint with highest epoch number in the directory \"model_path\" \n\n\t\tArgs:\n\t\t\tmodel_path: including the run_name\n\t\t\tlogger variable: to log messages\n\t\tReturns:\n\t\t\tcheckpoint: path to the latest checkpoint \n\t'''\n\n\tckpts = glob('{}/*.pt'.format(model_path))\n\tckpts = sorted(ckpts)\n\n\tif len(ckpts) == 0:\n\t\tlogger.warning('No Checkpoints Found')\n\n\t\treturn None\n\telse:\n\t\t#pdb.set_trace()\n\t\t#latest_epoch = max([int(x.split('_')[-1].split('.')[0]) for x in ckpts])\n\t\t#ckpts = sorted(ckpts, key= lambda x: int(x.split('_')[-1].split('.')[0]) , reverse=True )\n\t\tckpt_path = ckpts[0]\n\t\t#logger.info('Checkpoint found with epoch number : {}'.format(latest_epoch))\n\t\tlogger.debug('Checkpoint found at : {}'.format(ckpt_path))\n\n\t\treturn ckpt_path\n\ndef load_checkpoint(config, model, mode, ckpt_path, logger, device):\n\tcheckpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n\tmodel.load_state_dict(checkpoint['model_state_dict'])\n\tmodel.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\tif config.separate_opt:\n\t\tmodel.emb_optimizer.load_state_dict(checkpoint['emb_optimizer_state_dict'])\n\tstart_epoch = checkpoint['epoch']\n\tmin_train_loss  =checkpoint['min_train_loss']\n\tmin_val_loss = checkpoint['min_val_loss']\n\tvoc1 = checkpoint['voc1']\n\tvoc2 = checkpoint['voc2']\n\tmax_train_acc = checkpoint['max_train_acc']\n\tmax_val_acc = checkpoint['max_val_acc']\n\tmax_val_bleu = checkpoint['max_val_bleu']\n\tbest_epoch = checkpoint['best_epoch']\n\n\tmodel.to(device)\n\n\tif mode == 'train':\n\t\tmodel.train()\n\telse:\n\t\tmodel.eval()\n\n\tlogger.info('Successfully Loaded Checkpoint from {}, with epoch number: {} for {}'.format(ckpt_path, start_epoch, mode))\n\n\treturn start_epoch, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2\n\nclass Voc1:\n\tdef __init__(self):\n\t\tself.trimmed = False\n\t\tself.frequented = False\n\t\tself.w2id = {'<s>': 0, '</s>': 1, 'unk': 2}\n\t\tself.id2w = {0: '<s>', 1: '</s>', 2: 'unk'}\n\t\tself.w2c = {}\n\t\tself.nwords = 3\n\n\tdef add_word(self, word):\n\t\tif word not in self.w2id:\n\t\t\tself.w2id[word] = self.nwords\n\t\t\tself.id2w[self.nwords] = word\n\t\t\tself.w2c[word] = 1\n\t\t\tself.nwords += 1\n\t\telse:\n\t\t\tself.w2c[word] += 1\n\n\tdef add_sent(self, sent):\n\t\tfor word in sent.split():\n\t\t\tself.add_word(word)\n\n\tdef most_frequent(self, topk):\n\t\t# if self.frequented == True:\n\t\t# \treturn\n\t\t# self.frequented = True\n\n\t\tkeep_words = []\n\t\tcount = 3\n\t\tsort_by_value = sorted(\n\t\t\tself.w2c.items(), key=lambda kv: kv[1], reverse=True)\n\t\tfor word, freq in sort_by_value:\n\t\t\tkeep_words += [word]*freq\n\t\t\tcount += 1\n\t\t\tif count == topk:\n\t\t\t\tbreak\n\n\t\tself.w2id = {'<s>': 0, '</s>': 1, 'unk': 2}\n\t\tself.id2w = {0: '<s>', 1: '</s>', 2: 'unk'}\n\t\tself.w2c = {}\n\t\tself.nwords = 3\n\n\t\tfor word in keep_words:\n\t\t\tself.add_word(word)\n\n\tdef trim(self, mincount):\n\t\tif self.trimmed == True:\n\t\t\treturn\n\t\tself.trimmed = True\n\n\t\tkeep_words = []\n\t\tfor k, v in self.w2c.items():\n\t\t\tif v >= mincount:\n\t\t\t\tkeep_words += [k]*v\n\n\t\tself.w2id = {'<s>': 0, '</s>': 1, 'unk': 2}\n\t\tself.id2w = {0: '<s>', 1: '</s>', 2: 'unk'}\n\t\tself.w2c = {}\n\t\tself.nwords = 3\n\t\tfor word in keep_words:\n\t\t\tself.addWord(word)\n\n\tdef get_id(self, idx):\n\t\treturn self.w2id[idx]\n\n\tdef get_word(self, idx):\n\t\treturn self.id2w[idx]\n\n\tdef create_vocab_dict(self, args, train_dataloader):\n\t\tfor data in train_dataloader:\n\t\t\tfor sent in data['ques']:\n\t\t\t\tself.add_sent(sent)\n\n\t\tself.most_frequent(args.vocab_size)\n\t\tassert len(self.w2id) == self.nwords\n\t\tassert len(self.id2w) == self.nwords\n\n\tdef add_to_vocab_dict(self, args, dataloader):\n\t\tfor data in dataloader:\n\t\t\tfor sent in data['ques']:\n\t\t\t\tself.add_sent(sent)\n\n\t\tself.most_frequent(args.vocab_size)\n\t\tassert len(self.w2id) == self.nwords\n\t\tassert len(self.id2w) == self.nwords\n\nclass Voc2:\n\tdef __init__(self, config):\n\t\tself.frequented = False\n\t\tif config.more_nums:\n\t\t\tself.w2id = {'<s>': 0, '</s>': 1, '+': 2, '-': 3, '*': 4, '/': 5, 'number0': 6, 'number1': 7, 'number2': 8, 'number3': 9, 'number4': 10, 'number5': 11, 'number6': 12, 'number7': 13, 'number8': 14, 'number9': 15, 'number10': 16, 'number11': 17}\n\t\t\tself.id2w = {0: '<s>', 1: '</s>', 2: '+', 3: '-', 4: '*', 5: '/', 6: 'number0', 7: 'number1', 8: 'number2', 9: 'number3', 10: 'number4', 11: 'number5', 12: 'number6', 13: 'number7', 14: 'number8', 15: 'number9', 16: 'number10', 17: 'number11'}\n\t\t\tself.w2c = {'+': 0, '-': 0, '*': 0, '/': 0, 'number0': 0, 'number1': 0, 'number2': 0, 'number3': 0, 'number4': 0, 'number5': 0, 'number6': 0, 'number7': 0, 'number8': 0, 'number9': 0, 'number10': 0, 'number11': 0}\n\t\t\tself.nwords = 18\n\t\telif config.mawps_vocab:\n\t\t\t# '0.25', '8.0', '0.05', '60.0', '7.0', '5.0', '2.0', '4.0', '1.0', '12.0', '100.0', '25.0', '0.1', '3.0', '0.01', '0.5', '10.0'\n\t\t\tself.w2id = {'<s>': 0, '</s>': 1, '+': 2, '-': 3, '*': 4, '/': 5, 'number0': 6, 'number1': 7, 'number2': 8, 'number3': 9, 'number4': 10, '0.25': 11, '8.0': 12, '0.05': 13, '60.0': 14, '7.0': 15, '5.0': 16, '2.0': 17, '4.0': 18, '1.0': 19, '12.0': 20, '100.0': 21, '25.0': 22, '0.1': 23, '3.0': 24, '0.01': 25, '0.5': 26, '10.0': 27}\n\t\t\tself.id2w = {0: '<s>', 1: '</s>', 2: '+', 3: '-', 4: '*', 5: '/', 6: 'number0', 7: 'number1', 8: 'number2', 9: 'number3', 10: 'number4', 11: '0.25', 12: '8.0', 13: '0.05', 14: '60.0', 15: '7.0', 16: '5.0', 17: '2.0', 18: '4.0', 19: '1.0', 20: '12.0', 21: '100.0', 22: '25.0', 23: '0.1', 24: '3.0', 25: '0.01', 26: '0.5', 27: '10.0'}\n\t\t\tself.w2c = {'+': 0, '-': 0, '*': 0, '/': 0, 'number0': 0, 'number1': 0, 'number2': 0, 'number3': 0, 'number4': 0, '0.25': 0, '8.0': 0, '0.05': 0, '60.0': 0, '7.0': 0, '5.0': 0, '2.0': 0, '4.0': 0, '1.0': 0, '12.0': 0, '100.0': 0, '25.0': 0, '0.1': 0, '3.0': 0, '0.01': 0, '0.5': 0, '10.0': 0}\n\t\t\tself.nwords = 28\n\t\telse:\n\t\t\tself.w2id = {'<s>': 0, '</s>': 1, '+': 2, '-': 3, '*': 4, '/': 5, 'number0': 6, 'number1': 7, 'number2': 8, 'number3': 9, 'number4': 10}\n\t\t\tself.id2w = {0: '<s>', 1: '</s>', 2: '+', 3: '-', 4: '*', 5: '/', 6: 'number0', 7: 'number1', 8: 'number2', 9: 'number3', 10: 'number4'}\n\t\t\tself.w2c = {'+': 0, '-': 0, '*': 0, '/': 0, 'number0': 0, 'number1': 0, 'number2': 0, 'number3': 0, 'number4': 0}\n\t\t\tself.nwords = 11\n\n\tdef add_word(self, word):\n\t\tif word not in self.w2id: # IT SHOULD NEVER GO HERE!!\n\t\t\tself.w2id[word] = self.nwords\n\t\t\tself.id2w[self.nwords] = word\n\t\t\tself.w2c[word] = 1\n\t\t\tself.nwords += 1\n\t\telse:\n\t\t\tself.w2c[word] += 1\n\n\tdef add_sent(self, sent):\n\t\tfor word in sent.split():\n\t\t\tself.add_word(word)\n\n\tdef get_id(self, idx):\n\t\treturn self.w2id[idx]\n\n\tdef get_word(self, idx):\n\t\treturn self.id2w[idx]\n\n\tdef create_vocab_dict(self, args, train_dataloader):\n\t\tfor data in train_dataloader:\n\t\t\tfor sent in data['eqn']:\n\t\t\t\tself.add_sent(sent)\n\n\t\tassert len(self.w2id) == self.nwords\n\t\tassert len(self.id2w) == self.nwords\n\n\tdef add_to_vocab_dict(self, args, dataloader):\n\t\tfor data in dataloader:\n\t\t\tfor sent in data['eqn']:\n\t\t\t\tself.add_sent(sent)\n\n\t\tassert len(self.w2id) == self.nwords\n\t\tassert len(self.id2w) == self.nwords\n\ndef bleu_scorer(ref, hyp, script='default'):\n    refsend = []\n    for i in range(len(ref)):\n        refsi = []\n        for j in range(len(ref[i])):\n            refsi.append(ref[i][j].split())\n        refsend.append(refsi)\n\n    gensend = []\n    for i in range(len(hyp)):\n        gensend.append(hyp[i].split())\n\n    if script == 'nltk':\n        metrics = corpus_bleu(refsend, gensend)\n        return [metrics]\n\n    metrics = compute_bleu(refsend, gensend)\n    return metrics\n    \n    \n##################################################\n# Logger.py #\n##################################################\n\n'''Logging Modules'''\n\ndef get_logger(name, log_file_path='./logs/temp.log', logging_level=logging.INFO, log_format='%(asctime)s | %(levelname)s | %(filename)s: %(lineno)s : %(funcName)s() ::\\t %(message)s'):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging_level)\n\tformatter = logging.Formatter(log_format)\n\n\tfile_handler = logging.FileHandler(log_file_path, mode='w')\n\tfile_handler.setLevel(logging_level)\n\tfile_handler.setFormatter(formatter)\n\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging_level)\n\tstream_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(stream_handler)\n\n\treturn logger\n\ndef print_log(logger, dict):\n\tstring = ''\n\tfor key, value in dict.items():\n\t\tstring += '\\n {}: {}\\t'.format(key.replace('_', ' '), value)\n\tlogger.info(string)\n\ndef store_results(config, max_val_bleu, max_val_acc, min_val_loss, max_train_acc, min_train_loss, best_epoch):\n\ttry:\n\t\twith open(config.result_path) as f:\n\t\t\tres_data =json.load(f)\n\texcept:\n\t\tres_data = {}\n\ttry:\n\t\tmin_train_loss = min_train_loss.item()\n\texcept:\n\t\tpass\n\ttry:\n\t\tmin_val_loss = min_val_loss.item()\n\texcept:\n\t\tpass\n\ttry:\n\t\tdata= {'run name' : str(config.run_name)\n\t\t, 'max val acc': str(max_val_acc)\n\t\t, 'max train acc': str(max_train_acc)\n\t\t, 'max val bleu' : str(max_val_bleu)\n\t\t, 'min val loss' : str(min_val_loss)\n\t\t, 'min train loss': str(min_train_loss)\n\t\t, 'best epoch': str(best_epoch)\n\t\t, 'epochs' : config.epochs\n\t\t, 'dataset' : config.dataset\n\t\t, 'embedding': config.embedding\n\t\t, 'embedding_size': config.emb1_size\n\t\t, 'embedding_lr': config.emb_lr\n\t\t, 'freeze_emb': config.freeze_emb\n\t\t, 'cell_type' : config.cell_type\n\t\t, 'bidirectional' : config.bidirectional\n\t\t, 'hidden_size' : config.hidden_size\n\t\t, 'depth' : config.depth\n\t\t, 'lr' : config.lr\n\t\t, 'batch_size' : config.batch_size\n\t\t, 'dropout' : config.dropout\n\t\t, 'separate optimizers' : config.separate_opt\n\t\t, 'opt' : config.opt\n\t\t}\n\t\tres_data[str(config.run_name)] = data\n\n\t\twith open(config.result_path, 'w', encoding='utf-8') as f:\n\t\t\tjson.dump(res_data, f, ensure_ascii= False, indent= 4)\n\texcept:\n\t\tpdb.set_trace()\n\ndef store_val_results(config, acc_score, folds_scores):\n\ttry:\n\t\twith open(config.val_result_path) as f:\n\t\t\tres_data = json.load(f)\n\texcept:\n\t\tres_data = {}\n\n\ttry:\n\t\tdata= {'run_name' : str(config.run_name)\n\t\t, '5-fold avg acc score' : str(acc_score)\n\t\t, 'Fold0 acc' : folds_scores[0]\n\t\t, 'Fold1 acc' : folds_scores[1]\n\t\t, 'Fold2 acc' : folds_scores[2]\n\t\t, 'Fold3 acc' : folds_scores[3]\n\t\t, 'Fold4 acc' : folds_scores[4]\n\t\t, 'epochs' : config.epochs\n\t\t, 'embedding': config.embedding\n\t\t, 'embedding_size': config.emb1_size\n\t\t, 'embedding_lr': config.emb_lr\n\t\t, 'freeze_emb': config.freeze_emb\n\t\t, 'cell_type' : config.cell_type\n\t\t, 'bidirectional' : config.bidirectional\n\t\t, 'hidden_size' : config.hidden_size\n\t\t, 'depth' : config.depth\n\t\t, 'lr' : config.lr\n\t\t, 'batch_size' : config.batch_size\n\t\t, 'dropout' : config.dropout\n\t\t, 'separate optimizers' : config.separate_opt\n\t\t, 'opt' : config.opt\n\t\t}\n\t\tres_data[str(config.run_name)] = data\n\n\t\twith open(config.val_result_path, 'w', encoding='utf-8') as f:\n\t\t\tjson.dump(res_data, f, ensure_ascii= False, indent= 4)\n\texcept:\n\t\tpdb.set_trace()\n        \n##################################################\n# sentence_processing.py #\n##################################################\ndef sent_to_idx(voc, sent, max_length):\n\tidx_vec = []\n\tfor w in sent.split(' '):\n\t\ttry:\n\t\t\tidx = voc.get_id(w)\n\t\t\tidx_vec.append(idx)\n\t\texcept:\n\t\t\tidx_vec.append(voc.get_id('unk'))\n\t# idx_vec.append(voc.get_id('</s>'))\n\tif len(idx_vec) < max_length-1:\n\t\tidx_vec.append(voc.get_id('</s>'))\n\treturn idx_vec\n\n\ndef sents_to_idx(voc, sents, max_length):\n\tall_indexes = []\n\tfor sent in sents:\n\t\tall_indexes.append(sent_to_idx(voc, sent, max_length))\n\treturn all_indexes\n\n\ndef sent_to_tensor(voc, sentence, device, max_length):\n\tindexes = sent_to_idx(voc, sentence, max_length)\n\treturn torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\n\ndef batch_to_tensor(voc, sents, device, max_length):\n\tbatch_sent = []\n\t# batch_label = []\n\tfor sent in sents:\n\t\tsent_id = sent_to_tensor(voc, sent, device, max_length)\n\t\tbatch_sent.append(sent_id)\n\n\treturn batch_sent\n\n\ndef idx_to_sent(voc, tensor, no_eos=False):\n\tsent_word_list = []\n\tfor idx in tensor:\n\t\tword = voc.get_word(idx.item())\n\t\tif no_eos:\n\t\t\tif word != '</s>':\n\t\t\t\tsent_word_list.append(word)\n\t\t\t# else:\n\t\t\t# \tbreak\n\t\telse:\n\t\t\tsent_word_list.append(word)\n\treturn sent_word_list\n\n\ndef idx_to_sents(voc, tensors, no_eos=False):\n\ttensors = tensors.transpose(0, 1)\n\tbatch_word_list = []\n\tfor tensor in tensors:\n\t\tbatch_word_list.append(idx_to_sent(voc, tensor, no_eos))\n\n\treturn batch_word_list\n\n\ndef pad_seq(seq, max_length, voc):\n\tseq += [voc.get_id('</s>') for i in range(max_length - len(seq))]\n\treturn seq\n\n# def process_single(sent, label, voc, device):\n\ndef sort_by_len(seqs, input_len, device=None, dim=1):\n\torig_idx = list(range(seqs.size(dim)))\n\t# pdb.set_trace()\n\n\t# Index by which sorting needs to be done\n\tsorted_idx = sorted(orig_idx, key=lambda k: input_len[k], reverse=True)\n\tsorted_idx= torch.LongTensor(sorted_idx)\n\tif device:\n\t\tsorted_idx = sorted_idx.to(device)\n\n\tsorted_seqs = seqs.index_select(1, sorted_idx)\n\tsorted_lens=  [input_len[i] for i in sorted_idx]\n\n\t# For restoring original order\n\torig_idx = sorted(orig_idx, key=lambda k: sorted_idx[k])\n\torig_idx = torch.LongTensor(orig_idx)\n\tif device:\n\t\torig_idx = orig_idx.to(device)\n\treturn sorted_seqs, sorted_lens, orig_idx\n\n\ndef restore_order(seqs, input_len, orig_idx):\n\torig_seqs= [seqs[i] for i in orig_idx]\n\torig_lens= [input_len[i] for i in orig_idx]\n\treturn orig_seqs, orig_lens\n\n\ndef process_batch(sent1s, sent2s, voc1, voc2, device):\n\tinput_len1 = [len(s) for s in sent1s]\n\tinput_len2 = [len(s) for s in sent2s]\n\tmax_length_1 = max(input_len1)\n\tmax_length_2 = max(input_len2)\n\n\tsent1s_padded = [pad_seq(s, max_length_1, voc1) for s in sent1s]\n\tsent2s_padded = [pad_seq(s, max_length_2, voc2) for s in sent2s]\n\n\t# Convert to [Max_len X Batch]\n\tsent1_var = Variable(torch.LongTensor(sent1s_padded)).transpose(0, 1)\n\tsent2_var = Variable(torch.LongTensor(sent2s_padded)).transpose(0, 1)\n\n\tsent1_var = sent1_var.to(device)\n\tsent2_var = sent2_var.to(device)\n\n\treturn sent1_var, sent2_var, input_len1, input_len2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:49:27.797032Z","iopub.execute_input":"2024-11-13T07:49:27.797359Z","iopub.status.idle":"2024-11-13T07:49:27.895371Z","shell.execute_reply.started":"2024-11-13T07:49:27.797324Z","shell.execute_reply":"2024-11-13T07:49:27.894320Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## Confidence_estimation","metadata":{}},{"cell_type":"code","source":"def posterior_based_conf(test_ques, model):\n\n    decoded_words, decoded_log_probs = model.greedy_decode(test_ques, return_probs = True)\n    posteriors = [np.exp(sum(log_probs)) for log_probs in decoded_log_probs]\n    return decoded_words, posteriors\n\ndef similarity_based_conf(test_ques, train_ques,model, sim_criteria = 'bert_score'):\n    '''\n    Takes a batch of test question and evaluates their closest similarities between questions in training set.\n    Inputs:\n        test_ques: A list of strings containing a batch of test questions. Length: Batch Size\n        train_ques: A list containing **ALL** the questions present in training data. Length: |Training Data|\n        model: bert_seq2exp model\n        sim_criteria: Criteria used to evaluate similarity between test questions and training questions\n\n    Returns a numpy array containing closest similarity of each test input in the batch size. Shape: [Batch Size,]\n    '''\n\n    decoded_words = model.greedy_decode(test_ques)\n    if sim_criteria == 'bert_score':\n        similarities = bert_sim(test_ques, train_ques, model) #[Batch Size x |Training Data|]\n\n    elif sim_criteria == 'bleu_score':\n        similarities = bleu_sim(test_ques, train_ques)\n    else:\n        raise ValueError(\"Other similarity methods not implemented yet!\")\n\n    max_sims = np.max(similarities, axis = 1)\n    return decoded_words, max_sims\n\n\n\ndef bert_sim(queries, keys, model):\n    '''\n    Inputs\n        - queries: a batch of sentences whose similarity is to be measured with other sentences. Length: L_Q\n        - keys: those other sentences. Length: L_K\n        - model: bert_seq2exp model\n\n    Outputs: A numpy array containing similarites between each test sentence with all training examples. Shape: [L_Q, L_K]\n    '''\n\n    #Feed queries and keys to bert and obtain contextualized representation, using embeddings of [CLS]\n    #  (TODO: try pooling instead of [CLS])\n    with torch.no_grad():\n        queries_rep     = model.bert(queries)[0][:,0].detach().cpu().numpy()\n        keys_rep        = torch.cat([model.bert(keys[i:min(i+16, len(keys)),])[0][:,0] for i in range(0, len(keys), 16)], dim = 0)\n        keys_rep        = keys_rep.detach().cpu().numpy()\n\n    sims = np.dot(queries_rep / np.linalg.norm(queries_rep, axis = -1, keepdims = True),\n                 (keys_rep / np.linalg.norm(keys_rep, axis = -1, keepdims = True)).T)\n    return sims\n\n\ndef bleu_sim(queries, keys):\n    '''\n    Inputs:\n        - queries: a batch of sentences whose similarity is to be measured with other sentences. Length: L_Q\n        - keys: those other sentences. Length: L_K\n\n    Outputs: A numpy array containing bleu scores between each test sentence with all training examples. Shape: [L_Q, L_K]\n    '''\n    bleus = [[] for i in range(len(queries))]\n    for i in range(len(queries)):\n        for j in range(len(keys)):\n            refs = [[keys[j].split()]]\n            hyps = [queries[i].split()]\n            bleu = compute_bleu(refs, hyps)[0]\n            bleus[i].append(bleu)\n\n    sims = np.array(bleus)\n    return sims\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:49:27.897515Z","iopub.execute_input":"2024-11-13T07:49:27.897896Z","iopub.status.idle":"2024-11-13T07:49:27.913222Z","shell.execute_reply.started":"2024-11-13T07:49:27.897845Z","shell.execute_reply":"2024-11-13T07:49:27.912384Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## Dataloader.py","metadata":{}},{"cell_type":"code","source":"class TextDataset(Dataset):\n\t'''\n\t\tExpecting csv files with columns ['sent1', 'sent2']\n\n\t\tArgs:\n\t\t\t\t\t\tdata_path: Root folder Containing all the data\n\t\t\t\t\t\tdataset: Specific Folder==> data_path/dataset/\t(Should contain train.csv and dev.csv)\n\t\t\t\t\t\tmax_length: Self Explanatory\n\t\t\t\t\t\tis_debug: Load a subset of data for faster testing\n\t\t\t\t\t\tis_train: \n\n\t'''\n\n\tdef __init__(self, data_path='./kaggle/input/svamp-data/data/', dataset='mawps', datatype='train', max_length=30, is_debug=False, is_train=False, grade_info=False, type_info=False, challenge_info=False):\n\t\tif datatype=='train':\n\t\t\tfile_path = os.path.join(data_path, dataset, 'train.csv')\n\t\telif datatype=='dev':\n\t\t\tfile_path = os.path.join(data_path, dataset, 'dev.csv')\n\t\telse:\n\t\t\tfile_path = os.path.join(data_path, dataset, 'dev.csv')\n\n\t\tif grade_info:\n\t\t\tself.grade_info = True\n\t\telse:\n\t\t\tself.grade_info = False\n\n\t\tif type_info:\n\t\t\tself.type_info = True\n\t\telse:\n\t\t\tself.type_info = False\n\n\t\tif challenge_info:\n\t\t\tself.challenge_info = True\n\t\telse:\n\t\t\tself.challenge_info = False\n\n\t\tfile_df= pd.read_csv(file_path)\n\n\t\tself.ques= file_df['Question'].values\n\t\tself.eqn= file_df['Equation'].values\n\t\tself.nums= file_df['Numbers'].values\n\t\tself.ans= file_df['Answer'].values\n\n\t\tif grade_info:\n\t\t\tself.grade = file_df['Grade'].values\n\n\t\tif type_info:\n\t\t\tself.type = file_df['Type'].values\n\n\t\tif challenge_info:\n\t\t\tself.type = file_df['Type'].values\n\t\t\tself.var_type = file_df['Variation Type'].values\n\t\t\tself.annotator = file_df['Annotator'].values\n\t\t\tself.alternate = file_df['Alternate'].values\n\n\t\tif is_debug:\n\t\t\tself.ques= self.ques[:5000:500]\n\t\t\tself.eqn= self.eqn[:5000:500]\n\n\t\tself.max_length= max_length\n\n\t\tif grade_info and type_info:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans, self.grade, self.type)\n\t\telif grade_info and not type_info:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans, self.grade)\n\t\telif type_info and not grade_info:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans, self.type)\n\t\telif challenge_info:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans, self.type, self.var_type, self.annotator, self.alternate)\n\t\telse:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans)\n\n\t\tif is_train:\n\t\t\tall_sents = sorted(all_sents, key = lambda x : len(x[0].split()))\n\n\t\tif grade_info and type_info:\n\t\t\tself.ques, self.eqn, self.nums, self.ans, self.grade, self.type = zip(*all_sents)\n\t\telif grade_info and not type_info:\n\t\t\tself.ques, self.eqn, self.nums, self.ans, self.grade = zip(*all_sents)\n\t\telif type_info and not grade_info:\n\t\t\tself.ques, self.eqn, self.nums, self.ans, self.type = zip(*all_sents)\n\t\telif challenge_info:\n\t\t\tself.ques, self.eqn, self.nums, self.ans, self.type, self.var_type, self.annotator, self.alternate = zip(*all_sents)\n\t\telse:\n\t\t\tself.ques, self.eqn, self.nums, self.ans = zip(*all_sents)\n\n\tdef __len__(self):\n\t\treturn len(self.ques)\n\n\tdef __getitem__(self, idx):\n\t\tques = self.process_string(str(self.ques[idx]))\n\t\teqn = self.process_string(str(self.eqn[idx]))\n\t\tnums = self.nums[idx]\n\t\tans = self.ans[idx]\n\n\t\tif self.grade_info and self.type_info:\n\t\t\tgrade = self.grade[idx]\n\t\t\ttype1 = self.type[idx]\n\t\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans, 'grade': grade, 'type': type1}\n\t\telif self.grade_info and not self.type_info:\n\t\t\tgrade = self.grade[idx]\n\t\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans, 'grade': grade}\n\t\telif self.type_info and not self.grade_info:\n\t\t\ttype1 = self.type[idx]\n\t\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans, 'type': type1}\n\t\telif self.challenge_info:\n\t\t\ttype1 = self.type[idx]\n\t\t\tvar_type = self.var_type[idx]\n\t\t\tannotator = self.annotator[idx]\n\t\t\talternate = self.alternate[idx]\n\t\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans, 'type': type1, \n\t\t\t\t\t'var_type': var_type, 'annotator': annotator, 'alternate': alternate}\n\t\n\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans}\n\n\tdef curb_to_length(self, string):\n\t\treturn ' '.join(string.strip().split()[:self.max_length])\n\n\tdef process_string(self, string):\n\t\t#string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n\t\tstring = re.sub(r\"\\'s\", \" 's\", string)\n\t\tstring = re.sub(r\"\\'ve\", \" 've\", string)\n\t\tstring = re.sub(r\"n\\'t\", \" n't\", string)\n\t\tstring = re.sub(r\"\\'re\", \" 're\", string)\n\t\tstring = re.sub(r\"\\'d\", \" 'd\", string)\n\t\tstring = re.sub(r\"\\'ll\", \" 'll\", string)\n\t\t#string = re.sub(r\",\", \" , \", string)\n\t\t#string = re.sub(r\"!\", \" ! \", string)\n\t\t#string = re.sub(r\"\\(\", \" ( \", string)\n\t\t#string = re.sub(r\"\\)\", \" ) \", string)\n\t\t#string = re.sub(r\"\\?\", \" ? \", string)\n\t\t#string = re.sub(r\"\\s{2,}\", \" \", string)\n\t\treturn string\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:49:27.914403Z","iopub.execute_input":"2024-11-13T07:49:27.914757Z","iopub.status.idle":"2024-11-13T07:49:27.943634Z","shell.execute_reply.started":"2024-11-13T07:49:27.914723Z","shell.execute_reply":"2024-11-13T07:49:27.942675Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## Model.py","metadata":{}},{"cell_type":"code","source":"class Seq2SeqModel(nn.Module):\n\tdef __init__(self, config, voc1, voc2, device, logger, num_iters, EOS_tag='</s>', SOS_tag='<s>'):\n\t\tsuper(Seq2SeqModel, self).__init__()\n\n\t\tself.config = config\n\t\tself.device = device\n\t\tself.voc1 = voc1\n\t\tself.voc2 = voc2\n\t\tself.EOS_tag = EOS_tag\n\t\tself.SOS_tag = SOS_tag\n\t\tself.EOS_token = voc2.get_id(EOS_tag)\n\t\tself.SOS_token = voc2.get_id(SOS_tag)\n\t\tself.logger = logger\n\t\tself.num_iters = num_iters\n\n\t\tself.embedding2 = nn.Embedding(self.voc2.nwords, self.config.emb2_size)\n\t\tnn.init.uniform_(self.embedding2.weight, -1 * self.config.init_range, self.config.init_range)\n\n\t\tif self.config.embedding == 'bert':\n\t\t\tself.embedding1 = BertEncoder(self.config.emb_name, self.device, self.config.freeze_emb)\n\t\telif self.config.embedding == 'roberta':\n\t\t\tself.embedding1 = RobertaEncoder(self.config.emb_name, self.device, self.config.freeze_emb)\n\t\telif self.config.embedding == 'word2vec':\n\t\t\tself.config.emb1_size = 300\n\t\t\tself.embedding1 = nn.Embedding.from_pretrained(torch.FloatTensor(self._form_embeddings(self.config.word2vec_bin)), freeze = self.config.freeze_emb)\n\t\telse:\n\t\t\tself.embedding1  = nn.Embedding(self.voc1.nwords, self.config.emb1_size)\n\t\t\tnn.init.uniform_(self.embedding1.weight, -1 * self.config.init_range, self.config.init_range)\n\n\t\tself.logger.debug('Building Encoders...')\n\t\tself.encoder = Encoder(\n\t\t\tself.config.hidden_size,\n\t\t\tself.config.emb1_size,\n\t\t\tself.config.cell_type,\n\t\t\tself.config.depth,\n\t\t\tself.config.dropout,\n\t\t\tself.config.bidirectional\n\t\t)\n\n\t\tself.logger.debug('Encoders Built...')\n\n\t\tif self.config.use_attn:\n\t\t\tself.decoder    = LuongAttnDecoderRNN(self.config.attn_type,\n\t\t\t\t\t\t\t\t\t\t\t\t  self.embedding2,\n\t\t\t\t\t\t\t\t\t\t\t\t  self.config.cell_type,\n\t\t\t\t\t\t\t\t\t\t\t\t  self.config.hidden_size,\n\t\t\t\t\t\t\t\t\t\t\t\t  self.voc2.nwords,\n\t\t\t\t\t\t\t\t\t\t\t\t  self.config.depth,\n\t\t\t\t\t\t\t\t\t\t\t\t  self.config.dropout).to(device)\n\t\telse:\n\t\t\tself.decoder    = DecoderRNN(self.embedding2,\n\t\t\t\t\t\t\t\t\t\t self.config.cell_type,\n\t\t\t\t\t\t\t\t\t\t self.config.hidden_size,\n\t\t\t\t\t\t\t\t\t\t self.voc2.nwords,\n\t\t\t\t\t\t\t\t\t\t self.config.depth,\n\t\t\t\t\t\t\t\t\t\t self.config.dropout).to(device)\n\n\t\tself.logger.debug('Decoder RNN Built...')\n\n\t\tself.logger.debug('Initalizing Optimizer and Criterion...')\n\t\tself._initialize_optimizer()\n\n\t\t# nn.CrossEntropyLoss() does both F.log_softmax() and nn.NLLLoss() \n\t\tself.criterion = nn.NLLLoss() \n\n\t\tself.logger.info('All Model Components Initialized...')\n\n\tdef _form_embeddings(self, file_path):\n\t\tweights_all = models.KeyedVectors.load_word2vec_format(file_path, limit=200000, binary=True)\n\t\tweight_req  = torch.randn(self.voc1.nwords, self.config.emb1_size)\n\t\tfor key, value in self.voc1.id2w.items():\n\t\t\tif value in weights_all:\n\t\t\t\tweight_req[key] = torch.FloatTensor(weights_all[value])\n\n\t\treturn weight_req\t\n\n\tdef _initialize_optimizer(self):\n\t\tself.params =   list(self.embedding1.parameters()) + \\\n\t\t\t\t\t\tlist(self.encoder.parameters()) + \\\n\t\t\t\t\t\tlist(self.decoder.parameters())\n\n\t\tif self.config.separate_opt:\n\t\t\tself.emb_optimizer = AdamW(self.embedding1.parameters(), lr = self.config.emb_lr, correct_bias = True)\n\t\t\tself.optimizer = optim.Adam(\n\t\t\t\t[{\"params\": self.encoder.parameters()},\n\t\t\t\t{\"params\": self.decoder.parameters()}],\n\t\t\t\tlr = self.config.lr,\n\t\t\t)\n\t\telse:\n\t\t\tif self.config.opt == 'adam':\n\t\t\t\tself.optimizer = optim.Adam(\n\t\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t\t{\"params\": self.encoder.parameters()},\n\t\t\t\t\t{\"params\": self.decoder.parameters()}],\n\t\t\t\t\tlr = self.config.lr\n\t\t\t\t)\n\t\t\telif self.config.opt == 'adadelta':\n\t\t\t\tself.optimizer = optim.Adadelta(\n\t\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t\t{\"params\": self.encoder.parameters()},\n\t\t\t\t\t{\"params\": self.decoder.parameters()}],\n\t\t\t\t\tlr = self.config.lr\n\t\t\t\t)\n\t\t\telif self.config.opt == 'asgd':\n\t\t\t\tself.optimizer = optim.ASGD(\n\t\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t\t{\"params\": self.encoder.parameters()},\n\t\t\t\t\t{\"params\": self.decoder.parameters()}],\n\t\t\t\t\tlr = self.config.lr\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tself.optimizer = optim.SGD(\n\t\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t\t{\"params\": self.encoder.parameters()},\n\t\t\t\t\t{\"params\": self.decoder.parameters()}],\n\t\t\t\t\tlr = self.config.lr\n\t\t\t\t)\n\n\tdef forward(self, input_seq1, input_seq2, input_len1, input_len2):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tinput_seq1 (tensor): values are word indexes | size : [max_len x batch_size]\n\t\t\t\tinput_len1 (tensor): Length of each sequence in input_len1 | size : [batch_size]\n\t\t\t\tinput_seq2 (tensor): values are word indexes | size : [max_len x batch_size]\n\t\t\t\tinput_len2 (tensor): Length of each sequence in input_len2 | size : [batch_size]\n\t\t\tReturns:\n\t\t\t\tout (tensor) : Probabilities of each output label for each point | size : [batch_size x num_labels]\n\t\t'''\n\n\tdef trainer(self, ques, input_seq1, input_seq2, input_len1, input_len2, config, device=None ,logger=None):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tques (list): input examples as is (i.e. not indexed) | size : [batch_size]\n\t\t\tReturns:\n\t\t\t\t\n\t\t'''\n\t\tself.optimizer.zero_grad()\n\t\tif self.config.separate_opt:\n\t\t\tself.emb_optimizer.zero_grad()\n\n\t\tif self.config.embedding == 'bert' or self.config.embedding == 'roberta':\n\t\t\tinput_seq1, input_len1 = self.embedding1(ques)\n\t\t\tinput_seq1 = input_seq1.transpose(0,1)\n\t\t\t# input_seq1: Tensor [max_len x BS x emb1_size]\n\t\t\t# input_len1: List [BS]\n\t\t\tsorted_seqs, sorted_len, orig_idx = sort_by_len(input_seq1, input_len1, self.device)\n\t\t\t# sorted_seqs: Tensor [max_len x BS x emb1_size]\n\t\t\t# input_len1: List [BS]\n\t\t\t# orig_idx: Tensor [BS]\n\t\telse:\n\t\t\tsorted_seqs, sorted_len, orig_idx = sort_by_len(input_seq1, input_len1, self.device)\n\t\t\tsorted_seqs = self.embedding1(sorted_seqs)\n\n\t\tencoder_outputs, encoder_hidden = self.encoder(sorted_seqs, sorted_len, orig_idx, self.device)\n\t\t\n\t\tself.loss =0\n\n\t\tdecoder_input = torch.tensor([self.SOS_token for i in range(input_seq1.size(1))], device = self.device)\n\n\t\tif config.cell_type == 'lstm':\n\t\t\tdecoder_hidden = (encoder_hidden[0][:self.decoder.nlayers], encoder_hidden[1][:self.decoder.nlayers])\n\t\telse:\n\t\t\tdecoder_hidden = encoder_hidden[:self.decoder.nlayers]\n\n\t\tuse_teacher_forcing = True if random.random() < self.config.teacher_forcing_ratio else False\n\t\ttarget_len = max(input_len2)\n\n\t\tif use_teacher_forcing:\n\t\t\tfor step in range(target_len):\n\t\t\t\tif self.config.use_attn:\n\t\t\t\t\tdecoder_output, decoder_hidden, decoder_attention, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n\t\t\t\telse:\n\t\t\t\t\tdecoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n\t\t\t\tself.loss += self.criterion(decoder_output, input_seq2[step])\n\t\t\t\tdecoder_input = input_seq2[step]\n\t\telse:\n\t\t\tfor step in range(target_len):\n\t\t\t\tif self.config.use_attn:\n\t\t\t\t\tdecoder_output, decoder_hidden, decoder_attention, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n\t\t\t\telse:\n\t\t\t\t\tdecoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n\t\t\t\t\n\t\t\t\ttopv, topi = decoder_output.topk(1)\n\t\t\t\tself.loss += self.criterion(decoder_output, input_seq2[step])\n\t\t\t\tdecoder_input = topi.squeeze().detach() \n\n\t\tself.loss.backward()\n\t\tif self.config.max_grad_norm > 0:\n\t\t\ttorch.nn.utils.clip_grad_norm_(self.params, self.config.max_grad_norm)\n\t\tself.optimizer.step()\n\t\tif self.config.separate_opt:\n\t\t\tself.emb_optimizer.step()\n\n\t\treturn self.loss.item()/target_len\n\n\tdef greedy_decode(self, ques, input_seq1=None, input_seq2=None, input_len1=None, input_len2=None, validation=False, return_probs = False):\n\t\twith torch.no_grad():\n\t\t\tif self.config.embedding == 'bert' or self.config.embedding == 'roberta':\n\t\t\t\tinput_seq1, input_len1 = self.embedding1(ques)\n\t\t\t\tinput_seq1 = input_seq1.transpose(0,1)\n\t\t\t\tsorted_seqs, sorted_len, orig_idx = sort_by_len(input_seq1, input_len1, self.device)\n\t\t\telse:\n\t\t\t\tsorted_seqs, sorted_len, orig_idx = sort_by_len(input_seq1, input_len1, self.device)\n\t\t\t\tsorted_seqs = self.embedding1(sorted_seqs)\n\n\t\t\tencoder_outputs, encoder_hidden = self.encoder(sorted_seqs, sorted_len, orig_idx, self.device)\n\n\t\t\tloss = 0.0\n\t\t\tdecoder_input = torch.tensor([self.SOS_token for i in range(input_seq1.size(1))], device=self.device)\n\n\t\t\tif self.config.cell_type == 'lstm':\n\t\t\t\tdecoder_hidden = (encoder_hidden[0][:self.decoder.nlayers], encoder_hidden[1][:self.decoder.nlayers])\n\t\t\telse:\n\t\t\t\tdecoder_hidden = encoder_hidden[:self.decoder.nlayers]\n\n\t\t\tdecoded_words = [[] for i in range(input_seq1.size(1))]\n\t\t\tdecoded_probs = [[] for i in range(input_seq1.size(1))]\n\t\t\tdecoder_attentions = []\n\n\t\t\tif validation:\n\t\t\t\ttarget_len = max(input_len2)\n\t\t\telse:\n\t\t\t\ttarget_len = self.config.max_length\n\n\t\t\tfor step in range(target_len):\n\t\t\t\tif self.config.use_attn:\n\t\t\t\t\tdecoder_output, decoder_hidden, decoder_attention, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n\t\t\t\t\tdecoder_attentions.append(decoder_attention)\n\t\t\t\telse:\n\t\t\t\t\tdecoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n\n\t\t\t\tif validation:\n\t\t\t\t\tloss += self.criterion(decoder_output, input_seq2[step])\n\t\t\t\ttopv, topi = decoder_output.topk(1)\n\t\t\t\tfor i in range(input_seq1.size(1)):\n\t\t\t\t\tif topi[i].item() == self.EOS_token:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tdecoded_words[i].append(self.voc2.get_word(topi[i].item()))\n\t\t\t\t\tdecoded_probs[i].append(topv[i].item())\n\t\t\t\tdecoder_input = topi.squeeze().detach()\n\n\t\t\tif validation:\n\t\t\t\tif self.config.use_attn:\n\t\t\t\t\treturn loss/target_len, decoded_words, decoder_attentions[:step + 1]\n\t\t\t\telse:\n\t\t\t\t\treturn loss/target_len, decoded_words, None\n\t\t\telse:\n\t\t\t\tif return_probs:\n\t\t\t\t\treturn decoded_words, decoded_probs\n\n\t\t\t\treturn decoded_words\n\n\tdef obtain_hidden(self, config, ques, input_seq1=None, input_seq2=None, input_len1=None, input_len2=None):\n\t\twith torch.no_grad():\n\t\t\tif self.config.embedding == 'bert' or self.config.embedding == 'roberta':\n\t\t\t\tinput_seq1, input_len1 = self.embedding1(ques)\n\t\t\t\tinput_seq1 = input_seq1.transpose(0,1)\n\t\t\t\tsorted_seqs, sorted_len, orig_idx = sort_by_len(input_seq1, input_len1, self.device)\n\t\t\telse:\n\t\t\t\tsorted_seqs, sorted_len, orig_idx = sort_by_len(input_seq1, input_len1, self.device)\n\t\t\t\tsorted_seqs = self.embedding1(sorted_seqs)\n\n\t\t\tencoder_outputs, encoder_hidden = self.encoder(sorted_seqs, sorted_len, orig_idx, self.device)\n\n\t\t\tloss =0.0\n\t\t\tdecoder_input = torch.tensor([self.SOS_token for i in range(input_seq1.size(1))], device=self.device)\n\n\t\t\tif self.config.cell_type == 'lstm':\n\t\t\t\tdecoder_hidden = (encoder_hidden[0][:self.decoder.nlayers], encoder_hidden[1][:self.decoder.nlayers])\n\t\t\telse:\n\t\t\t\tdecoder_hidden = encoder_hidden[:self.decoder.nlayers]\n\n\t\t\tdecoded_words = [[] for i in range(input_seq1.size(1))]\n\t\t\tdecoder_attentions = []\n\n\t\t\thiddens = []\n\n\t\t\ttarget_len = max(input_len2)\n\n\t\t\tfor step in range(target_len):\n\t\t\t\tif self.config.use_attn:\n\t\t\t\t\tdecoder_output, decoder_hidden, decoder_attention, hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n\t\t\t\t\tdecoder_attentions.append(decoder_attention)\n\t\t\t\telse:\n\t\t\t\t\tdecoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n\n\t\t\t\ttopv, topi = decoder_output.topk(1)\n\t\t\t\tfor i in range(input_seq1.size(1)):\n\t\t\t\t\tif topi[i].item() == self.EOS_token:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tdecoded_words[i].append(self.voc2.get_word(topi[i].item()))\n\t\t\t\t\thiddens.append([self.voc2.get_word(topi[i].item()), hidden[i]])\n\t\t\t\tdecoder_input = topi.squeeze().detach()\n\n\t\t\treturn hiddens, decoded_words\n\ndef build_model(config, voc1, voc2, device, logger, num_iters):\n\t'''\n\t\tAdd Docstring\n\t'''\n\tmodel = Seq2SeqModel(config, voc1, voc2, device, logger, num_iters)\n\tmodel = model.to(device)\n\n\treturn model\n\ndef train_model(model, train_dataloader, val_dataloader, voc1, voc2, device, config, logger, epoch_offset= 0, min_val_loss=float('inf'), max_val_bleu=0.0, max_val_acc = 0.0, min_train_loss=float('inf'), max_train_acc = 0.0, best_epoch = 0, writer= None):\n\t'''\n\t\tAdd Docstring\n\t'''\n\n\tif config.histogram and config.save_writer and writer:\n\t\tfor name, param in model.named_parameters():\n\t\t\twriter.add_histogram(name, param, epoch_offset)\n\t\n\testop_count=0\n\t\n\tfor epoch in range(1, config.epochs + 1):\n\t\tod = OrderedDict()\n\t\tod['Epoch'] = epoch + epoch_offset\n\t\tprint_log(logger, od)\n\n\t\tbatch_num = 1\n\t\ttrain_loss_epoch = 0.0\n\t\ttrain_acc_epoch = 0.0\n\t\ttrain_acc_epoch_cnt = 0.0\n\t\ttrain_acc_epoch_tot = 0.0\n\t\tval_loss_epoch = 0.0\n\n\t\tstart_time= time()\n\t\ttotal_batches = len(train_dataloader)\n\n\t\tfor data in train_dataloader:\n\t\t\tques = data['ques']\n\n\t\t\tsent1s = sents_to_idx(voc1, data['ques'], config.max_length)\n\t\t\tsent2s = sents_to_idx(voc2, data['eqn'], config.max_length)\n\t\t\tsent1_var, sent2_var, input_len1, input_len2  = process_batch(sent1s, sent2s, voc1, voc2, device)\n\n\t\t\tnums = data['nums']\n\t\t\tans = data['ans']\n\n\t\t\tmodel.train()\n\n\t\t\tloss = model.trainer(ques, sent1_var, sent2_var, input_len1, input_len2, config, device, logger)\n\t\t\ttrain_loss_epoch += loss\n\n\t\t\tif config.show_train_acc:\n\t\t\t\tmodel.eval()\n\n\t\t\t\t_, decoder_output, _ = model.greedy_decode(ques, sent1_var, sent2_var, input_len1, input_len2, validation=True)\n\t\t\t\ttemp_acc_cnt, temp_acc_tot, _ = cal_score(decoder_output, nums, ans, data['eqn'])\n\t\t\t\ttrain_acc_epoch_cnt += temp_acc_cnt\n\t\t\t\ttrain_acc_epoch_tot += temp_acc_tot\n\n\t\t\tbatch_num+=1\n\t\t\tprint(\"Completed {} / {}...\".format(batch_num, total_batches), end = '\\r', flush = True)\n\n\t\ttrain_loss_epoch = train_loss_epoch/len(train_dataloader)\n\t\tif config.show_train_acc:\n\t\t\ttrain_acc_epoch = train_acc_epoch_cnt/train_acc_epoch_tot\n\t\telse:\n\t\t\ttrain_acc_epoch = 0.0\n\n\t\ttime_taken = (time() - start_time)/60.0\n\n\t\tif config.save_writer and writer:\n\t\t\twriter.add_scalar('loss/train_loss', train_loss_epoch, epoch + epoch_offset)\n\n\t\tlogger.debug('Training for epoch {} completed...\\nTime Taken: {}'.format(epoch, time_taken))\n\t\tlogger.debug('Starting Validation')\n\n\t\tval_bleu_epoch, val_loss_epoch, val_acc_epoch = run_validation(config=config, model=model, dataloader=val_dataloader, voc1=voc1, voc2=voc2, device=device, logger=logger, epoch_num = epoch)\n\n\t\tif train_loss_epoch < min_train_loss:\n\t\t\tmin_train_loss = train_loss_epoch\n\n\t\tif train_acc_epoch > max_train_acc:\n\t\t\tmax_train_acc = train_acc_epoch\n\n\t\tif val_bleu_epoch[0] > max_val_bleu:\n\t\t\tmax_val_bleu = val_bleu_epoch[0]\n\n\t\tif val_loss_epoch < min_val_loss:\n\t\t\tmin_val_loss = val_loss_epoch\n\n\t\tif val_acc_epoch > max_val_acc:\n\t\t\tmax_val_acc = val_acc_epoch\n\t\t\tbest_epoch = epoch + epoch_offset\n\n\t\t\tif config.separate_opt:\n\t\t\t\tstate = {\n\t\t\t\t\t'epoch' : epoch + epoch_offset,\n\t\t\t\t\t'best_epoch': best_epoch,\n\t\t\t\t\t'model_state_dict': model.state_dict(),\n\t\t\t\t\t'voc1': model.voc1,\n\t\t\t\t\t'voc2': model.voc2,\n\t\t\t\t\t'optimizer_state_dict': model.optimizer.state_dict(),\n\t\t\t\t\t'emb_optimizer_state_dict': model.emb_optimizer.state_dict(),\n\t\t\t\t\t'train_loss_epoch' : train_loss_epoch,\n\t\t\t\t\t'min_train_loss' : min_train_loss,\n\t\t\t\t\t'train_acc_epoch' : train_acc_epoch,\n\t\t\t\t\t'max_train_acc' : max_train_acc,\n\t\t\t\t\t'val_loss_epoch' : val_loss_epoch,\n\t\t\t\t\t'min_val_loss' : min_val_loss,\n\t\t\t\t\t'val_acc_epoch' : val_acc_epoch,\n\t\t\t\t\t'max_val_acc' : max_val_acc,\n\t\t\t\t\t'val_bleu_epoch': val_bleu_epoch[0],\n\t\t\t\t\t'max_val_bleu': max_val_bleu\n\t\t\t\t}\n\t\t\telse:\n\t\t\t\tstate = {\n\t\t\t\t\t'epoch' : epoch + epoch_offset,\n\t\t\t\t\t'best_epoch': best_epoch,\n\t\t\t\t\t'model_state_dict': model.state_dict(),\n\t\t\t\t\t'voc1': model.voc1,\n\t\t\t\t\t'voc2': model.voc2,\n\t\t\t\t\t'optimizer_state_dict': model.optimizer.state_dict(),\n\t\t\t\t\t'train_loss_epoch' : train_loss_epoch,\n\t\t\t\t\t'min_train_loss' : min_train_loss,\n\t\t\t\t\t'train_acc_epoch' : train_acc_epoch,\n\t\t\t\t\t'max_train_acc' : max_train_acc,\n\t\t\t\t\t'val_loss_epoch' : val_loss_epoch,\n\t\t\t\t\t'min_val_loss' : min_val_loss,\n\t\t\t\t\t'val_acc_epoch' : val_acc_epoch,\n\t\t\t\t\t'max_val_acc' : max_val_acc,\n\t\t\t\t\t'val_bleu_epoch': val_bleu_epoch[0],\n\t\t\t\t\t'max_val_bleu': max_val_bleu\n\t\t\t\t}\n\t\t\tlogger.debug('Validation Bleu: {}'.format(val_bleu_epoch[0]))\n\n\t\t\tif config.save_model:\n\t\t\t\tsave_checkpoint(state, epoch + epoch_offset, logger, config.model_path, config.ckpt)\n\t\t\testop_count = 0\n\t\telse:\n\t\t\testop_count+=1\n\n\t\tif config.save_writer and writer:\n\t\t\twriter.add_scalar('loss/val_loss', val_loss_epoch, epoch + epoch_offset)\n\t\t\twriter.add_scalar('acc/val_score', val_bleu_epoch[0], epoch + epoch_offset)\n\n\t\tod = OrderedDict()\n\t\tod['Epoch'] = epoch + epoch_offset\n\t\tod['best_epoch'] = best_epoch\n\t\tod['train_loss_epoch'] = train_loss_epoch\n\t\tod['min_train_loss'] = min_train_loss\n\t\tod['val_loss_epoch']= val_loss_epoch\n\t\tod['min_val_loss']= min_val_loss\n\t\tod['train_acc_epoch'] = train_acc_epoch\n\t\tod['max_train_acc'] = max_train_acc\n\t\tod['val_acc_epoch'] = val_acc_epoch\n\t\tod['max_val_acc'] = max_val_acc\n\t\tod['val_bleu_epoch'] = val_bleu_epoch\n\t\tod['max_val_bleu'] = max_val_bleu\n\t\tprint_log(logger, od)\n\n\t\tif config.histogram and config.save_writer and writer:\n\t\t\tfor name, param in model.named_parameters():\n\t\t\t\twriter.add_histogram(name, param, epoch + epoch_offset)\n\n\t\tif estop_count > config.early_stopping:\n\t\t\tlogger.debug('Early Stopping at Epoch: {} after no improvement in {} epochs'.format(epoch, estop_count))\n\t\t\tbreak\n\n\tif config.save_writer:\n\t\twriter.export_scalars_to_json(os.path.join(config.board_path, 'all_scalars.json'))\n\t\twriter.close()\n\n\tlogger.info('Training Completed for {} epochs'.format(config.epochs))\n\n\tif config.results:\n\t\tstore_results(config, max_val_bleu, max_val_acc, min_val_loss, max_train_acc, min_train_loss, best_epoch)\n\t\tlogger.info('Scores saved at {}'.format(config.result_path))\n\n\treturn max_val_acc\n\ndef run_validation(config, model, dataloader, voc1, voc2, device, logger, epoch_num):\n\tbatch_num = 1\n\tval_loss_epoch = 0.0\n\tval_bleu_epoch = 0.0\n\tval_acc_epoch = 0.0\n\tval_acc_epoch_cnt = 0.0\n\tval_acc_epoch_tot = 0.0\n\n\tmodel.eval()\n\n\trefs= []\n\thyps= []\n\n\tif config.mode == 'test':\n\t\tquestions, gen_eqns, act_eqns, scores = [], [], [], []\n\n\tdisplay_n = config.batch_size\n\n\twith open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n\t\tf_out.write('---------------------------------------\\n')\n\t\tf_out.write('Epoch: ' + str(epoch_num) + '\\n')\n\t\tf_out.write('---------------------------------------\\n')\n\ttotal_batches = len(dataloader)\n\tfor data in dataloader:\n\t\tsent1s = sents_to_idx(voc1, data['ques'], config.max_length)\n\t\tsent2s = sents_to_idx(voc2, data['eqn'], config.max_length)\n\t\tnums = data['nums']\n\t\tans = data['ans']\n\t\tif config.grade_disp:\n\t\t\tgrade = data['grade']\n\t\tif config.type_disp:\n\t\t\ttype1 = data['type']\n\t\tif config.challenge_disp:\n\t\t\ttype1 = data['type']\n\t\t\tvar_type = data['var_type']\n\t\t\tannotator = data['annotator']\n\t\t\talternate = data['alternate']\n\n\t\tques = data['ques']\n\n\t\tsent1_var, sent2_var, input_len1, input_len2 = process_batch(sent1s, sent2s, voc1, voc2, device)\n\n\t\tval_loss, decoder_output, decoder_attn = model.greedy_decode(ques, sent1_var, sent2_var, input_len1, input_len2, validation=True)\n\n\t\ttemp_acc_cnt, temp_acc_tot, disp_corr = cal_score(decoder_output, nums, ans, data['eqn'])\n\t\tval_acc_epoch_cnt += temp_acc_cnt\n\t\tval_acc_epoch_tot += temp_acc_tot\n\n\t\tsent1s = idx_to_sents(voc1, sent1_var, no_eos= True)\n\t\tsent2s = idx_to_sents(voc2, sent2_var, no_eos= True)\n\n\t\trefs += [[' '.join(sent2s[i])] for i in range(sent2_var.size(1))]\n\t\thyps += [' '.join(decoder_output[i]) for i in range(sent1_var.size(1))]\n\n\t\tif config.mode == 'test':\n\t\t\tquestions+= data['ques']\n\t\t\tgen_eqns += [' '.join(decoder_output[i]) for i in range(sent1_var.size(1))]\n\t\t\tact_eqns += [' '.join(sent2s[i]) for i in range(sent2_var.size(1))]\n\t\t\tscores   += [cal_score([decoder_output[i]], [nums[i]], [ans[i]], [data['eqn'][i]])[0] for i in range(sent1_var.size(1))]\n\n\t\twith open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n\t\t\tf_out.write('Batch: ' + str(batch_num) + '\\n')\n\t\t\tf_out.write('---------------------------------------\\n')\n\t\t\tfor i in range(len(sent1s[:display_n])):\n\t\t\t\ttry:\n\t\t\t\t\tf_out.write('Example: ' + str(i) + '\\n')\n\t\t\t\t\tif config.grade_disp:\n\t\t\t\t\t\tf_out.write('Grade: ' + str(grade[i].item()) + '\\n')\n\t\t\t\t\tif config.type_disp:\n\t\t\t\t\t\tf_out.write('Type: ' + str(type1[i]) + '\\n')\n\t\t\t\t\tf_out.write('Source: ' + stack_to_string(sent1s[i]) + '\\n')\n\t\t\t\t\tf_out.write('Target: ' + stack_to_string(sent2s[i]) + '\\n')\n\t\t\t\t\tf_out.write('Generated: ' + stack_to_string(decoder_output[i]) + '\\n')\n\t\t\t\t\tif config.challenge_disp:\n\t\t\t\t\t\tf_out.write('Type: ' + str(type1[i]) + '\\n')\n\t\t\t\t\t\tf_out.write('Variation Type: ' + str(var_type[i]) + '\\n')\n\t\t\t\t\t\tf_out.write('Annotator: ' + str(annotator[i]) + '\\n')\n\t\t\t\t\t\tf_out.write('Alternate: ' + str(alternate[i].item()) + '\\n')\n\t\t\t\t\tif config.nums_disp:\n\t\t\t\t\t\tsrc_nums = 0\n\t\t\t\t\t\ttgt_nums = 0\n\t\t\t\t\t\tpred_nums = 0\n\t\t\t\t\t\tfor k in range(len(sent1s[i])):\n\t\t\t\t\t\t\tif sent1s[i][k][:6] == 'number':\n\t\t\t\t\t\t\t\tsrc_nums += 1\n\t\t\t\t\t\tfor k in range(len(sent2s[i])):\n\t\t\t\t\t\t\tif sent2s[i][k][:6] == 'number':\n\t\t\t\t\t\t\t\ttgt_nums += 1\n\t\t\t\t\t\tfor k in range(len(decoder_output[i])):\n\t\t\t\t\t\t\tif decoder_output[i][k][:6] == 'number':\n\t\t\t\t\t\t\t\tpred_nums += 1\n\t\t\t\t\t\tf_out.write('Numbers in question: ' + str(src_nums) + '\\n')\n\t\t\t\t\t\tf_out.write('Numbers in Target Equation: ' + str(tgt_nums) + '\\n')\n\t\t\t\t\t\tf_out.write('Numbers in Predicted Equation: ' + str(pred_nums) + '\\n')\n\t\t\t\t\tf_out.write('Result: ' + str(disp_corr[i]) + '\\n' + '\\n')\n\t\t\t\texcept:\n\t\t\t\t\tlogger.warning('Exception: Failed to generate')\n\t\t\t\t\tpdb.set_trace()\n\t\t\t\t\tbreak\n\t\t\tf_out.write('---------------------------------------\\n')\n\t\t\tf_out.close()\n\n\t\tif batch_num % config.display_freq ==0:\n\t\t\tfor i in range(len(sent1s[:display_n])):\n\t\t\t\ttry:\n\t\t\t\t\tod = OrderedDict()\n\t\t\t\t\tlogger.info('-------------------------------------')\n\t\t\t\t\tod['Source'] = ' '.join(sent1s[i])\n\n\t\t\t\t\tod['Target'] = ' '.join(sent2s[i])\n\n\t\t\t\t\tod['Generated'] = ' '.join(decoder_output[i])\n\t\t\t\t\tprint_log(logger, od)\n\t\t\t\t\tlogger.info('-------------------------------------')\n\t\t\t\texcept:\n\t\t\t\t\tlogger.warning('Exception: Failed to generate')\n\t\t\t\t\tpdb.set_trace()\n\t\t\t\t\tbreak\n\n\t\tval_loss_epoch += val_loss\n\t\tbatch_num +=1\n\t\tprint(\"Completed {} / {}...\".format(batch_num, total_batches), end = '\\r', flush = True)\n\n\tval_bleu_epoch = bleu_scorer(refs, hyps)\n\tif config.mode == 'test':\n\t\tresults_df = pd.DataFrame([questions, act_eqns, gen_eqns, scores]).transpose()\n\t\tresults_df.columns = ['Question', 'Actual Equation', 'Generated Equation', 'Score']\n\t\tcsv_file_path = os.path.join(config.outputs_path, config.dataset+'.csv')\n\t\tresults_df.to_csv(csv_file_path, index = False)\n\t\treturn sum(scores)/len(scores)\n\n\tval_acc_epoch = val_acc_epoch_cnt/val_acc_epoch_tot\n\n\treturn val_bleu_epoch, val_loss_epoch/len(dataloader), val_acc_epoch\n\ndef estimate_confidence(config, model, dataloader, logger):\n\t\n\tquestions\t= []\n\tact_eqns \t= []\n\tgen_eqns\t= []\n\tscores\t\t= []\n\tconfs\t\t= []\n\tbatch_num = 0\n\t\n\t#Load training data (Will be useful for similarity based methods)\n\ttrain_df \t= pd.read_csv(os.path.join('data',config.dataset,'train.csv'))\n\ttrain_ques\t= train_df['Question'].values \n\t\n\ttotal_batches = len(dataloader)\n\tlogger.info(\"Beginning estimating confidence based on {} criteria\".format(config.conf))\n\tstart = time()\n\tfor data in dataloader:\n\t\tques, eqn, nums, ans = data['ques'], data['eqn'], data['nums'], data['ans']\n\t\t\n\t\tif config.conf == 'posterior':\n\t\t\tdecoded_words, confidence = posterior_based_conf(ques, model)\n\t\telif config.conf == 'similarity':\n\t\t\tdecoded_words, confidence = similarity_based_conf(ques, train_ques, model, sim_criteria= config.sim_criteria)\n\t\telse:\n\t\t\t#TODO: Implement other methods\n\t\t\traise ValueError(\"Other confidence methods not implemented yet. Use -conf posterior\")\n\t\t\n\t\tif not config.adv:\n\t\t\tcorrect_or_not = [cal_score([decoded_words[i]], [nums[i]], [ans[i]])[0] for i in range(len(decoded_words))]\n\t\telse:\n\t\t\tcorrect_or_not = [-1 for i in range(len(decoded_words))]\n\n\t\tgen_eqn = [' '.join(words) for words in decoded_words]\n\t\t\n\t\tquestions \t+= ques\n\t\tact_eqns\t+= eqn\n\t\tgen_eqns\t+= gen_eqn\n\t\tscores\t\t+= correct_or_not\n\t\tconfs\t\t+= list(confidence)\n\t\tbatch_num\t+= 1\n\t\tprint(\"Completed {} / {}...\".format(batch_num, total_batches), end = '\\r', flush = True)\n\n\tresults_df = pd.DataFrame([questions, act_eqns, gen_eqns, scores, confs]).transpose()\n\tresults_df.columns = ['Question', 'Actual Equation', 'Generated Equation', 'Score', 'Confidence']\n\tif config.conf != 'similarity':\n\t\tcsv_file_path = os.path.join('ConfidenceEstimates',config.dataset + '_' + config.run_name + '_' + config.conf + '.csv')\n\telse:\n\t\tcsv_file_path = os.path.join('ConfidenceEstimates',config.dataset + '_' + config.run_name + '_' + config.conf + '_' + config.sim_criteria + '.csv')\n\tresults_df.to_csv(csv_file_path)\n\tlogger.info(\"Done in {} seconds\".format(time() - start))\n\ndef get_hiddens(config, model, val_dataloader, voc1, voc2, device):\n\tbatch_num =1\n\t\n\tmodel.eval()\n\n\thiddens = []\n\toperands = []\n\n\tfor data in val_dataloader:\n\t\tif len(data['ques']) == config.batch_size:\n\t\t\tsent1s = sents_to_idx(voc1, data['ques'], config.max_length)\n\t\t\tsent2s = sents_to_idx(voc2, data['eqn'], config.max_length)\n\t\t\tnums = data['nums']\n\t\t\tans = data['ans']\n\n\t\t\tques = data['ques']\n\n\t\t\tsent1_var, sent2_var, input_len1, input_len2 = process_batch(sent1s, sent2s, voc1, voc2, device)\n\n\t\t\thidden, decoder_output = model.obtain_hidden(config, ques, sent1_var, sent2_var, input_len1, input_len2)\n\n\t\t\tinfix = get_infix_eq(decoder_output, nums)[0] # WORKS ONLY FOR BATCH SIZE 1\n\t\t\twords = infix.split()\n\n\t\t\ttype_rep = []\n\t\t\toperand_types = []\n\n\t\t\tfor w in range(len(words)):\n\t\t\t\tif words[w] == '/':\n\t\t\t\t\tif words[w-1][0] == 'n':\n\t\t\t\t\t\toperand_types.append(['dividend', words[w-1]])\n\t\t\t\t\tif words[w+1][0] == 'n':\n\t\t\t\t\t\toperand_types.append(['divisor', words[w+1]])\n\t\t\t\telif words[w] == '-':\n\t\t\t\t\tif words[w-1][0] == 'n':\n\t\t\t\t\t\toperand_types.append(['minuend', words[w-1]])\n\t\t\t\t\tif words[w+1][0] == 'n':\n\t\t\t\t\t\toperand_types.append(['subtrahend', words[w+1]])\n\n\t\t\tfor z in range(len(operand_types)):\n\t\t\t\tentity = operand_types[z][1]\n\t\t\t\tfor y in range(len(hidden)):\n\t\t\t\t\tif hidden[y][0] == entity:\n\t\t\t\t\t\ttype_rep.append([operand_types[z][0], hidden[y][1]])\n\n\t\t\thiddens = hiddens + hidden\n\t\t\toperands = operands + type_rep\n\n\treturn hiddens, operands","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:49:27.946097Z","iopub.execute_input":"2024-11-13T07:49:27.946504Z","iopub.status.idle":"2024-11-13T07:49:28.065903Z","shell.execute_reply.started":"2024-11-13T07:49:27.946440Z","shell.execute_reply":"2024-11-13T07:49:28.064879Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## Main.py","metadata":{}},{"cell_type":"code","source":"global log_folder\nglobal model_folder\nglobal result_folder\nglobal data_path\nglobal board_path\n\nlog_folder = 'logs'\nmodel_folder = 'models'\noutputs_folder = 'outputs'\nresult_folder = './out/'\ndata_path = '/kaggle/input/svamp-dataset/data/'\nboard_path = './runs/'\n\ndef load_data(config, logger):\n\t'''\n\t\tLoads the data from the datapath in torch dataset form\n\n\t\tArgs:\n\t\t\tconfig (dict) : configuration/args\n\t\t\tlogger (logger) : logger object for logging\n\n\t\tReturns:\n\t\t\tdataloader(s) \n\t'''\n\tif config.mode == 'train':\n\t\tlogger.debug('Loading Training Data...')\n\n\t\t'''Load Datasets'''\n\t\tprint(data_path)\n\t\ttrain_set = TextDataset(data_path=data_path, dataset=config.dataset,\n\t\t\t\t\t\t\t\tdatatype='train', max_length=config.max_length, is_debug=config.debug)\n\t\tval_set = TextDataset(data_path=data_path, dataset=config.dataset, datatype='dev', max_length=config.max_length, \n\t\t\t\t\t\t\t\tis_debug=config.debug, grade_info=config.grade_disp, type_info=config.type_disp, \n\t\t\t\t\t\t\t\tchallenge_info=config.challenge_disp)\n\t\t\n\t\t'''In case of sort by length, write a different case with shuffle=False '''\n\t\ttrain_dataloader = DataLoader(\n\t\t\ttrain_set, batch_size=config.batch_size, shuffle=True, num_workers=5)\n\t\tval_dataloader = DataLoader(\n\t\t\tval_set, batch_size=config.batch_size, shuffle=True, num_workers=5)\n\n\t\ttrain_size = len(train_dataloader) * config.batch_size\n\t\tval_size = len(val_dataloader)* config.batch_size\n\t\t\n\t\tmsg = 'Training and Validation Data Loaded:\\nTrain Size: {}\\nVal Size: {}'.format(train_size, val_size)\n\t\tlogger.info(msg)\n\n\t\treturn train_dataloader, val_dataloader\n\n\telif config.mode == 'test' or config.mode == 'conf':\n\t\tlogger.debug('Loading Test Data...')\n\n\t\ttest_set = TextDataset(data_path=data_path, dataset=config.dataset,\n\t\t\t\t\t\t\t   datatype='test', max_length=config.max_length, is_debug=config.debug)\n\t\ttest_dataloader = DataLoader(\n\t\t\ttest_set, batch_size=config.batch_size, shuffle=True, num_workers=5)\n\n\t\tlogger.info('Test Data Loaded...')\n\t\treturn test_dataloader\n\n\telse:\n\t\tlogger.critical('Invalid Mode Specified')\n\t\traise Exception('{} is not a valid mode'.format(config.mode))\n\n\nkaggle_args = {\n    'mode': 'train',\n    'gpu': 0,\n    'embedding': 'bert',\n    'emb_name': 'bert-base-uncased',\n    'emb1_size': 768,\n    'hidden_size': 32,\n    'depth': 1,\n    'lr': 0.0002,\n    'emb_lr': 8e-6,\n    'batch_size': 16,\n    'epochs': 10,\n    'dataset': 'mawps-asdiv-a_svamp',\n    'full_cv': False,\n    'run_name': 'run_cv_asdiv-a',\n}\n\nconfig =  parse_arguments(kaggle_args)\n\nmode = config.mode\nif mode == 'train':\n    is_train = True\nelse:\n    is_train = False\n\n''' Set seed for reproducibility'''\nnp.random.seed(config.seed)\ntorch.manual_seed(config.seed)\nrandom.seed(config.seed)\n\n'''GPU initialization'''\ndevice = gpu_init_pytorch(config.gpu)\n\nif config.full_cv:\n    global data_path \n    data_name = config.dataset\n    data_path = data_path + data_name + '/'\n    config.val_result_path = os.path.join(result_folder, 'CV_results_{}.json'.format(data_name))\n    fold_acc_score = 0.0\n    folds_scores = []\n    for z in range(5):\n        run_name = config.run_name + '_fold' + str(z)\n        config.dataset = 'fold' + str(z)\n        config.log_path = os.path.join(log_folder, run_name)\n        config.model_path = os.path.join(model_folder, run_name)\n        config.board_path = os.path.join(board_path, run_name)\n        config.outputs_path = os.path.join(outputs_folder, run_name)\n\n        vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n        vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n        config_file = os.path.join(config.model_path, 'config.p')\n        log_file = os.path.join(config.log_path, 'log.txt')\n\n        if config.results:\n            config.result_path = os.path.join(result_folder, 'val_results_{}_{}.json'.format(data_name, config.dataset))\n\n        if is_train:\n            create_save_directories(config.log_path)\n            create_save_directories(config.model_path)\n            create_save_directories(config.outputs_path)\n        else:\n            create_save_directories(config.log_path)\n            create_save_directories(config.result_path)\n\n        logger = get_logger(run_name, log_file, logging.DEBUG)\n        writer = SummaryWriter(config.board_path)\n\n        logger.debug('Created Relevant Directories')\n        logger.info('Experiment Name: {}'.format(config.run_name))\n\n        '''Read Files and create/load Vocab'''\n        if is_train:\n            train_dataloader, val_dataloader = load_data(config, logger)\n\n            logger.debug('Creating Vocab...')\n\n            voc1 = Voc1()\n            voc1.create_vocab_dict(config, train_dataloader)\n\n            # To Do : Remove Later\n            voc1.add_to_vocab_dict(config, val_dataloader)\n\n            voc2 = Voc2(config)\n            voc2.create_vocab_dict(config, train_dataloader)\n\n            # To Do : Remove Later\n            voc2.add_to_vocab_dict(config, val_dataloader)\n\n            logger.info(\n                'Vocab Created with number of words : {}'.format(voc1.nwords))\n\n            with open(vocab1_path, 'wb') as f:\n                pickle.dump(voc1, f, protocol=pickle.HIGHEST_PROTOCOL)\n            with open(vocab2_path, 'wb') as f:\n                pickle.dump(voc2, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n            logger.info('Vocab saved at {}'.format(vocab1_path))\n\n        else:\n            test_dataloader = load_data(config, logger)\n            logger.info('Loading Vocab File...')\n\n            with open(vocab1_path, 'rb') as f:\n                voc1 = pickle.load(f)\n            with open(vocab2_path, 'rb') as f:\n                voc2 = pickle.load(f)\n\n            logger.info('Vocab Files loaded from {}\\nNumber of Words: {}'.format(vocab1_path, voc1.nwords))\n\n        checkpoint = get_latest_checkpoint(config.model_path, logger)\n\n        if is_train:\n            model = build_model(config=config, voc1=voc1, voc2=voc2, device=device, logger=logger, num_iters=len(train_dataloader))\n\n            logger.info('Initialized Model')\n\n            if checkpoint == None:\n                min_val_loss = torch.tensor(float('inf')).item()\n                min_train_loss = torch.tensor(float('inf')).item()\n                max_val_bleu = 0.0\n                max_val_acc = 0.0\n                max_train_acc = 0.0\n                best_epoch = 0\n                epoch_offset = 0\n            else:\n                epoch_offset, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2 = load_checkpoint(config, model, config.mode, checkpoint, logger, device)\n\n            with open(config_file, 'wb') as f:\n                pickle.dump(vars(config), f, protocol=pickle.HIGHEST_PROTOCOL)\n\n            logger.debug('Config File Saved')\n\n            logger.info('Starting Training Procedure')\n            max_val_acc = train_model(model, train_dataloader, val_dataloader, voc1, voc2,\n                        device, config, logger, epoch_offset, min_val_loss, max_val_bleu, max_val_acc, min_train_loss, max_train_acc, best_epoch, writer)\n\n        else:\n            gpu = config.gpu\n\n            with open(config_file, 'rb') as f:\n                config = AttrDict(pickle.load(f))\n                config.gpu = gpu\n\n            model = build_model(config=config, voc1=voc1, voc2=voc2, device=device, logger=logger)\n\n            epoch_offset, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2 = load_checkpoint(config, model, config.mode, checkpoint, logger, device)\n\n            logger.info('Prediction from')\n            od = OrderedDict()\n            od['epoch'] = epoch_offset\n            od['min_train_loss'] = min_train_loss\n            od['min_val_loss'] = min_val_loss\n            od['max_train_acc'] = max_train_acc\n            od['max_val_acc'] = max_val_acc\n            od['max_val_bleu'] = max_val_bleu\n            od['best_epoch'] = best_epoch\n            print_log(logger, od)\n\n            test_acc_epoch, test_loss_epoch = run_validation(config, model, test_dataloader, voc1, voc2, device, logger)\n            logger.info('Accuracy: {} \\t Loss: {}'.format(test_acc_epoch, test_loss_epoch))\n\n        fold_acc_score += max_val_acc\n        folds_scores.append(max_val_acc)\n\n    fold_acc_score = fold_acc_score/5\n    store_val_results(config, fold_acc_score, folds_scores)\n    logger.info('Final Val score: {}'.format(fold_acc_score))\n\n\nelse:\n    '''Run Config files/paths'''\n    run_name = config.run_name\n    config.log_path = os.path.join(log_folder, run_name)\n    config.model_path = os.path.join(model_folder, run_name)\n    config.board_path = os.path.join(board_path, run_name)\n    config.outputs_path = os.path.join(outputs_folder, run_name)\n\n    vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n    vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n    config_file = os.path.join(config.model_path, 'config.p')\n    log_file = os.path.join(config.log_path, 'log.txt')\n\n    if config.results:\n        config.result_path = os.path.join(result_folder, 'val_results_{}.json'.format(config.dataset))\n\n    if is_train:\n        create_save_directories(config.log_path)\n        create_save_directories(config.model_path)\n        create_save_directories(config.outputs_path)\n    else:\n        create_save_directories(config.log_path)\n        create_save_directories(config.result_path)\n\n    logger = get_logger(run_name, log_file, logging.DEBUG)\n    writer = SummaryWriter(config.board_path)\n\n    logger.debug('Created Relevant Directories')\n    logger.info('Experiment Name: {}'.format(config.run_name))\n\n    '''Read Files and create/load Vocab'''\n    if is_train:\n        train_dataloader, val_dataloader = load_data(config, logger)\n\n        logger.debug('Creating Vocab...')\n\n        voc1 = Voc1()\n        voc1.create_vocab_dict(config, train_dataloader)\n\n        # To Do : Remove Later\n        voc1.add_to_vocab_dict(config, val_dataloader)\n\n        voc2 = Voc2(config)\n        voc2.create_vocab_dict(config, train_dataloader)\n\n        # To Do : Remove Later\n        voc2.add_to_vocab_dict(config, val_dataloader)\n\n        logger.info(\n            'Vocab Created with number of words : {}'.format(voc1.nwords))\n\n        with open(vocab1_path, 'wb') as f:\n            pickle.dump(voc1, f, protocol=pickle.HIGHEST_PROTOCOL)\n        with open(vocab2_path, 'wb') as f:\n            pickle.dump(voc2, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.info('Vocab saved at {}'.format(vocab1_path))\n\n    else:\n        test_dataloader = load_data(config, logger)\n        logger.info('Loading Vocab File...')\n\n        with open(vocab1_path, 'rb') as f:\n            voc1 = pickle.load(f)\n        with open(vocab2_path, 'rb') as f:\n            voc2 = pickle.load(f)\n\n        logger.info('Vocab Files loaded from {}\\nNumber of Words: {}'.format(vocab1_path, voc1.nwords))\n\n    checkpoint = get_latest_checkpoint(config.model_path, logger)\n\n    if is_train:\n        model = build_model(config=config, voc1=voc1, voc2=voc2, device=device, logger=logger, num_iters=len(train_dataloader))\n\n        logger.info('Initialized Model')\n\n        if checkpoint == None:\n            min_val_loss = torch.tensor(float('inf')).item()\n            min_train_loss = torch.tensor(float('inf')).item()\n            max_val_bleu = 0.0\n            max_val_acc = 0.0\n            max_train_acc = 0.0\n            best_epoch = 0\n            epoch_offset = 0\n        else:\n            epoch_offset, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2 = load_checkpoint(config, model, config.mode, checkpoint, logger, device)\n\n        with open(config_file, 'wb') as f:\n            pickle.dump(vars(config), f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Config File Saved')\n\n        logger.info('Starting Training Procedure')\n        train_model(model, train_dataloader, val_dataloader, voc1, voc2,\n                    device, config, logger, epoch_offset, min_val_loss, max_val_bleu, max_val_acc, min_train_loss, max_train_acc, best_epoch, writer)\n\n    else :\n        gpu = config.gpu\n        conf = config.conf\n        sim_criteria = config.sim_criteria\n        adv = config.adv\n        mode = config.mode\n        dataset = config.dataset\n        batch_size = config.batch_size\n        with open(config_file, 'rb') as f:\n            config = AttrDict(pickle.load(f))\n            config.gpu = gpu\n            config.conf = conf\n            config.sim_criteria = sim_criteria\n            config.adv = adv\n            config.mode = mode\n            config.dataset = dataset\n            config.batch_size = batch_size\n\n        model = build_model(config=config, voc1=voc1, voc2=voc2, device=device, logger=logger,num_iters=len(test_dataloader))\n\n        epoch_offset, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2 = load_checkpoint(config, model, config.mode, checkpoint, logger, device)\n\n        logger.info('Prediction from')\n        od = OrderedDict()\n        od['epoch'] = epoch_offset\n        od['min_train_loss'] = min_train_loss\n        od['min_val_loss'] = min_val_loss\n        od['max_train_acc'] = max_train_acc\n        od['max_val_acc'] = max_val_acc\n        od['max_val_bleu'] = max_val_bleu\n        od['best_epoch'] = best_epoch\n        print_log(logger, od)\n\n        if config.mode == 'test':\n            test_acc_epoch = run_validation(config, model, test_dataloader, voc1, voc2, device, logger, 0)\n            logger.info('Accuracy: {}'.format(test_acc_epoch))\n        else:\n            estimate_confidence(config, model, test_dataloader, logger)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:49:28.068068Z","iopub.execute_input":"2024-11-13T07:49:28.068464Z","iopub.status.idle":"2024-11-13T07:58:04.548028Z","shell.execute_reply.started":"2024-11-13T07:49:28.068421Z","shell.execute_reply":"2024-11-13T07:58:04.546319Z"}},"outputs":[{"name":"stderr","text":"2024-11-13 07:49:28,128 | DEBUG | 1513005854.py: 262 : <module>() ::\t Created Relevant Directories\n2024-11-13 07:49:28,128 | DEBUG | 1513005854.py: 262 : <module>() ::\t Created Relevant Directories\n2024-11-13 07:49:28,128 | DEBUG | 1513005854.py: 262 : <module>() ::\t Created Relevant Directories\n2024-11-13 07:49:28,128 | DEBUG | 1513005854.py: 262 : <module>() ::\t Created Relevant Directories\n2024-11-13 07:49:28,128 | DEBUG | 1513005854.py: 262 : <module>() ::\t Created Relevant Directories\n2024-11-13 07:49:28,132 | INFO | 1513005854.py: 263 : <module>() ::\t Experiment Name: run_cv_asdiv-a\n2024-11-13 07:49:28,132 | INFO | 1513005854.py: 263 : <module>() ::\t Experiment Name: run_cv_asdiv-a\n2024-11-13 07:49:28,132 | INFO | 1513005854.py: 263 : <module>() ::\t Experiment Name: run_cv_asdiv-a\n2024-11-13 07:49:28,132 | INFO | 1513005854.py: 263 : <module>() ::\t Experiment Name: run_cv_asdiv-a\n2024-11-13 07:49:28,132 | INFO | 1513005854.py: 263 : <module>() ::\t Experiment Name: run_cv_asdiv-a\n2024-11-13 07:49:28,138 | DEBUG | 1513005854.py: 26 : load_data() ::\t Loading Training Data...\n2024-11-13 07:49:28,138 | DEBUG | 1513005854.py: 26 : load_data() ::\t Loading Training Data...\n2024-11-13 07:49:28,138 | DEBUG | 1513005854.py: 26 : load_data() ::\t Loading Training Data...\n2024-11-13 07:49:28,138 | DEBUG | 1513005854.py: 26 : load_data() ::\t Loading Training Data...\n2024-11-13 07:49:28,138 | DEBUG | 1513005854.py: 26 : load_data() ::\t Loading Training Data...\n2024-11-13 07:49:28,185 | INFO | 1513005854.py: 46 : load_data() ::\t Training and Validation Data Loaded:\nTrain Size: 3152\nVal Size: 1008\n2024-11-13 07:49:28,185 | INFO | 1513005854.py: 46 : load_data() ::\t Training and Validation Data Loaded:\nTrain Size: 3152\nVal Size: 1008\n2024-11-13 07:49:28,185 | INFO | 1513005854.py: 46 : load_data() ::\t Training and Validation Data Loaded:\nTrain Size: 3152\nVal Size: 1008\n2024-11-13 07:49:28,185 | INFO | 1513005854.py: 46 : load_data() ::\t Training and Validation Data Loaded:\nTrain Size: 3152\nVal Size: 1008\n2024-11-13 07:49:28,185 | INFO | 1513005854.py: 46 : load_data() ::\t Training and Validation Data Loaded:\nTrain Size: 3152\nVal Size: 1008\n2024-11-13 07:49:28,189 | DEBUG | 1513005854.py: 269 : <module>() ::\t Creating Vocab...\n2024-11-13 07:49:28,189 | DEBUG | 1513005854.py: 269 : <module>() ::\t Creating Vocab...\n2024-11-13 07:49:28,189 | DEBUG | 1513005854.py: 269 : <module>() ::\t Creating Vocab...\n2024-11-13 07:49:28,189 | DEBUG | 1513005854.py: 269 : <module>() ::\t Creating Vocab...\n2024-11-13 07:49:28,189 | DEBUG | 1513005854.py: 269 : <module>() ::\t Creating Vocab...\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/svamp-dataset/data/\n","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:49:29,657 | INFO | 1513005854.py: 283 : <module>() ::\t Vocab Created with number of words : 4086\n2024-11-13 07:49:29,657 | INFO | 1513005854.py: 283 : <module>() ::\t Vocab Created with number of words : 4086\n2024-11-13 07:49:29,657 | INFO | 1513005854.py: 283 : <module>() ::\t Vocab Created with number of words : 4086\n2024-11-13 07:49:29,657 | INFO | 1513005854.py: 283 : <module>() ::\t Vocab Created with number of words : 4086\n2024-11-13 07:49:29,657 | INFO | 1513005854.py: 283 : <module>() ::\t Vocab Created with number of words : 4086\n2024-11-13 07:49:29,668 | INFO | 1513005854.py: 291 : <module>() ::\t Vocab saved at models/run_cv_asdiv-a/vocab1.p\n2024-11-13 07:49:29,668 | INFO | 1513005854.py: 291 : <module>() ::\t Vocab saved at models/run_cv_asdiv-a/vocab1.p\n2024-11-13 07:49:29,668 | INFO | 1513005854.py: 291 : <module>() ::\t Vocab saved at models/run_cv_asdiv-a/vocab1.p\n2024-11-13 07:49:29,668 | INFO | 1513005854.py: 291 : <module>() ::\t Vocab saved at models/run_cv_asdiv-a/vocab1.p\n2024-11-13 07:49:29,668 | INFO | 1513005854.py: 291 : <module>() ::\t Vocab saved at models/run_cv_asdiv-a/vocab1.p\n2024-11-13 07:49:29,674 | WARNING | 1857244066.py: 195 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-13 07:49:29,674 | WARNING | 1857244066.py: 195 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-13 07:49:29,674 | WARNING | 1857244066.py: 195 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-13 07:49:29,674 | WARNING | 1857244066.py: 195 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-13 07:49:29,674 | WARNING | 1857244066.py: 195 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-13 07:49:30,122 | DEBUG | 477948701.py: 30 : __init__() ::\t Building Encoders...\n2024-11-13 07:49:30,122 | DEBUG | 477948701.py: 30 : __init__() ::\t Building Encoders...\n2024-11-13 07:49:30,122 | DEBUG | 477948701.py: 30 : __init__() ::\t Building Encoders...\n2024-11-13 07:49:30,122 | DEBUG | 477948701.py: 30 : __init__() ::\t Building Encoders...\n2024-11-13 07:49:30,122 | DEBUG | 477948701.py: 30 : __init__() ::\t Building Encoders...\n2024-11-13 07:49:30,132 | DEBUG | 477948701.py: 40 : __init__() ::\t Encoders Built...\n2024-11-13 07:49:30,132 | DEBUG | 477948701.py: 40 : __init__() ::\t Encoders Built...\n2024-11-13 07:49:30,132 | DEBUG | 477948701.py: 40 : __init__() ::\t Encoders Built...\n2024-11-13 07:49:30,132 | DEBUG | 477948701.py: 40 : __init__() ::\t Encoders Built...\n2024-11-13 07:49:30,132 | DEBUG | 477948701.py: 40 : __init__() ::\t Encoders Built...\n2024-11-13 07:49:30,139 | DEBUG | 477948701.py: 58 : __init__() ::\t Decoder RNN Built...\n2024-11-13 07:49:30,139 | DEBUG | 477948701.py: 58 : __init__() ::\t Decoder RNN Built...\n2024-11-13 07:49:30,139 | DEBUG | 477948701.py: 58 : __init__() ::\t Decoder RNN Built...\n2024-11-13 07:49:30,139 | DEBUG | 477948701.py: 58 : __init__() ::\t Decoder RNN Built...\n2024-11-13 07:49:30,139 | DEBUG | 477948701.py: 58 : __init__() ::\t Decoder RNN Built...\n2024-11-13 07:49:30,143 | DEBUG | 477948701.py: 60 : __init__() ::\t Initalizing Optimizer and Criterion...\n2024-11-13 07:49:30,143 | DEBUG | 477948701.py: 60 : __init__() ::\t Initalizing Optimizer and Criterion...\n2024-11-13 07:49:30,143 | DEBUG | 477948701.py: 60 : __init__() ::\t Initalizing Optimizer and Criterion...\n2024-11-13 07:49:30,143 | DEBUG | 477948701.py: 60 : __init__() ::\t Initalizing Optimizer and Criterion...\n2024-11-13 07:49:30,143 | DEBUG | 477948701.py: 60 : __init__() ::\t Initalizing Optimizer and Criterion...\n2024-11-13 07:49:30,150 | INFO | 477948701.py: 66 : __init__() ::\t All Model Components Initialized...\n2024-11-13 07:49:30,150 | INFO | 477948701.py: 66 : __init__() ::\t All Model Components Initialized...\n2024-11-13 07:49:30,150 | INFO | 477948701.py: 66 : __init__() ::\t All Model Components Initialized...\n2024-11-13 07:49:30,150 | INFO | 477948701.py: 66 : __init__() ::\t All Model Components Initialized...\n2024-11-13 07:49:30,150 | INFO | 477948701.py: 66 : __init__() ::\t All Model Components Initialized...\n2024-11-13 07:49:30,317 | INFO | 1513005854.py: 309 : <module>() ::\t Initialized Model\n2024-11-13 07:49:30,317 | INFO | 1513005854.py: 309 : <module>() ::\t Initialized Model\n2024-11-13 07:49:30,317 | INFO | 1513005854.py: 309 : <module>() ::\t Initialized Model\n2024-11-13 07:49:30,317 | INFO | 1513005854.py: 309 : <module>() ::\t Initialized Model\n2024-11-13 07:49:30,317 | INFO | 1513005854.py: 309 : <module>() ::\t Initialized Model\n2024-11-13 07:49:30,322 | DEBUG | 1513005854.py: 325 : <module>() ::\t Config File Saved\n2024-11-13 07:49:30,322 | DEBUG | 1513005854.py: 325 : <module>() ::\t Config File Saved\n2024-11-13 07:49:30,322 | DEBUG | 1513005854.py: 325 : <module>() ::\t Config File Saved\n2024-11-13 07:49:30,322 | DEBUG | 1513005854.py: 325 : <module>() ::\t Config File Saved\n2024-11-13 07:49:30,322 | DEBUG | 1513005854.py: 325 : <module>() ::\t Config File Saved\n2024-11-13 07:49:30,326 | INFO | 1513005854.py: 327 : <module>() ::\t Starting Training Procedure\n2024-11-13 07:49:30,326 | INFO | 1513005854.py: 327 : <module>() ::\t Starting Training Procedure\n2024-11-13 07:49:30,326 | INFO | 1513005854.py: 327 : <module>() ::\t Starting Training Procedure\n2024-11-13 07:49:30,326 | INFO | 1513005854.py: 327 : <module>() ::\t Starting Training Procedure\n2024-11-13 07:49:30,326 | INFO | 1513005854.py: 327 : <module>() ::\t Starting Training Procedure\n2024-11-13 07:49:30,331 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n2024-11-13 07:49:30,331 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n2024-11-13 07:49:30,331 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n2024-11-13 07:49:30,331 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n2024-11-13 07:49:30,331 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:50:14,695 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 1 completed...\nTime Taken: 0.7393231471379598\n2024-11-13 07:50:14,695 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 1 completed...\nTime Taken: 0.7393231471379598\n2024-11-13 07:50:14,695 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 1 completed...\nTime Taken: 0.7393231471379598\n2024-11-13 07:50:14,695 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 1 completed...\nTime Taken: 0.7393231471379598\n2024-11-13 07:50:14,695 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 1 completed...\nTime Taken: 0.7393231471379598\n2024-11-13 07:50:14,700 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:50:14,700 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:50:14,700 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:50:14,700 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:50:14,700 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:50:20,355 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:50:20,355 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:50:20,355 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:50:20,355 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:50:20,355 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:50:20,360 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 2.174502324533542\t\n min train loss: 2.174502324533542\t\n val loss epoch: 1.613183617591858\t\n min val loss: 1.613183617591858\t\n train acc epoch: 0.00031867431485022306\t\n max train acc: 0.00031867431485022306\t\n val acc epoch: 0.015\t\n max val acc: 0.015\t\n val bleu epoch: (0.0, [0.798, 0.0, 0.0, 0.0], 0.0844158583405561, 0.2880184331797235, 1000, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:50:20,360 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 2.174502324533542\t\n min train loss: 2.174502324533542\t\n val loss epoch: 1.613183617591858\t\n min val loss: 1.613183617591858\t\n train acc epoch: 0.00031867431485022306\t\n max train acc: 0.00031867431485022306\t\n val acc epoch: 0.015\t\n max val acc: 0.015\t\n val bleu epoch: (0.0, [0.798, 0.0, 0.0, 0.0], 0.0844158583405561, 0.2880184331797235, 1000, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:50:20,360 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 2.174502324533542\t\n min train loss: 2.174502324533542\t\n val loss epoch: 1.613183617591858\t\n min val loss: 1.613183617591858\t\n train acc epoch: 0.00031867431485022306\t\n max train acc: 0.00031867431485022306\t\n val acc epoch: 0.015\t\n max val acc: 0.015\t\n val bleu epoch: (0.0, [0.798, 0.0, 0.0, 0.0], 0.0844158583405561, 0.2880184331797235, 1000, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:50:20,360 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 2.174502324533542\t\n min train loss: 2.174502324533542\t\n val loss epoch: 1.613183617591858\t\n min val loss: 1.613183617591858\t\n train acc epoch: 0.00031867431485022306\t\n max train acc: 0.00031867431485022306\t\n val acc epoch: 0.015\t\n max val acc: 0.015\t\n val bleu epoch: (0.0, [0.798, 0.0, 0.0, 0.0], 0.0844158583405561, 0.2880184331797235, 1000, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:50:20,360 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 2.174502324533542\t\n min train loss: 2.174502324533542\t\n val loss epoch: 1.613183617591858\t\n min val loss: 1.613183617591858\t\n train acc epoch: 0.00031867431485022306\t\n max train acc: 0.00031867431485022306\t\n val acc epoch: 0.015\t\n max val acc: 0.015\t\n val bleu epoch: (0.0, [0.798, 0.0, 0.0, 0.0], 0.0844158583405561, 0.2880184331797235, 1000, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:50:20,365 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n2024-11-13 07:50:20,365 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n2024-11-13 07:50:20,365 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n2024-11-13 07:50:20,365 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n2024-11-13 07:50:20,365 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:51:05,131 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 2 completed...\nTime Taken: 0.7460256139437358\n2024-11-13 07:51:05,131 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 2 completed...\nTime Taken: 0.7460256139437358\n2024-11-13 07:51:05,131 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 2 completed...\nTime Taken: 0.7460256139437358\n2024-11-13 07:51:05,131 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 2 completed...\nTime Taken: 0.7460256139437358\n2024-11-13 07:51:05,131 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 2 completed...\nTime Taken: 0.7460256139437358\n2024-11-13 07:51:05,135 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:51:05,135 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:51:05,135 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:51:05,135 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:51:05,135 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:51:10,907 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:51:10,907 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:51:10,907 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:51:10,907 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:51:10,907 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.0\n2024-11-13 07:51:10,913 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 1.283494258047966\t\n min train loss: 1.283494258047966\t\n val loss epoch: 1.2596991062164307\t\n min val loss: 1.2596991062164307\t\n train acc epoch: 0.022944550669216062\t\n max train acc: 0.022944550669216062\t\n val acc epoch: 0.038\t\n max val acc: 0.038\t\n val bleu epoch: (0.0, [0.5411997363216875, 0.2015732546705998, 0.047388781431334626, 0.0], 0.8655727295446725, 0.8738479262672811, 3034, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:51:10,913 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 1.283494258047966\t\n min train loss: 1.283494258047966\t\n val loss epoch: 1.2596991062164307\t\n min val loss: 1.2596991062164307\t\n train acc epoch: 0.022944550669216062\t\n max train acc: 0.022944550669216062\t\n val acc epoch: 0.038\t\n max val acc: 0.038\t\n val bleu epoch: (0.0, [0.5411997363216875, 0.2015732546705998, 0.047388781431334626, 0.0], 0.8655727295446725, 0.8738479262672811, 3034, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:51:10,913 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 1.283494258047966\t\n min train loss: 1.283494258047966\t\n val loss epoch: 1.2596991062164307\t\n min val loss: 1.2596991062164307\t\n train acc epoch: 0.022944550669216062\t\n max train acc: 0.022944550669216062\t\n val acc epoch: 0.038\t\n max val acc: 0.038\t\n val bleu epoch: (0.0, [0.5411997363216875, 0.2015732546705998, 0.047388781431334626, 0.0], 0.8655727295446725, 0.8738479262672811, 3034, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:51:10,913 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 1.283494258047966\t\n min train loss: 1.283494258047966\t\n val loss epoch: 1.2596991062164307\t\n min val loss: 1.2596991062164307\t\n train acc epoch: 0.022944550669216062\t\n max train acc: 0.022944550669216062\t\n val acc epoch: 0.038\t\n max val acc: 0.038\t\n val bleu epoch: (0.0, [0.5411997363216875, 0.2015732546705998, 0.047388781431334626, 0.0], 0.8655727295446725, 0.8738479262672811, 3034, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:51:10,913 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 1.283494258047966\t\n min train loss: 1.283494258047966\t\n val loss epoch: 1.2596991062164307\t\n min val loss: 1.2596991062164307\t\n train acc epoch: 0.022944550669216062\t\n max train acc: 0.022944550669216062\t\n val acc epoch: 0.038\t\n max val acc: 0.038\t\n val bleu epoch: (0.0, [0.5411997363216875, 0.2015732546705998, 0.047388781431334626, 0.0], 0.8655727295446725, 0.8738479262672811, 3034, 3472)\t\n max val bleu: 0.0\t\n2024-11-13 07:51:10,917 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n2024-11-13 07:51:10,917 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n2024-11-13 07:51:10,917 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n2024-11-13 07:51:10,917 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n2024-11-13 07:51:10,917 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:51:55,938 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 3 completed...\nTime Taken: 0.750271757443746\n2024-11-13 07:51:55,938 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 3 completed...\nTime Taken: 0.750271757443746\n2024-11-13 07:51:55,938 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 3 completed...\nTime Taken: 0.750271757443746\n2024-11-13 07:51:55,938 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 3 completed...\nTime Taken: 0.750271757443746\n2024-11-13 07:51:55,938 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 3 completed...\nTime Taken: 0.750271757443746\n2024-11-13 07:51:55,942 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:51:55,942 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:51:55,942 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:51:55,942 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:51:55,942 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:52:01,715 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17013697465031893\n2024-11-13 07:52:01,715 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17013697465031893\n2024-11-13 07:52:01,715 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17013697465031893\n2024-11-13 07:52:01,715 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17013697465031893\n2024-11-13 07:52:01,715 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17013697465031893\n2024-11-13 07:52:01,719 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.9431342226599677\t\n min train loss: 0.9431342226599677\t\n val loss epoch: 1.220763921737671\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.14117272147864882\t\n max train acc: 0.14117272147864882\t\n val acc epoch: 0.112\t\n max val acc: 0.112\t\n val bleu epoch: (0.17013697465031893, [0.6209837930054023, 0.2487087802940008, 0.12195121951219512, 0.04448742746615087], 1.0, 1.0129608294930876, 3517, 3472)\t\n max val bleu: 0.17013697465031893\t\n2024-11-13 07:52:01,719 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.9431342226599677\t\n min train loss: 0.9431342226599677\t\n val loss epoch: 1.220763921737671\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.14117272147864882\t\n max train acc: 0.14117272147864882\t\n val acc epoch: 0.112\t\n max val acc: 0.112\t\n val bleu epoch: (0.17013697465031893, [0.6209837930054023, 0.2487087802940008, 0.12195121951219512, 0.04448742746615087], 1.0, 1.0129608294930876, 3517, 3472)\t\n max val bleu: 0.17013697465031893\t\n2024-11-13 07:52:01,719 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.9431342226599677\t\n min train loss: 0.9431342226599677\t\n val loss epoch: 1.220763921737671\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.14117272147864882\t\n max train acc: 0.14117272147864882\t\n val acc epoch: 0.112\t\n max val acc: 0.112\t\n val bleu epoch: (0.17013697465031893, [0.6209837930054023, 0.2487087802940008, 0.12195121951219512, 0.04448742746615087], 1.0, 1.0129608294930876, 3517, 3472)\t\n max val bleu: 0.17013697465031893\t\n2024-11-13 07:52:01,719 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.9431342226599677\t\n min train loss: 0.9431342226599677\t\n val loss epoch: 1.220763921737671\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.14117272147864882\t\n max train acc: 0.14117272147864882\t\n val acc epoch: 0.112\t\n max val acc: 0.112\t\n val bleu epoch: (0.17013697465031893, [0.6209837930054023, 0.2487087802940008, 0.12195121951219512, 0.04448742746615087], 1.0, 1.0129608294930876, 3517, 3472)\t\n max val bleu: 0.17013697465031893\t\n2024-11-13 07:52:01,719 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.9431342226599677\t\n min train loss: 0.9431342226599677\t\n val loss epoch: 1.220763921737671\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.14117272147864882\t\n max train acc: 0.14117272147864882\t\n val acc epoch: 0.112\t\n max val acc: 0.112\t\n val bleu epoch: (0.17013697465031893, [0.6209837930054023, 0.2487087802940008, 0.12195121951219512, 0.04448742746615087], 1.0, 1.0129608294930876, 3517, 3472)\t\n max val bleu: 0.17013697465031893\t\n2024-11-13 07:52:01,723 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n2024-11-13 07:52:01,723 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n2024-11-13 07:52:01,723 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n2024-11-13 07:52:01,723 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n2024-11-13 07:52:01,723 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:52:47,010 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 4 completed...\nTime Taken: 0.7547236124674479\n2024-11-13 07:52:47,010 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 4 completed...\nTime Taken: 0.7547236124674479\n2024-11-13 07:52:47,010 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 4 completed...\nTime Taken: 0.7547236124674479\n2024-11-13 07:52:47,010 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 4 completed...\nTime Taken: 0.7547236124674479\n2024-11-13 07:52:47,010 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 4 completed...\nTime Taken: 0.7547236124674479\n2024-11-13 07:52:47,015 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:52:47,015 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:52:47,015 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:52:47,015 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:52:47,015 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:52:52,824 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17079564693963897\n2024-11-13 07:52:52,824 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17079564693963897\n2024-11-13 07:52:52,824 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17079564693963897\n2024-11-13 07:52:52,824 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17079564693963897\n2024-11-13 07:52:52,824 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.17079564693963897\n2024-11-13 07:52:52,829 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.8040544736203807\t\n min train loss: 0.8040544736203807\t\n val loss epoch: 1.347752332687378\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1755895474824729\t\n max train acc: 0.1755895474824729\t\n val acc epoch: 0.115\t\n max val acc: 0.115\t\n val bleu epoch: (0.17079564693963897, [0.6682570867246702, 0.25595005852516584, 0.127319257837492, 0.03907637655417407], 1.0, 1.0262096774193548, 3563, 3472)\t\n max val bleu: 0.17079564693963897\t\n2024-11-13 07:52:52,829 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.8040544736203807\t\n min train loss: 0.8040544736203807\t\n val loss epoch: 1.347752332687378\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1755895474824729\t\n max train acc: 0.1755895474824729\t\n val acc epoch: 0.115\t\n max val acc: 0.115\t\n val bleu epoch: (0.17079564693963897, [0.6682570867246702, 0.25595005852516584, 0.127319257837492, 0.03907637655417407], 1.0, 1.0262096774193548, 3563, 3472)\t\n max val bleu: 0.17079564693963897\t\n2024-11-13 07:52:52,829 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.8040544736203807\t\n min train loss: 0.8040544736203807\t\n val loss epoch: 1.347752332687378\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1755895474824729\t\n max train acc: 0.1755895474824729\t\n val acc epoch: 0.115\t\n max val acc: 0.115\t\n val bleu epoch: (0.17079564693963897, [0.6682570867246702, 0.25595005852516584, 0.127319257837492, 0.03907637655417407], 1.0, 1.0262096774193548, 3563, 3472)\t\n max val bleu: 0.17079564693963897\t\n2024-11-13 07:52:52,829 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.8040544736203807\t\n min train loss: 0.8040544736203807\t\n val loss epoch: 1.347752332687378\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1755895474824729\t\n max train acc: 0.1755895474824729\t\n val acc epoch: 0.115\t\n max val acc: 0.115\t\n val bleu epoch: (0.17079564693963897, [0.6682570867246702, 0.25595005852516584, 0.127319257837492, 0.03907637655417407], 1.0, 1.0262096774193548, 3563, 3472)\t\n max val bleu: 0.17079564693963897\t\n2024-11-13 07:52:52,829 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.8040544736203807\t\n min train loss: 0.8040544736203807\t\n val loss epoch: 1.347752332687378\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1755895474824729\t\n max train acc: 0.1755895474824729\t\n val acc epoch: 0.115\t\n max val acc: 0.115\t\n val bleu epoch: (0.17079564693963897, [0.6682570867246702, 0.25595005852516584, 0.127319257837492, 0.03907637655417407], 1.0, 1.0262096774193548, 3563, 3472)\t\n max val bleu: 0.17079564693963897\t\n2024-11-13 07:52:52,833 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n2024-11-13 07:52:52,833 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n2024-11-13 07:52:52,833 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n2024-11-13 07:52:52,833 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n2024-11-13 07:52:52,833 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:53:37,818 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 5 completed...\nTime Taken: 0.7496691465377807\n2024-11-13 07:53:37,818 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 5 completed...\nTime Taken: 0.7496691465377807\n2024-11-13 07:53:37,818 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 5 completed...\nTime Taken: 0.7496691465377807\n2024-11-13 07:53:37,818 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 5 completed...\nTime Taken: 0.7496691465377807\n2024-11-13 07:53:37,818 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 5 completed...\nTime Taken: 0.7496691465377807\n2024-11-13 07:53:37,823 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:53:37,823 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:53:37,823 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:53:37,823 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:53:37,823 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:53:43,708 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.7371042163339767\t\n min train loss: 0.7371042163339767\t\n val loss epoch: 1.4336364269256592\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.18132568514977693\t\n max train acc: 0.18132568514977693\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19183057805954024, [0.6684952978056427, 0.2662659123055163, 0.13402625820568928, 0.05676328502415459], 1.0, 1.1025345622119815, 3828, 3472)\t\n max val bleu: 0.19183057805954024\t\n2024-11-13 07:53:43,708 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.7371042163339767\t\n min train loss: 0.7371042163339767\t\n val loss epoch: 1.4336364269256592\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.18132568514977693\t\n max train acc: 0.18132568514977693\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19183057805954024, [0.6684952978056427, 0.2662659123055163, 0.13402625820568928, 0.05676328502415459], 1.0, 1.1025345622119815, 3828, 3472)\t\n max val bleu: 0.19183057805954024\t\n2024-11-13 07:53:43,708 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.7371042163339767\t\n min train loss: 0.7371042163339767\t\n val loss epoch: 1.4336364269256592\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.18132568514977693\t\n max train acc: 0.18132568514977693\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19183057805954024, [0.6684952978056427, 0.2662659123055163, 0.13402625820568928, 0.05676328502415459], 1.0, 1.1025345622119815, 3828, 3472)\t\n max val bleu: 0.19183057805954024\t\n2024-11-13 07:53:43,708 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.7371042163339767\t\n min train loss: 0.7371042163339767\t\n val loss epoch: 1.4336364269256592\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.18132568514977693\t\n max train acc: 0.18132568514977693\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19183057805954024, [0.6684952978056427, 0.2662659123055163, 0.13402625820568928, 0.05676328502415459], 1.0, 1.1025345622119815, 3828, 3472)\t\n max val bleu: 0.19183057805954024\t\n2024-11-13 07:53:43,708 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.7371042163339767\t\n min train loss: 0.7371042163339767\t\n val loss epoch: 1.4336364269256592\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.18132568514977693\t\n max train acc: 0.18132568514977693\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19183057805954024, [0.6684952978056427, 0.2662659123055163, 0.13402625820568928, 0.05676328502415459], 1.0, 1.1025345622119815, 3828, 3472)\t\n max val bleu: 0.19183057805954024\t\n2024-11-13 07:53:43,713 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n2024-11-13 07:53:43,713 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n2024-11-13 07:53:43,713 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n2024-11-13 07:53:43,713 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n2024-11-13 07:53:43,713 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:54:28,896 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 6 completed...\nTime Taken: 0.7529789725939433\n2024-11-13 07:54:28,896 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 6 completed...\nTime Taken: 0.7529789725939433\n2024-11-13 07:54:28,896 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 6 completed...\nTime Taken: 0.7529789725939433\n2024-11-13 07:54:28,896 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 6 completed...\nTime Taken: 0.7529789725939433\n2024-11-13 07:54:28,896 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 6 completed...\nTime Taken: 0.7529789725939433\n2024-11-13 07:54:28,900 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:54:28,900 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:54:28,900 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:54:28,900 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:54:28,900 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:54:34,743 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.6801659926444343\t\n min train loss: 0.6801659926444343\t\n val loss epoch: 1.381201982498169\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1988527724665392\t\n max train acc: 0.1988527724665392\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19786575289419225, [0.6805034815211569, 0.2757863935625457, 0.13321799307958476, 0.06130790190735695], 1.0, 1.0754608294930876, 3734, 3472)\t\n max val bleu: 0.19786575289419225\t\n2024-11-13 07:54:34,743 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.6801659926444343\t\n min train loss: 0.6801659926444343\t\n val loss epoch: 1.381201982498169\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1988527724665392\t\n max train acc: 0.1988527724665392\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19786575289419225, [0.6805034815211569, 0.2757863935625457, 0.13321799307958476, 0.06130790190735695], 1.0, 1.0754608294930876, 3734, 3472)\t\n max val bleu: 0.19786575289419225\t\n2024-11-13 07:54:34,743 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.6801659926444343\t\n min train loss: 0.6801659926444343\t\n val loss epoch: 1.381201982498169\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1988527724665392\t\n max train acc: 0.1988527724665392\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19786575289419225, [0.6805034815211569, 0.2757863935625457, 0.13321799307958476, 0.06130790190735695], 1.0, 1.0754608294930876, 3734, 3472)\t\n max val bleu: 0.19786575289419225\t\n2024-11-13 07:54:34,743 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.6801659926444343\t\n min train loss: 0.6801659926444343\t\n val loss epoch: 1.381201982498169\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1988527724665392\t\n max train acc: 0.1988527724665392\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19786575289419225, [0.6805034815211569, 0.2757863935625457, 0.13321799307958476, 0.06130790190735695], 1.0, 1.0754608294930876, 3734, 3472)\t\n max val bleu: 0.19786575289419225\t\n2024-11-13 07:54:34,743 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.6801659926444343\t\n min train loss: 0.6801659926444343\t\n val loss epoch: 1.381201982498169\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.1988527724665392\t\n max train acc: 0.1988527724665392\t\n val acc epoch: 0.111\t\n max val acc: 0.115\t\n val bleu epoch: (0.19786575289419225, [0.6805034815211569, 0.2757863935625457, 0.13321799307958476, 0.06130790190735695], 1.0, 1.0754608294930876, 3734, 3472)\t\n max val bleu: 0.19786575289419225\t\n2024-11-13 07:54:34,747 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n2024-11-13 07:54:34,747 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n2024-11-13 07:54:34,747 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n2024-11-13 07:54:34,747 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n2024-11-13 07:54:34,747 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:55:19,843 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 7 completed...\nTime Taken: 0.7514898339907329\n2024-11-13 07:55:19,843 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 7 completed...\nTime Taken: 0.7514898339907329\n2024-11-13 07:55:19,843 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 7 completed...\nTime Taken: 0.7514898339907329\n2024-11-13 07:55:19,843 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 7 completed...\nTime Taken: 0.7514898339907329\n2024-11-13 07:55:19,843 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 7 completed...\nTime Taken: 0.7514898339907329\n2024-11-13 07:55:19,847 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:55:19,847 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:55:19,847 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:55:19,847 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:55:19,847 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:55:25,660 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.21332353411658372\n2024-11-13 07:55:25,660 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.21332353411658372\n2024-11-13 07:55:25,660 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.21332353411658372\n2024-11-13 07:55:25,660 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.21332353411658372\n2024-11-13 07:55:25,660 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.21332353411658372\n2024-11-13 07:55:25,664 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.632317229515405\t\n min train loss: 0.632317229515405\t\n val loss epoch: 1.4509685039520264\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.22020395156150413\t\n max train acc: 0.22020395156150413\t\n val acc epoch: 0.125\t\n max val acc: 0.125\t\n val bleu epoch: (0.21332353411658372, [0.6979253112033195, 0.2910133843212237, 0.15294117647058825, 0.06666666666666667], 1.0, 1.0411866359447004, 3615, 3472)\t\n max val bleu: 0.21332353411658372\t\n2024-11-13 07:55:25,664 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.632317229515405\t\n min train loss: 0.632317229515405\t\n val loss epoch: 1.4509685039520264\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.22020395156150413\t\n max train acc: 0.22020395156150413\t\n val acc epoch: 0.125\t\n max val acc: 0.125\t\n val bleu epoch: (0.21332353411658372, [0.6979253112033195, 0.2910133843212237, 0.15294117647058825, 0.06666666666666667], 1.0, 1.0411866359447004, 3615, 3472)\t\n max val bleu: 0.21332353411658372\t\n2024-11-13 07:55:25,664 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.632317229515405\t\n min train loss: 0.632317229515405\t\n val loss epoch: 1.4509685039520264\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.22020395156150413\t\n max train acc: 0.22020395156150413\t\n val acc epoch: 0.125\t\n max val acc: 0.125\t\n val bleu epoch: (0.21332353411658372, [0.6979253112033195, 0.2910133843212237, 0.15294117647058825, 0.06666666666666667], 1.0, 1.0411866359447004, 3615, 3472)\t\n max val bleu: 0.21332353411658372\t\n2024-11-13 07:55:25,664 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.632317229515405\t\n min train loss: 0.632317229515405\t\n val loss epoch: 1.4509685039520264\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.22020395156150413\t\n max train acc: 0.22020395156150413\t\n val acc epoch: 0.125\t\n max val acc: 0.125\t\n val bleu epoch: (0.21332353411658372, [0.6979253112033195, 0.2910133843212237, 0.15294117647058825, 0.06666666666666667], 1.0, 1.0411866359447004, 3615, 3472)\t\n max val bleu: 0.21332353411658372\t\n2024-11-13 07:55:25,664 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.632317229515405\t\n min train loss: 0.632317229515405\t\n val loss epoch: 1.4509685039520264\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.22020395156150413\t\n max train acc: 0.22020395156150413\t\n val acc epoch: 0.125\t\n max val acc: 0.125\t\n val bleu epoch: (0.21332353411658372, [0.6979253112033195, 0.2910133843212237, 0.15294117647058825, 0.06666666666666667], 1.0, 1.0411866359447004, 3615, 3472)\t\n max val bleu: 0.21332353411658372\t\n2024-11-13 07:55:25,668 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n2024-11-13 07:55:25,668 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n2024-11-13 07:55:25,668 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n2024-11-13 07:55:25,668 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n2024-11-13 07:55:25,668 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:56:10,821 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 8 completed...\nTime Taken: 0.7525042851765951\n2024-11-13 07:56:10,821 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 8 completed...\nTime Taken: 0.7525042851765951\n2024-11-13 07:56:10,821 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 8 completed...\nTime Taken: 0.7525042851765951\n2024-11-13 07:56:10,821 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 8 completed...\nTime Taken: 0.7525042851765951\n2024-11-13 07:56:10,821 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 8 completed...\nTime Taken: 0.7525042851765951\n2024-11-13 07:56:10,826 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:56:10,826 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:56:10,826 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:56:10,826 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:56:10,826 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:56:16,619 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.22385528009206107\n2024-11-13 07:56:16,619 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.22385528009206107\n2024-11-13 07:56:16,619 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.22385528009206107\n2024-11-13 07:56:16,619 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.22385528009206107\n2024-11-13 07:56:16,619 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.22385528009206107\n2024-11-13 07:56:16,625 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.5804588939736139\t\n min train loss: 0.5804588939736139\t\n val loss epoch: 1.5635582208633423\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.2995538559592097\t\n max train acc: 0.2995538559592097\t\n val acc epoch: 0.142\t\n max val acc: 0.142\t\n val bleu epoch: (0.22385528009206107, [0.6924101198402131, 0.29509981851179673, 0.15726495726495726, 0.0781456953642384], 1.0, 1.0815092165898617, 3755, 3472)\t\n max val bleu: 0.22385528009206107\t\n2024-11-13 07:56:16,625 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.5804588939736139\t\n min train loss: 0.5804588939736139\t\n val loss epoch: 1.5635582208633423\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.2995538559592097\t\n max train acc: 0.2995538559592097\t\n val acc epoch: 0.142\t\n max val acc: 0.142\t\n val bleu epoch: (0.22385528009206107, [0.6924101198402131, 0.29509981851179673, 0.15726495726495726, 0.0781456953642384], 1.0, 1.0815092165898617, 3755, 3472)\t\n max val bleu: 0.22385528009206107\t\n2024-11-13 07:56:16,625 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.5804588939736139\t\n min train loss: 0.5804588939736139\t\n val loss epoch: 1.5635582208633423\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.2995538559592097\t\n max train acc: 0.2995538559592097\t\n val acc epoch: 0.142\t\n max val acc: 0.142\t\n val bleu epoch: (0.22385528009206107, [0.6924101198402131, 0.29509981851179673, 0.15726495726495726, 0.0781456953642384], 1.0, 1.0815092165898617, 3755, 3472)\t\n max val bleu: 0.22385528009206107\t\n2024-11-13 07:56:16,625 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.5804588939736139\t\n min train loss: 0.5804588939736139\t\n val loss epoch: 1.5635582208633423\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.2995538559592097\t\n max train acc: 0.2995538559592097\t\n val acc epoch: 0.142\t\n max val acc: 0.142\t\n val bleu epoch: (0.22385528009206107, [0.6924101198402131, 0.29509981851179673, 0.15726495726495726, 0.0781456953642384], 1.0, 1.0815092165898617, 3755, 3472)\t\n max val bleu: 0.22385528009206107\t\n2024-11-13 07:56:16,625 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.5804588939736139\t\n min train loss: 0.5804588939736139\t\n val loss epoch: 1.5635582208633423\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.2995538559592097\t\n max train acc: 0.2995538559592097\t\n val acc epoch: 0.142\t\n max val acc: 0.142\t\n val bleu epoch: (0.22385528009206107, [0.6924101198402131, 0.29509981851179673, 0.15726495726495726, 0.0781456953642384], 1.0, 1.0815092165898617, 3755, 3472)\t\n max val bleu: 0.22385528009206107\t\n2024-11-13 07:56:16,629 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n2024-11-13 07:56:16,629 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n2024-11-13 07:56:16,629 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n2024-11-13 07:56:16,629 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n2024-11-13 07:56:16,629 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:57:01,982 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 9 completed...\nTime Taken: 0.755810538927714\n2024-11-13 07:57:01,982 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 9 completed...\nTime Taken: 0.755810538927714\n2024-11-13 07:57:01,982 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 9 completed...\nTime Taken: 0.755810538927714\n2024-11-13 07:57:01,982 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 9 completed...\nTime Taken: 0.755810538927714\n2024-11-13 07:57:01,982 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 9 completed...\nTime Taken: 0.755810538927714\n2024-11-13 07:57:01,986 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:57:01,986 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:57:01,986 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:57:01,986 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:57:01,986 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:57:07,863 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2259107508225161\n2024-11-13 07:57:07,863 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2259107508225161\n2024-11-13 07:57:07,863 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2259107508225161\n2024-11-13 07:57:07,863 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2259107508225161\n2024-11-13 07:57:07,863 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2259107508225161\n2024-11-13 07:57:07,868 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.5245336416913368\t\n min train loss: 0.5245336416913368\t\n val loss epoch: 1.6859229803085327\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.3467176545570427\t\n max train acc: 0.3467176545570427\t\n val acc epoch: 0.156\t\n max val acc: 0.156\t\n val bleu epoch: (0.2259107508225161, [0.7017721518987342, 0.2952542372881356, 0.1635897435897436, 0.07684210526315789], 1.0, 1.137672811059908, 3950, 3472)\t\n max val bleu: 0.2259107508225161\t\n2024-11-13 07:57:07,868 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.5245336416913368\t\n min train loss: 0.5245336416913368\t\n val loss epoch: 1.6859229803085327\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.3467176545570427\t\n max train acc: 0.3467176545570427\t\n val acc epoch: 0.156\t\n max val acc: 0.156\t\n val bleu epoch: (0.2259107508225161, [0.7017721518987342, 0.2952542372881356, 0.1635897435897436, 0.07684210526315789], 1.0, 1.137672811059908, 3950, 3472)\t\n max val bleu: 0.2259107508225161\t\n2024-11-13 07:57:07,868 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.5245336416913368\t\n min train loss: 0.5245336416913368\t\n val loss epoch: 1.6859229803085327\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.3467176545570427\t\n max train acc: 0.3467176545570427\t\n val acc epoch: 0.156\t\n max val acc: 0.156\t\n val bleu epoch: (0.2259107508225161, [0.7017721518987342, 0.2952542372881356, 0.1635897435897436, 0.07684210526315789], 1.0, 1.137672811059908, 3950, 3472)\t\n max val bleu: 0.2259107508225161\t\n2024-11-13 07:57:07,868 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.5245336416913368\t\n min train loss: 0.5245336416913368\t\n val loss epoch: 1.6859229803085327\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.3467176545570427\t\n max train acc: 0.3467176545570427\t\n val acc epoch: 0.156\t\n max val acc: 0.156\t\n val bleu epoch: (0.2259107508225161, [0.7017721518987342, 0.2952542372881356, 0.1635897435897436, 0.07684210526315789], 1.0, 1.137672811059908, 3950, 3472)\t\n max val bleu: 0.2259107508225161\t\n2024-11-13 07:57:07,868 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.5245336416913368\t\n min train loss: 0.5245336416913368\t\n val loss epoch: 1.6859229803085327\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.3467176545570427\t\n max train acc: 0.3467176545570427\t\n val acc epoch: 0.156\t\n max val acc: 0.156\t\n val bleu epoch: (0.2259107508225161, [0.7017721518987342, 0.2952542372881356, 0.1635897435897436, 0.07684210526315789], 1.0, 1.137672811059908, 3950, 3472)\t\n max val bleu: 0.2259107508225161\t\n2024-11-13 07:57:07,871 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n2024-11-13 07:57:07,871 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n2024-11-13 07:57:07,871 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n2024-11-13 07:57:07,871 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n2024-11-13 07:57:07,871 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 198 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:57:52,917 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 10 completed...\nTime Taken: 0.7507085204124451\n2024-11-13 07:57:52,917 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 10 completed...\nTime Taken: 0.7507085204124451\n2024-11-13 07:57:52,917 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 10 completed...\nTime Taken: 0.7507085204124451\n2024-11-13 07:57:52,917 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 10 completed...\nTime Taken: 0.7507085204124451\n2024-11-13 07:57:52,917 | DEBUG | 477948701.py: 369 : train_model() ::\t Training for epoch 10 completed...\nTime Taken: 0.7507085204124451\n2024-11-13 07:57:52,921 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:57:52,921 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:57:52,921 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:57:52,921 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n2024-11-13 07:57:52,921 | DEBUG | 477948701.py: 370 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 64 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-13 07:57:58,750 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2386855797201566\n2024-11-13 07:57:58,750 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2386855797201566\n2024-11-13 07:57:58,750 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2386855797201566\n2024-11-13 07:57:58,750 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2386855797201566\n2024-11-13 07:57:58,750 | DEBUG | 477948701.py: 429 : train_model() ::\t Validation Bleu: 0.2386855797201566\n2024-11-13 07:57:58,755 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.4673842300416651\t\n min train loss: 0.4673842300416651\t\n val loss epoch: 1.548175573348999\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.4181007010834927\t\n max train acc: 0.4181007010834927\t\n val acc epoch: 0.164\t\n max val acc: 0.164\t\n val bleu epoch: (0.2386855797201566, [0.7120042587170615, 0.30758070366340223, 0.1752988047808765, 0.0845442536327609], 1.0, 1.0820852534562213, 3757, 3472)\t\n max val bleu: 0.2386855797201566\t\n2024-11-13 07:57:58,755 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.4673842300416651\t\n min train loss: 0.4673842300416651\t\n val loss epoch: 1.548175573348999\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.4181007010834927\t\n max train acc: 0.4181007010834927\t\n val acc epoch: 0.164\t\n max val acc: 0.164\t\n val bleu epoch: (0.2386855797201566, [0.7120042587170615, 0.30758070366340223, 0.1752988047808765, 0.0845442536327609], 1.0, 1.0820852534562213, 3757, 3472)\t\n max val bleu: 0.2386855797201566\t\n2024-11-13 07:57:58,755 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.4673842300416651\t\n min train loss: 0.4673842300416651\t\n val loss epoch: 1.548175573348999\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.4181007010834927\t\n max train acc: 0.4181007010834927\t\n val acc epoch: 0.164\t\n max val acc: 0.164\t\n val bleu epoch: (0.2386855797201566, [0.7120042587170615, 0.30758070366340223, 0.1752988047808765, 0.0845442536327609], 1.0, 1.0820852534562213, 3757, 3472)\t\n max val bleu: 0.2386855797201566\t\n2024-11-13 07:57:58,755 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.4673842300416651\t\n min train loss: 0.4673842300416651\t\n val loss epoch: 1.548175573348999\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.4181007010834927\t\n max train acc: 0.4181007010834927\t\n val acc epoch: 0.164\t\n max val acc: 0.164\t\n val bleu epoch: (0.2386855797201566, [0.7120042587170615, 0.30758070366340223, 0.1752988047808765, 0.0845442536327609], 1.0, 1.0820852534562213, 3757, 3472)\t\n max val bleu: 0.2386855797201566\t\n2024-11-13 07:57:58,755 | INFO | 1857244066.py: 424 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.4673842300416651\t\n min train loss: 0.4673842300416651\t\n val loss epoch: 1.548175573348999\t\n min val loss: 1.220763921737671\t\n train acc epoch: 0.4181007010834927\t\n max train acc: 0.4181007010834927\t\n val acc epoch: 0.164\t\n max val acc: 0.164\t\n val bleu epoch: (0.2386855797201566, [0.7120042587170615, 0.30758070366340223, 0.1752988047808765, 0.0845442536327609], 1.0, 1.0820852534562213, 3757, 3472)\t\n max val bleu: 0.2386855797201566\t\n2024-11-13 07:57:58,759 | INFO | 477948701.py: 468 : train_model() ::\t Training Completed for 10 epochs\n2024-11-13 07:57:58,759 | INFO | 477948701.py: 468 : train_model() ::\t Training Completed for 10 epochs\n2024-11-13 07:57:58,759 | INFO | 477948701.py: 468 : train_model() ::\t Training Completed for 10 epochs\n2024-11-13 07:57:58,759 | INFO | 477948701.py: 468 : train_model() ::\t Training Completed for 10 epochs\n2024-11-13 07:57:58,759 | INFO | 477948701.py: 468 : train_model() ::\t Training Completed for 10 epochs\n","output_type":"stream"},{"name":"stdout","text":"--Return--\nNone\n> \u001b[0;32m/tmp/ipykernel_30/1857244066.py\u001b[0m(469)\u001b[0;36mstore_results\u001b[0;34m()\u001b[0m\n\u001b[0;32m    467 \u001b[0;31m                        \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 469 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mstore_val_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  exit\n"}],"execution_count":38},{"cell_type":"code","source":"# # Custom input problem\n# custom_problem = [\"John has 5 apples. John gave 3 to Mary. How many apples does John have now?\"]\n\n# run_name = config.run_name\n# config.log_path = os.path.join(log_folder, run_name)\n# config.model_path = os.path.join(model_folder, run_name)\n# config.board_path = os.path.join(board_path, run_name)\n# config.outputs_path = os.path.join(outputs_folder, run_name)\n\n# vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n# vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n# config_file = os.path.join(config.model_path, 'config.p')\n# log_file = os.path.join(config.log_path, 'log.txt')\n    \n# with open(vocab1_path, 'rb') as f:\n# \tvoc1 = pickle.load(f)\n# with open(vocab2_path, 'rb') as f:\n# \tvoc2 = pickle.load(f)\n\n# device = gpu_init_pytorch(config.gpu)\n\n# # Convert input problem to indices using your vocabulary (voc1)\n# input_problem = sents_to_idx(voc1, custom_problem, config.max_length)\n\n# def process_batch(sent1s, voc1, device):\n# \tinput_len1 = [len(s) for s in sent1s]\n# \tmax_length_1 = max(input_len1)\n\n# \tsent1s_padded = [pad_seq(s, max_length_1, voc1) for s in sent1s]\n\n# \t# Convert to [Max_len X Batch]\n# \tsent1_var = Variable(torch.LongTensor(sent1s_padded)).transpose(0, 1)\n\n# \tsent1_var = sent1_var.to(device)\n\n# \treturn sent1_var, input_len1\n\n# # Process the batch for input\n# sent1_var, input_len1 = process_batch(input_problem, voc1, device)\n\n# # Generate the predicted output using greedy decoding\n# decoder_output = model.greedy_decode(custom_problem, sent1_var, None, input_len1, None)\n\n# # Print the generated equation\n# print(\"Generated Equation: \", ' '.join(decoder_output[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:58:04.548870Z","iopub.status.idle":"2024-11-13T07:58:04.549243Z","shell.execute_reply.started":"2024-11-13T07:58:04.549062Z","shell.execute_reply":"2024-11-13T07:58:04.549081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_full_question(question, numbers):\n    for i, num in enumerate(numbers):\n        placeholder = f\"number{i}\"\n        question = question.replace(placeholder, str(num))\n    return question\n\ndef convert_eqn(equation, numbers):\n    for i, num in enumerate(numbers):\n        placeholder = f\"number{i}\"\n        equation = equation.replace(placeholder, str(num))\n    return equation\n\n\n# Function to write evaluation results into a file\ndef write_to_file(filename, data):\n    with open(filename, 'w') as f:\n        for line in data:\n            f.write(line + '\\n')\n\n# Loop over the validation data and collect output for file\noutput_lines = []\nfor data in val_dataloader:\n    # Convert questions and equations to index representations\n    sent1s = sents_to_idx(voc1, data['ques'], config.max_length)\n    sent2s = sents_to_idx(voc2, data['eqn'], config.max_length)\n    nums = data['nums']\n    ans = data['ans']\n    \n    # Prepare data for the model\n    ques = data['ques']\n    sent1_var, sent2_var, input_len1, input_len2 = process_batch(sent1s, sent2s, voc1, voc2, device)\n    \n    # Perform decoding\n    val_loss, decoder_output, decoder_attn = model.greedy_decode(\n        ques, sent1_var, sent2_var, input_len1, input_len2, validation=True\n    )\n    \n    # Iterate over each entry in the batch and collect the required information\n    for i in range(len(ques)):\n        # Retrieve question, expected equation, numbers, and decoder output\n        question = ques[i]\n        expected_eqn = data['eqn'][i]\n        decoded_eqn = ' '.join(decoder_output[i])  # Convert list to string format\n        numbers = list(map(int, nums[i].split()))\n        true_answer = ans[i].item()\n\n        # Convert the equation tokens and evaluate the decoded answer\n        op = stack_to_string(decoder_output[i])\n        num = [float(nu) for nu in nums[i].split()]\n        pred = ans_evaluator(op, num)\n        \n        # Generate the converted question and equations\n        converted_question = generate_full_question(question, numbers)\n        converted_expected_eqn = convert_eqn(expected_eqn, numbers)\n        converted_decoded_eqn = convert_eqn(decoded_eqn, numbers)\n\n        # Compare decoded answer with true answer\n        result_comparison = \"Correct\" if abs(pred - true_answer) <= 0.1 else \"Incorrect\"\n\n        # Prepare output for file\n        # output_lines.append(f\"Question {i+1}: {question}\")\n        output_lines.append(f\"Converted Question {i+1}: {converted_question}\")\n        # output_lines.append(f\"Expected Equation: {expected_eqn}\")\n        # output_lines.append(f\"Converted Expected Equation: {converted_expected_eqn}\")\n        # output_lines.append(f\"Decoded Equation: {decoded_eqn}\")\n        # output_lines.append(f\"Converted Decoded Equation: {converted_decoded_eqn}\")\n        # output_lines.append(f\"Numbers: {numbers}\")\n        output_lines.append(f\"True Answer: {true_answer}\")\n        output_lines.append(f\"Decoded Answer: {pred}\")\n        output_lines.append(f\"Predicted Result: {result_comparison}\")\n        output_lines.append(\"-\" * 80)\n\n\n# Write all collected lines to eval.txt\nwrite_to_file(\"rnn_eval_bert.txt\", output_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:59:02.262656Z","iopub.execute_input":"2024-11-13T07:59:02.263072Z","iopub.status.idle":"2024-11-13T07:59:07.308445Z","shell.execute_reply.started":"2024-11-13T07:59:02.263033Z","shell.execute_reply":"2024-11-13T07:59:07.307281Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import torch\ntorch.save(model, '/kaggle/working/entire_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:58:04.553305Z","iopub.status.idle":"2024-11-13T07:58:04.553690Z","shell.execute_reply.started":"2024-11-13T07:58:04.553506Z","shell.execute_reply":"2024-11-13T07:58:04.553529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/kaggle_working_dir.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:58:04.555638Z","iopub.status.idle":"2024-11-13T07:58:04.556382Z","shell.execute_reply.started":"2024-11-13T07:58:04.556116Z","shell.execute_reply":"2024-11-13T07:58:04.556143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'/kaggle/working/kaggle_working_dir.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T07:58:04.557614Z","iopub.status.idle":"2024-11-13T07:58:04.558092Z","shell.execute_reply.started":"2024-11-13T07:58:04.557840Z","shell.execute_reply":"2024-11-13T07:58:04.557865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}