{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9912417,"sourceType":"datasetVersion","datasetId":6090767}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformers","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport sys\nimport pdb\nimport torch.nn as nn\nimport torch\nimport math\nimport collections\nimport logging\nimport json\nimport pandas as pd\n\nfrom transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\nfrom sympy import Eq, solve\nfrom sympy.parsing.sympy_parser import parse_expr\nimport sympy as sp\n\nfrom glob import glob\nfrom torch.autograd import Variable\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport argparse\n\nimport re\nfrom torch.utils.data import Dataset\nimport unicodedata\nfrom collections import OrderedDict\n\nimport random\nfrom torch.utils.data import DataLoader\nfrom tensorboardX import SummaryWriter\ntry:\n\timport cPickle as pickle\nexcept ImportError:\n\timport pickle\n\n    \nfrom time import time\nfrom torch import optim\nimport torch.nn.functional as F\nfrom transformers import AdamW\n# from pytorch_pretrained_bert.optimization import BertAdam\n# from tensorboardX import SummaryWriter\nfrom gensim import models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:55:02.643834Z","iopub.execute_input":"2024-11-17T15:55:02.644893Z","iopub.status.idle":"2024-11-17T15:55:14.629647Z","shell.execute_reply.started":"2024-11-17T15:55:02.644813Z","shell.execute_reply":"2024-11-17T15:55:14.628836Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Components","metadata":{}},{"cell_type":"code","source":"########################################################\n# contextual_embeddings.py #\n########################################################\n\nclass BertEncoder(nn.Module):\n\tdef __init__(self, bert_model = 'bert-base-uncased',device = 'cuda:0 ', freeze_bert = False):\n\t\tsuper(BertEncoder, self).__init__()\n\t\tself.bert_layer = BertModel.from_pretrained(bert_model)\n\t\tself.bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n\t\tself.device = device\n\t\t\n\t\tif freeze_bert:\n\t\t\tfor p in self.bert_layer.parameters():\n\t\t\t\tp.requires_grad = False\n\t\t\n\tdef bertify_input(self, sentences):\n\t\t'''\n\t\tPreprocess the input sentences using bert tokenizer and converts them to a torch tensor containing token ids\n\t\t\n\t\tArgs:\n\t\t\tsentences (list): source sentences\n\t\tReturns:\n\t\t\ttoken_ids (tensor): tokenized sentences | size: [BS x S]\n\t\t\tattn_masks (tensor): masks padded indices | size: [BS x S]\n\t\t\tinput_lengths (list): lengths of sentences | size: [BS]\n\t\t'''\n\n\t\t# Tokenize the input sentences for feeding into BERT\n\t\tall_tokens  = [['[CLS]'] + self.bert_tokenizer.tokenize(sentence) + ['[SEP]'] for sentence in sentences]\n\t\t\n\t\t# Pad all the sentences to a maximum length\n\t\tinput_lengths = [len(tokens) for tokens in all_tokens]\n\t\tmax_length    = max(input_lengths)\n\t\tpadded_tokens = [tokens + ['[PAD]' for _ in range(max_length - len(tokens))] for tokens in all_tokens]\n\n\t\t# Convert tokens to token ids\n\t\ttoken_ids = torch.tensor([self.bert_tokenizer.convert_tokens_to_ids(tokens) for tokens in padded_tokens]).to(self.device)\n\n\t\t# Obtain attention masks\n\t\tpad_token = self.bert_tokenizer.convert_tokens_to_ids('[PAD]')\n\t\tattn_masks = (token_ids != pad_token).long()\n\n\t\treturn token_ids, attn_masks, input_lengths\n\n\tdef forward(self, sentences):\n\t\t'''\n\t\tFeed the batch of sentences to a BERT encoder to obtain contextualized representations of each token\n\t\t\n\t\tArgs:\n\t\t\tsentences (list): source sentences\n\t\tReturns:\n\t\t\tcont_reps (tensor): BERT Embeddings | size: [BS x S x d_model]\n\t\t\ttoken_ids (tensor): tokenized sentences | size: [BS x S]\n\t\t'''\n\n\t\t# Preprocess sentences\n\t\ttoken_ids, attn_masks, input_lengths = self.bertify_input(sentences)\n\n\t\t# Feed through bert\n\t\tcont_reps, _ = self.bert_layer(token_ids, attention_mask = attn_masks)\n\n\t\treturn cont_reps, token_ids\n\nclass RobertaEncoder(nn.Module):\n\tdef __init__(self, roberta_model = 'roberta-base', device = 'cuda:0 ', freeze_roberta = False):\n\t\tsuper(RobertaEncoder, self).__init__()\n\t\tself.roberta_layer = RobertaModel.from_pretrained(roberta_model)\n\t\tself.roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model)\n\t\tself.device = device\n\t\t\n\t\tif freeze_roberta:\n\t\t\tfor p in self.roberta_layer.parameters():\n\t\t\t\tp.requires_grad = False\n\t\t\n\tdef robertify_input(self, sentences):\n\t\t'''\n\t\tPreprocess the input sentences using roberta tokenizer and converts them to a torch tensor containing token ids\n\t\t\n\t\tArgs:\n\t\t\tsentences (list): source sentences\n\t\tReturns:\n\t\t\ttoken_ids (tensor): tokenized sentences | size: [BS x S]\n\t\t\tattn_masks (tensor): masks padded indices | size: [BS x S]\n\t\t\tinput_lengths (list): lengths of sentences | size: [BS]\n\t\t'''\n\n\t\t# Tokenize the input sentences for feeding into RoBERTa\n\t\tall_tokens  = [['<s>'] + self.roberta_tokenizer.tokenize(sentence) + ['</s>'] for sentence in sentences]\n\t\t\n\t\t# Pad all the sentences to a maximum length\n\t\tinput_lengths = [len(tokens) for tokens in all_tokens]\n\t\tmax_length    = max(input_lengths)\n\t\tpadded_tokens = [tokens + ['<pad>' for _ in range(max_length - len(tokens))] for tokens in all_tokens]\n\n\t\t# Convert tokens to token ids\n\t\ttoken_ids = torch.tensor([self.roberta_tokenizer.convert_tokens_to_ids(tokens) for tokens in padded_tokens]).to(self.device)\n\n\t\t# Obtain attention masks\n\t\tpad_token = self.roberta_tokenizer.convert_tokens_to_ids('<pad>')\n\t\tattn_masks = (token_ids != pad_token).long()\n\n\t\treturn token_ids, attn_masks, input_lengths\n\n\tdef forward(self, sentences):\n\t\t'''\n\t\tFeed the batch of sentences to a RoBERTa encoder to obtain contextualized representations of each token\n\t\t'''\n\t\t# Preprocess sentences\n\t\ttoken_ids, attn_masks, input_lengths = self.robertify_input(sentences)\n\n\t\t# Feed through RoBERTa\n\t\toutput = self.roberta_layer(token_ids, attention_mask = attn_masks)\n        \n\t\tcont_reps = output.last_hidden_state\n\n\t\treturn cont_reps, token_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:55:14.631330Z","iopub.execute_input":"2024-11-17T15:55:14.631807Z","iopub.status.idle":"2024-11-17T15:55:14.650646Z","shell.execute_reply.started":"2024-11-17T15:55:14.631774Z","shell.execute_reply":"2024-11-17T15:55:14.649785Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## utils","metadata":{}},{"cell_type":"code","source":"########################################################\n# bleu.py #\n########################################################\n\ndef _get_ngrams(segment, max_order):\n    ngram_counts = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i+order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 smooth=False):\n \n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n    reference_length = 0\n    translation_length = 0\n    for (references, translation) in zip(reference_corpus,\n                                       translation_corpus):\n        reference_length += min(len(r) for r in references)\n        translation_length += len(translation)\n\n        merged_ref_ngram_counts = collections.Counter()\n        for reference in references:\n            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n        translation_ngram_counts = _get_ngrams(translation, max_order)\n        overlap = translation_ngram_counts & merged_ref_ngram_counts\n        for ngram in overlap:\n            matches_by_order[len(ngram)-1] += overlap[ngram]\n        for order in range(1, max_order+1):\n            possible_matches = len(translation) - order + 1\n            if possible_matches > 0:\n                possible_matches_by_order[order-1] += possible_matches\n\n    precisions = [0] * max_order\n    for i in range(0, max_order):\n        if smooth:\n            precisions[i] = ((matches_by_order[i] + 1.) /\n                           (possible_matches_by_order[i] + 1.))\n        else:\n            if possible_matches_by_order[i] > 0:\n                precisions[i] = (float(matches_by_order[i]) /\n                             possible_matches_by_order[i])\n            else:\n                precisions[i] = 0.0\n\n    if min(precisions) > 0:\n        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n        geo_mean = math.exp(p_log_sum)\n    else:\n        geo_mean = 0\n\n    ratio = float(translation_length) / reference_length\n\n    if ratio > 1.0:\n        bp = 1.\n    else:\n        if ratio > 1E-1:\n            bp = math.exp(1 - 1. / ratio)\n        else:\n            bp = 1E-2\n\n    bleu = geo_mean * bp\n\n    return (bleu, precisions, bp, ratio, translation_length, reference_length)\n\n########################################################\n# evaluate.py #\n########################################################\n\ndef format_eq(eq):\n\tfin_eq = \"\"\n\tls = ['0','1','2','3','4','5','6','7','8','9','.']\n\ttemp_num = \"\"\n\tflag = 0\n\tfor i in eq:\n\t\tif flag > 0:\n\t\t\tfin_eq = fin_eq + i\n\t\t\tflag = flag-1\n\t\telif i == 'n':\n\t\t\tflag = 6\n\t\t\tif fin_eq == \"\":\n\t\t\t\tfin_eq = fin_eq + i\n\t\t\telse:\n\t\t\t\tfin_eq = fin_eq + ' ' + i\n\t\telif i in ls:\n\t\t\ttemp_num = temp_num + i\n\t\telif i == ' ':\n\t\t\tif temp_num == \"\":\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tif fin_eq == \"\":\n\t\t\t\t\tfin_eq = fin_eq + temp_num\n\t\t\t\telse:\n\t\t\t\t\tfin_eq = fin_eq + ' ' + temp_num\n\t\t\ttemp_num = \"\"\n\t\telse:\n\t\t\tif fin_eq == \"\":\n\t\t\t\tif temp_num == \"\":\n\t\t\t\t\tfin_eq = fin_eq + i\n\t\t\t\telse:\n\t\t\t\t\tfin_eq = fin_eq + temp_num + ' ' + i\n\t\t\telse:\n\t\t\t\tif temp_num == \"\":\n\t\t\t\t\tfin_eq = fin_eq + ' ' + i\n\t\t\t\telse:\n\t\t\t\t\tfin_eq = fin_eq + ' ' + temp_num + ' ' + i\n\t\t\ttemp_num = \"\"\n\tif temp_num != \"\":\n\t\tfin_eq = fin_eq + ' ' + temp_num\n\treturn fin_eq\n\ndef prefix_to_infix(prefix):\n\toperators = ['+', '-', '*', '/']\n\tstack = []\n\telements = format_eq(prefix).split()\n\tfor i in range(len(elements)-1, -1, -1):\n\t\tif elements[i] in operators and len(stack)>1:\n\t\t\top1 = stack.pop(-1)\n\t\t\top2 = stack.pop(-1)\n\t\t\tfin_operand = '(' + ' ' + op1 + ' ' + elements[i] + ' ' + op2 + ' ' + ')'\n\t\t\tstack.append(fin_operand)\n\t\telse:\n\t\t\tstack.append(elements[i])\n\ttry:\n\t\treturn stack[0]\n\texcept:\n\t\treturn ''\n\ndef stack_to_string(stack):\n\top = \"\"\n\tfor i in stack:\n\t\tif op == \"\":\n\t\t\top = op + i\n\t\telse:\n\t\t\top = op + ' ' + i\n\treturn op\n\ndef back_align(eq, list_num):\n\telements = eq.split()\n\tfor i in range(len(elements)):\n\t\tif elements[i][0] == 'n':\n\t\t\tindex = int(elements[i][6])\n\t\t\ttry:\n\t\t\t\tnumber = str(list_num[index])\n\t\t\texcept:\n\t\t\t\treturn '-1000.112'\n\t\t\telements[i] = number\n\treturn stack_to_string(elements)    \n\ndef ans_evaluator(eq, list_num):\n\t#pdb.set_trace()\n\tinfix = prefix_to_infix(eq)\n\taligned = back_align(infix, list_num)\n\ttry:\n\t\tfinal_ans = parse_expr(aligned, evaluate = True)\n\texcept:\n\t\tfinal_ans = -1000.112\n\treturn final_ans\n\ndef cal_score(outputs, nums, ans):\n\tcorr = 0\n\ttot = 0\n\tdisp_corr = []\n\tfor i in range(len(outputs)):\n\t\top = stack_to_string(outputs[i])\n\t\tnum = nums[i].split()\n\t\tnum = [float(nu) for nu in num]\n\t\tanswer = ans[i].item()\n\n\t\tpred = ans_evaluator(op, num)\n\n\t\tif abs(pred - answer) <= 0.01:\n\t\t\tcorr+=1\n\t\t\ttot+=1\n\t\t\tdisp_corr.append(1)\n\t\telse:\n\t\t\ttot+=1\n\t\t\tdisp_corr.append(0)\n\n\treturn corr, tot, disp_corr\n\ndef get_infix_eq(outputs, nums):\n\teqs = []\n\tfor i in range(len(outputs)):\n\t\top = stack_to_string(outputs[i])\n\t\tnum = nums[i].split()\n\t\tnum = [float(nu) for nu in num]\n\n\t\tinfix = prefix_to_infix(op)\n\t\teqs.append(infix)\n\n\treturn eqs\n\n\n########################################################\n# helper.py #\n########################################################\n\ndef gpu_init_pytorch(gpu_num):\n\t'''\n\t\tInitialize GPU\n\n\t\tArgs:\n\t\t\tgpu_num (int): Which GPU to use\n\t\tReturns:\n\t\t\tdevice (torch.device): GPU device\n\t'''\n\n\ttorch.cuda.set_device(int(gpu_num))\n\tdevice = torch.device(\"cuda:{}\".format(\n\t\tgpu_num) if torch.cuda.is_available() else \"cpu\")\n\treturn device\n\ndef create_save_directories(path):\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\ndef save_checkpoint(state, epoch, logger, model_path, ckpt):\n\t'''\n\t\tSaves the model state along with epoch number. The name format is important for \n\t\tthe load functions. Don't mess with it.\n\n\t\tArgs:\n\t\t\tstate (dict): model state\n\t\t\tepoch (int): current epoch\n\t\t\tlogger (logger): logger variable to log messages\n\t\t\tmodel_path (string): directory to save models\n\t\t\tckpt (string): checkpoint name\n\t'''\n\n\tckpt_path = os.path.join(model_path, '{}_{}.pt'.format(ckpt, epoch))\n\tlogger.info('Saving Checkpoint at : {}'.format(ckpt_path))\n\ttorch.save(state, ckpt_path)\n\ndef get_latest_checkpoint(model_path, logger):\n\t'''\n\t\tLooks for the checkpoint with highest epoch number in the directory \"model_path\" \n\n\t\tArgs:\n\t\t\tmodel_path (string): directory where model is saved\n\t\t\tlogger (logger): logger variable to log messages\n\t\tReturns:\n\t\t\tckpt_path: checkpoint path to the latest checkpoint \n\t'''\n\n\tckpts = glob('{}/*.pt'.format(model_path))\n\tckpts = sorted(ckpts)\n\n\tif len(ckpts) == 0:\n\t\tlogger.warning('No Checkpoints Found')\n\n\t\treturn None\n\telse:\n\t\tlatest_epoch = max([int(x.split('_')[-1].split('.')[0]) for x in ckpts])\n\t\tckpts = sorted(ckpts, key= lambda x: int(x.split('_')[-1].split('.')[0]) , reverse=True )\n\t\tckpt_path = ckpts[0]\n\t\tlogger.info('Checkpoint found with epoch number : {}'.format(latest_epoch))\n\t\tlogger.debug('Checkpoint found at : {}'.format(ckpt_path))\n\n\t\treturn ckpt_path\n\ndef load_checkpoint(model, mode, ckpt_path, logger, device):\n\t'''\n\t\tLoad the model at checkpoint\n\n\t\tArgs:\n\t\t\tmodel (object of class TransformerModel): model\n\t\t\tmode (string): train or test mode\n\t\t\tckpt_path: checkpoint path to the latest checkpoint \n\t\t\tlogger (logger): logger variable to log messages\n\t\t\tdevice (torch.device): GPU device\n\t\tReturns:\n\t\t\tstart_epoch (int): epoch from which to start\n\t\t\tmin_train_loss (float): minimum train loss\n\t\t\tmin_val_loss (float): minimum validation loss\n\t\t\tmax_train_acc (float): maximum train accuracy\n\t\t\tmax_val_acc (float): maximum validation accuracy score\n\t\t\tmax_val_bleu (float): maximum valiadtion bleu score\n\t\t\tbest_epoch (int): epoch with highest validation accuracy\n\t\t\tvoc1 (object of class Voc1): vocabulary of source\n\t\t\tvoc2 (object of class Voc2): vocabulary of target\n\t'''\n\n\tcheckpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n\tmodel.load_state_dict(checkpoint['model_state_dict'])\n\tmodel.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\tstart_epoch = checkpoint['epoch']\n\tmin_train_loss  =checkpoint['min_train_loss']\n\tmin_val_loss = checkpoint['min_val_loss']\n\tvoc1 = checkpoint['voc1']\n\tvoc2 = checkpoint['voc2']\n\tmax_train_acc = checkpoint['max_train_acc']\n\tmax_val_acc = checkpoint['max_val_acc']\n\tmax_val_bleu = checkpoint['max_val_bleu']\n\tbest_epoch = checkpoint['best_epoch']\n\n\tmodel.to(device)\n\n\tif mode == 'train':\n\t\tmodel.train()\n\telse:\n\t\tmodel.eval()\n\n\tlogger.info('Successfully Loaded Checkpoint from {}, with epoch number: {} for {}'.format(ckpt_path, start_epoch, mode))\n\n\treturn start_epoch, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2\n\nclass Voc1:\n\tdef __init__(self):\n\t\tself.trimmed = False\n\t\tself.frequented = False\n\t\tself.w2id = {'<s>': 0, '</s>': 1, 'unk': 2}\n\t\tself.id2w = {0: '<s>', 1: '</s>', 2: 'unk'}\n\t\tself.w2c = {}\n\t\tself.nwords = 3\n\n\tdef add_word(self, word):\n\t\tif word not in self.w2id:\n\t\t\tself.w2id[word] = self.nwords\n\t\t\tself.id2w[self.nwords] = word\n\t\t\tself.w2c[word] = 1\n\t\t\tself.nwords += 1\n\t\telse:\n\t\t\tself.w2c[word] += 1\n\n\tdef add_sent(self, sent):\n\t\tfor word in sent.split():\n\t\t\tself.add_word(word)\n\n\tdef most_frequent(self, topk):\n\t\t# if self.frequented == True:\n\t\t# \treturn\n\t\t# self.frequented = True\n\n\t\tkeep_words = []\n\t\tcount = 3\n\t\tsort_by_value = sorted(\n\t\t\tself.w2c.items(), key=lambda kv: kv[1], reverse=True)\n\t\tfor word, freq in sort_by_value:\n\t\t\tkeep_words += [word]*freq\n\t\t\tcount += 1\n\t\t\tif count == topk:\n\t\t\t\tbreak\n\n\t\tself.w2id = {'<s>': 0, '</s>': 1, 'unk': 2}\n\t\tself.id2w = {0: '<s>', 1: '</s>', 2: 'unk'}\n\t\tself.w2c = {}\n\t\tself.nwords = 3\n\n\t\tfor word in keep_words:\n\t\t\tself.add_word(word)\n\n\tdef trim(self, mincount):\n\t\tif self.trimmed == True:\n\t\t\treturn\n\t\tself.trimmed = True\n\n\t\tkeep_words = []\n\t\tfor k, v in self.w2c.items():\n\t\t\tif v >= mincount:\n\t\t\t\tkeep_words += [k]*v\n\n\t\tself.w2id = {'<s>': 0, '</s>': 1, 'unk': 2}\n\t\tself.id2w = {0: '<s>', 1: '</s>', 2: 'unk'}\n\t\tself.w2c = {}\n\t\tself.nwords = 3\n\t\tfor word in keep_words:\n\t\t\tself.addWord(word)\n\n\tdef get_id(self, idx):\n\t\treturn self.w2id[idx]\n\n\tdef get_word(self, idx):\n\t\treturn self.id2w[idx]\n\n\tdef create_vocab_dict(self, args, train_dataloader):\n\t\tfor data in train_dataloader:\n\t\t\tfor sent in data['ques']:\n\t\t\t\tself.add_sent(sent)\n\n\t\tself.most_frequent(args.vocab_size)\n\t\tassert len(self.w2id) == self.nwords\n\t\tassert len(self.id2w) == self.nwords\n\n\tdef add_to_vocab_dict(self, args, dataloader):\n\t\tfor data in dataloader:\n\t\t\tfor sent in data['ques']:\n\t\t\t\tself.add_sent(sent)\n\n\t\tself.most_frequent(args.vocab_size)\n\t\tassert len(self.w2id) == self.nwords\n\t\tassert len(self.id2w) == self.nwords\n\nclass Voc2:\n\tdef __init__(self, config):\n\t\tself.frequented = False\n\t\tif config.mawps_vocab:\n\t\t\t# '0.25', '8.0', '0.05', '60.0', '7.0', '5.0', '2.0', '4.0', '1.0', '12.0', '100.0', '25.0', '0.1', '3.0', '0.01', '0.5', '10.0'\n\t\t\tself.w2id = {'<s>': 11, '</s>': 1, '+': 2, '-': 3, '*': 4, '/': 5, 'number0': 6, 'number1': 7, 'number2': 8, 'number3': 9, 'number4': 10, '0.25': 0, '8.0': 12, '0.05': 13, '60.0': 14, '7.0': 15, '5.0': 16, '2.0': 17, '4.0': 18, '1.0': 19, '12.0': 20, '100.0': 21, '25.0': 22, '0.1': 23, '3.0': 24, '0.01': 25, '0.5': 26, '10.0': 27, 'unk': 28}\n\t\t\tself.id2w = {11: '<s>', 1: '</s>', 2: '+', 3: '-', 4: '*', 5: '/', 6: 'number0', 7: 'number1', 8: 'number2', 9: 'number3', 10: 'number4', 0: '0.25', 12: '8.0', 13: '0.05', 14: '60.0', 15: '7.0', 16: '5.0', 17: '2.0', 18: '4.0', 19: '1.0', 20: '12.0', 21: '100.0', 22: '25.0', 23: '0.1', 24: '3.0', 25: '0.01', 26: '0.5', 27: '10.0', 28: 'unk'}\n\t\t\tself.w2c = {'+': 0, '-': 0, '*': 0, '/': 0, 'number0': 0, 'number1': 0, 'number2': 0, 'number3': 0, 'number4': 0, '0.25': 0, '8.0': 0, '0.05': 0, '60.0': 0, '7.0': 0, '5.0': 0, '2.0': 0, '4.0': 0, '1.0': 0, '12.0': 0, '100.0': 0, '25.0': 0, '0.1': 0, '3.0': 0, '0.01': 0, '0.5': 0, '10.0': 0, 'unk': 0}\n\t\t\tself.nwords = 29\n\t\telse:\n\t\t\tself.w2id = {'<s>': 11, '</s>': 1, '+': 2, '-': 3, '*': 4, '/': 5, 'number0': 6, 'number1': 7, 'number2': 8, 'number3': 9, 'number4': 10, 'number5': 0, 'unk': 12}\n\t\t\tself.id2w = {11: '<s>', 1: '</s>', 2: '+', 3: '-', 4: '*', 5: '/', 6: 'number0', 7: 'number1', 8: 'number2', 9: 'number3', 10: 'number4', 0: 'number5', 12: 'unk'}\n\t\t\tself.w2c = {'+': 0, '-': 0, '*': 0, '/': 0, 'number0': 0, 'number1': 0, 'number2': 0, 'number3': 0, 'number4': 0, 'number5': 0, 'unk': 0}\n\t\t\tself.nwords = 13 # For some reason, model outputs NANs if I keep <s> as token 0 - That's because it was masking out the <s> token in make_len_mask in model\n\n\tdef add_word(self, word):\n\t\tif word not in self.w2id: # IT SHOULD NEVER GO HERE!!\n\t\t\tself.w2id[word] = self.nwords\n\t\t\tself.id2w[self.nwords] = word\n\t\t\tself.w2c[word] = 1\n\t\t\tself.nwords += 1\n\t\telse:\n\t\t\tself.w2c[word] += 1\n\n\tdef add_sent(self, sent):\n\t\tfor word in sent.split():\n\t\t\tself.add_word(word)\n\n\tdef get_id(self, idx):\n\t\treturn self.w2id[idx]\n\n\tdef get_word(self, idx):\n\t\treturn self.id2w[idx]\n\n\tdef create_vocab_dict(self, args, train_dataloader):\n\t\tfor data in train_dataloader:\n\t\t\tfor sent in data['eqn']:\n\t\t\t\tself.add_sent(sent)\n\n\t\tassert len(self.w2id) == self.nwords\n\t\tassert len(self.id2w) == self.nwords\n\n\tdef add_to_vocab_dict(self, args, dataloader):\n\t\tfor data in dataloader:\n\t\t\tfor sent in data['eqn']:\n\t\t\t\tself.add_sent(sent)\n\n\t\tassert len(self.w2id) == self.nwords\n\t\tassert len(self.id2w) == self.nwords\n        \ndef bleu_scorer(ref, hyp, script='default'):\n\t'''\n\t\tBleu Scorer (Send list of list of references, and a list of hypothesis)\n\t'''\n\t\n\trefsend = []\n\tfor i in range(len(ref)):\n\t\trefsi = []\n\t\tfor j in range(len(ref[i])):\n\t\t\trefsi.append(ref[i][j].split())\n\t\trefsend.append(refsi)\n\n\tgensend = []\n\tfor i in range(len(hyp)):\n\t\tgensend.append(hyp[i].split())\n\n\tif script == 'nltk':\n\t\tmetrics = corpus_bleu(refsend, gensend)\n\t\treturn [metrics]\n\n\tmetrics = compute_bleu(refsend, gensend)\n\treturn metrics\n\n########################################################\n# logger.py #\n########################################################\n\ndef get_logger(name, log_file_path='./logs/temp.log', logging_level=logging.INFO, \n\t\t\t\tlog_format='%(asctime)s | %(levelname)s | %(filename)s: %(lineno)s : %(funcName)s() ::\\t %(message)s'):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging_level)\n\tformatter = logging.Formatter(log_format)\n\n\tfile_handler = logging.FileHandler(log_file_path, mode='w') # Sends logging output to a disk file\n\tfile_handler.setLevel(logging_level)\n\tfile_handler.setFormatter(formatter)\n\n\tstream_handler = logging.StreamHandler() # Sends logging output to stdout\n\tstream_handler.setLevel(logging_level)\n\tstream_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(stream_handler)\n\n\t# logger.addFilter(ContextFilter(expt_name))\n\n\treturn logger\n\n\ndef print_log(logger, dict):\n\tstring = ''\n\tfor key, value in dict.items():\n\t\tstring += '\\n {}: {}\\t'.format(key.replace('_', ' '), value)\n\t# string = string.strip()\n\tlogger.info(string)\n\n\n\ndef store_results(config, max_val_bleu, max_val_acc, min_val_loss, max_train_acc, min_train_loss, best_epoch):\n\ttry:\n\t\twith open(config.result_path) as f:\n\t\t\tres_data =json.load(f)\n\texcept:\n\t\tres_data = {}\n\ttry:\n\t\tmin_train_loss = min_train_loss.item()\n\texcept:\n\t\tpass\n\ttry:\n\t\tmin_val_loss = min_val_loss.item()\n\texcept:\n\t\tpass\n\ttry:\n\n\t\tdata= {'run_name' : str(config.run_name)\n\t\t, 'max val acc': str(max_val_acc)\n\t\t, 'max train acc': str(max_train_acc)\n\t\t, 'max val bleu' : str(max_val_bleu)\n\t\t, 'min val loss' : str(min_val_loss)\n\t\t, 'min train loss': str(min_train_loss)\n\t\t, 'best epoch': str(best_epoch)\n\t\t, 'epochs' : config.epochs\n\t\t, 'dataset' : config.dataset\n\t\t, 'embedding': config.embedding\n\t\t, 'embedding_lr': config.emb_lr\n\t\t, 'freeze_emb': config.freeze_emb\n\t\t, 'i/p and o/p embedding size' : config.d_model\n\t\t, 'encoder_layers' : config.encoder_layers\n\t\t, 'decoder_layers' : config.decoder_layers\n\t\t, 'heads' : config.heads\n\t\t, 'FFN size' : config.d_ff\n\t\t, 'lr' : config.lr\n\t\t, 'batch_size' : config.batch_size\n\t\t, 'dropout' : config.dropout\n\t\t, 'opt' : config.opt\n\t\t}\n\t\tres_data[str(config.run_name)] = data\n\n\t\twith open(config.result_path, 'w', encoding='utf-8') as f:\n\t\t\tjson.dump(res_data, f, ensure_ascii= False, indent= 4)\n\texcept:\n\t\tpdb.set_trace()\n\ndef store_val_results(config, acc_score, folds_scores):\n\ttry:\n\t\twith open(config.val_result_path) as f:\n\t\t\tres_data = json.load(f)\n\texcept:\n\t\tres_data = {}\n\ttry:\n\t\tdata= {'run_name' : str(config.run_name)\n\t\t, '5-fold avg acc score' : str(acc_score)\n\t\t, 'Fold0 acc' : folds_scores[0]\n\t\t, 'Fold1 acc' : folds_scores[1]\n\t\t, 'Fold2 acc' : folds_scores[2]\n\t\t, 'Fold3 acc' : folds_scores[3]\n\t\t, 'Fold4 acc' : folds_scores[4]\n\t\t, 'dataset' : config.dataset\n\t\t, 'embedding': config.embedding\n\t\t, 'embedding_lr': config.emb_lr\n\t\t, 'freeze_emb': config.freeze_emb\n\t\t, 'i/p and o/p embedding size' : config.d_model\n\t\t, 'encoder_layers' : config.encoder_layers\n\t\t, 'decoder_layers' : config.decoder_layers\n\t\t, 'heads' : config.heads\n\t\t, 'FFN size' : config.d_ff\n\t\t, 'lr' : config.lr\n\t\t, 'batch_size' : config.batch_size\n\t\t, 'dropout' : config.dropout\n\t\t, 'opt' : config.opt\n\t\t}\n\t\t# res_data.update(data)\n\t\tres_data[str(config.run_name)] = data\n\n\t\twith open(config.val_result_path, 'w', encoding='utf-8') as f:\n\t\t\tjson.dump(res_data, f, ensure_ascii= False, indent= 4)\n\texcept:\n\t\tpdb.set_trace()\n        \n        \n########################################################\n# sentence_processing.py #\n########################################################\n\ndef sent_to_idx(voc, sent, max_length, flag = 0):\n\tif flag == 0:\n\t\tidx_vec = []\n\telse:\n\t\tidx_vec = [voc.get_id('<s>')]\n\tfor w in sent.split(' '):\n\t\ttry:\n\t\t\tidx = voc.get_id(w)\n\t\t\tidx_vec.append(idx)\n\t\texcept:\n\t\t\tidx_vec.append(voc.get_id('unk'))\n\t# idx_vec.append(voc.get_id('</s>'))\n\tif flag == 1 and len(idx_vec) < max_length-1:\n\t\tidx_vec.append(voc.get_id('</s>'))\n\treturn idx_vec\n\ndef sents_to_idx(voc, sents, max_length, flag = 0):\n\tall_indexes = []\n\tfor sent in sents:\n\t\tall_indexes.append(sent_to_idx(voc, sent, max_length, flag))\n\treturn all_indexes\n\ndef sent_to_tensor(voc, sentence, device, max_length):\n\tindexes = sent_to_idx(voc, sentence, max_length)\n\treturn torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\ndef batch_to_tensor(voc, sents, device, max_length):\n\tbatch_sent = []\n\t# batch_label = []\n\tfor sent in sents:\n\t\tsent_id = sent_to_tensor(voc, sent, device, max_length)\n\t\tbatch_sent.append(sent_id)\n\n\treturn batch_sent\n\ndef idx_to_sent(voc, tensor, no_eos=False):\n\tsent_word_list = []\n\tfor idx in tensor:\n\t\tword = voc.get_word(idx.item())\n\t\tif no_eos:\n\t\t\tif word != '</s>':\n\t\t\t\tsent_word_list.append(word)\n\t\t\t# else:\n\t\t\t# \tbreak\n\t\telse:\n\t\t\tsent_word_list.append(word)\n\treturn sent_word_list\n\ndef idx_to_sents(voc, tensors, no_eos=False):\n\ttensors = tensors.transpose(0, 1)\n\tbatch_word_list = []\n\tfor tensor in tensors:\n\t\tbatch_word_list.append(idx_to_sent(voc, tensor, no_eos))\n\n\treturn batch_word_list\n\ndef pad_seq(seq, max_length, voc):\n\tseq += [voc.get_id('</s>') for i in range(max_length - len(seq))]\n\treturn seq\n\ndef sort_by_len(seqs, input_len, device=None, dim=1):\n\torig_idx = list(range(seqs.size(dim)))\n\n\t# Index by which sorting needs to be done\n\tsorted_idx = sorted(orig_idx, key=lambda k: input_len[k], reverse=True)\n\tsorted_idx= torch.LongTensor(sorted_idx)\n\tif device:\n\t\tsorted_idx = sorted_idx.to(device)\n\n\tsorted_seqs = seqs.index_select(1, sorted_idx)\n\tsorted_lens=  [input_len[i] for i in sorted_idx]\n\n\t# For restoring original order\n\torig_idx = sorted(orig_idx, key=lambda k: sorted_idx[k])\n\torig_idx = torch.LongTensor(orig_idx)\n\tif device:\n\t\torig_idx = orig_idx.to(device)\n\treturn sorted_seqs, sorted_lens, orig_idx\n\ndef restore_order(seqs, input_len, orig_idx):\n\torig_seqs= [seqs[i] for i in orig_idx]\n\torig_lens= [input_len[i] for i in orig_idx]\n\treturn orig_seqs, orig_lens\n\ndef process_batch(sent1s, sent2s, voc1, voc2, device):\n\tinput_len1 = [len(s) for s in sent1s]\n\tinput_len2 = [len(s) for s in sent2s]\n\tmax_length_1 = max(input_len1)\n\tmax_length_2 = max(input_len2)\n\n\tsent1s_padded = [pad_seq(s, max_length_1, voc1) for s in sent1s]\n\tsent2s_padded = [pad_seq(s, max_length_2, voc2) for s in sent2s]\n\n\t# Convert to [Max_len X Batch]\n\tsent1_var = Variable(torch.LongTensor(sent1s_padded)).transpose(0, 1)\n\tsent2_var = Variable(torch.LongTensor(sent2s_padded)).transpose(0, 1)\n\n\tsent1_var = sent1_var.to(device)\n\tsent2_var = sent2_var.to(device)\n\n\treturn sent1_var, sent2_var, input_len1, input_len2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T16:26:26.236440Z","iopub.execute_input":"2024-11-17T16:26:26.236827Z","iopub.status.idle":"2024-11-17T16:26:26.340905Z","shell.execute_reply.started":"2024-11-17T16:26:26.236791Z","shell.execute_reply":"2024-11-17T16:26:26.339905Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## args.py","metadata":{}},{"cell_type":"code","source":"def build_parser():\n\t# Data loading parameters\n\tparser = argparse.ArgumentParser(description='Run Single sequence model')\n\n\t# Mode specifications\n\tparser.add_argument('-mode', type=str, default='train', choices=['train', 'test'], help='Modes: train, test')\n\tparser.add_argument('-debug', dest='debug', action='store_true', help='Operate in debug mode')\n\tparser.add_argument('-no-debug', dest='debug', action='store_false', help='Operate in normal mode')\n\tparser.set_defaults(debug=False)\n\t\n\t# Run Config\n\tparser.add_argument('-run_name', type=str, default='debug', help='run name for logs')\n\tparser.add_argument('-dataset', type=str, default='asdiv-a_fold0_final', help='Dataset')\n\tparser.add_argument('-display_freq', type=int, default= 10000, help='number of batches after which to display samples')\n\tparser.add_argument('-outputs', dest='outputs', action='store_true', help='Show full validation outputs')\n\tparser.add_argument('-no-outputs', dest='outputs', action='store_false', help='Do not show full validation outputs')\n\tparser.set_defaults(outputs=True)\n\tparser.add_argument('-results', dest='results', action='store_true', help='Store results')\n\tparser.add_argument('-no-results', dest='results', action='store_false', help='Do not store results')\n\tparser.set_defaults(results=True)\n\n\t# Meta Attributes\n\tparser.add_argument('-vocab_size', type=int, default=30000, help='Vocabulary size to consider')\n\tparser.add_argument('-histogram', dest='histogram', action='store_true', help='Operate in debug mode')\n\tparser.add_argument('-no-histogram', dest='histogram', action='store_false', help='Operate in normal mode')\n\tparser.set_defaults(histogram=False)\n\tparser.add_argument('-save_writer', dest='save_writer',action='store_true', help='To write tensorboard')\n\tparser.add_argument('-no-save_writer', dest='save_writer', action='store_false', help='Dont write tensorboard')\n\tparser.set_defaults(save_writer=False)\n\n\t# Device Configuration\n\tparser.add_argument('-gpu', type=int, default=2, help='Specify the gpu to use')\n\tparser.add_argument('-early_stopping', type=int, default=500, help='Early Stopping after n epoch')\n\tparser.add_argument('-seed', type=int, default=6174, help='Default seed to set')\n\tparser.add_argument('-logging', type=int, default=1, help='Set to 0 if you do not require logging')\n\tparser.add_argument('-ckpt', type=str, default='model', help='Checkpoint file name')\n\tparser.add_argument('-save_model', dest='save_model',action='store_true', help='To save the model')\n\tparser.add_argument('-no-save_model', dest='save_model', action='store_false', help='Dont save the model')\n\tparser.set_defaults(save_model=True)\n\n\t# Transformer parameters\n\tparser.add_argument('-heads', type=int, default=8, help='Number of Attention Heads')\n\tparser.add_argument('-encoder_layers', type=int, default=6, help='Number of layers in encoder')\n\tparser.add_argument('-decoder_layers', type=int, default=6, help='Number of layers in decoder')\n\tparser.add_argument('-d_model', type=int, default=300, help='the number of expected features in the encoder inputs') #768? features of BERT? HAS TO BE 300 if using word2Vec\n\tparser.add_argument('-d_ff', type=int, default=1200, help='Embedding dimensions of intermediate FFN Layer (refer Vaswani et. al)')\n\tparser.add_argument('-lr', type=float, default=0.001, help='Learning rate')\n\tparser.add_argument('-dropout', type=float, default=0.1, help= 'Dropout probability for input/output/state units (0.0: no dropout)')\n\tparser.add_argument('-warmup', type=float, default=0.1, help='Proportion of training to perform linear learning rate warmup for')\n\tparser.add_argument('-max_grad_norm', type=float, default=0.25, help='Clip gradients to this norm')\n\tparser.add_argument('-batch_size', type=int, default=16, help='Batch size')\n\n\tparser.add_argument('-max_length', type=int, default=80, help='Specify max decode steps: Max length string to output')\n\tparser.add_argument('-init_range', type=float, default=0.08, help='Initialization range for seq2seq model')\n\t\n\tparser.add_argument('-embedding', type=str, default='word2vec', choices=['bert', 'roberta', 'word2vec', 'random'], help='Embeddings')\n\tparser.add_argument('-word2vec_bin', type=str, default='/datadrive/satwik/global_data/GoogleNews-vectors-negative300.bin', help='Binary file of word2vec')\n\tparser.add_argument('-emb_name', type=str, default='roberta-base', choices=['bert-base-uncased', 'roberta-base'], help='Which pre-trained model')\n\tparser.add_argument('-emb_lr', type=float, default=1e-5, help='Larning rate to train embeddings')\n\tparser.add_argument('-freeze_emb', dest='freeze_emb', action='store_true', help='Freeze embedding weights')\n\tparser.add_argument('-no-freeze_emb', dest='freeze_emb', action='store_false', help='Train embedding weights')\n\tparser.set_defaults(freeze_emb=False)\n\n\tparser.add_argument('-epochs', type=int, default=10, help='Maximum # of training epochs')\n\tparser.add_argument('-opt', type=str, default='adamw', choices=['adam', 'adamw', 'adadelta', 'sgd', 'asgd'], help='Optimizer for training')\n\n\tparser.add_argument('-grade_disp', dest='grade_disp', action='store_true', help='Display grade information in validation outputs')\n\tparser.add_argument('-no-grade_disp', dest='grade_disp', action='store_false', help='Don\\'t display grade information')\n\tparser.set_defaults(grade_disp=False)\n\tparser.add_argument('-type_disp', dest='type_disp', action='store_true', help='Display Type information in validation outputs')\n\tparser.add_argument('-no-type_disp', dest='type_disp', action='store_false', help='Don\\'t display Type information')\n\tparser.set_defaults(type_disp=False)\n\tparser.add_argument('-challenge_disp', dest='challenge_disp', action='store_true', help='Display information in validation outputs')\n\tparser.add_argument('-no-challenge_disp', dest='challenge_disp', action='store_false', help='Don\\'t display information')\n\tparser.set_defaults(challenge_disp=False)\n\tparser.add_argument('-nums_disp', dest='nums_disp', action='store_true', help='Display number of numbers information in validation outputs')\n\tparser.add_argument('-no-nums_disp', dest='nums_disp', action='store_false', help='Don\\'t display number of numbers information')\n\tparser.set_defaults(nums_disp=True)\n\tparser.add_argument('-more_nums', dest='more_nums', action='store_true', help='More numbers in Voc2')\n\tparser.add_argument('-no-more_nums', dest='more_nums', action='store_false', help='Usual numbers in Voc2')\n\tparser.set_defaults(more_nums=False)\n\tparser.add_argument('-mawps_vocab', dest='mawps_vocab', action='store_true', help='Custom Numbers in Voc2')\n\tparser.add_argument('-no-mawps_vocab', dest='mawps_vocab', action='store_false', help='No Custom Numbers in Voc2')\n\tparser.set_defaults(mawps_vocab=False)\n\n\tparser.add_argument('-show_train_acc', dest='show_train_acc', action='store_true', help='Calculate the train accuracy')\n\tparser.add_argument('-no-show_train_acc', dest='show_train_acc', action='store_false', help='Don\\'t calculate the train accuracy')\n\tparser.set_defaults(show_train_acc=True)\n\n\tparser.add_argument('-full_cv', dest='full_cv', action='store_true', help='5-fold CV')\n\tparser.add_argument('-no-full_cv', dest='full_cv', action='store_false', help='No 5-fold CV')\n\tparser.set_defaults(full_cv=False)\n\n\treturn parser\n\ndef parse_arguments(arg_dict=None):\n    parser = build_parser()\n    if arg_dict:\n        # Override default values with provided dictionary values\n        args = parser.parse_args([])\n        for key, value in arg_dict.items():\n            setattr(args, key, value)\n        return args\n    else:\n        return parser.parse_args()  # If no dictionary is provided, use default command line arguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:55:14.754097Z","iopub.execute_input":"2024-11-17T15:55:14.754405Z","iopub.status.idle":"2024-11-17T15:55:14.784785Z","shell.execute_reply.started":"2024-11-17T15:55:14.754372Z","shell.execute_reply":"2024-11-17T15:55:14.783883Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Dataloader.py","metadata":{}},{"cell_type":"code","source":"class TextDataset(Dataset):\n\t'''\n\t\tExpecting csv files with columns ['Question', 'Equation', 'Numbers', 'Answer']\n\n\t\tArgs:\n\t\t\t\t\t\tdata_path: Root folder Containing all the data\n\t\t\t\t\t\tdataset: Specific Folder ==> data_path/dataset/\t(Should contain train.csv and dev.csv)\n\t\t\t\t\t\tmax_length: Self Explanatory\n\t\t\t\t\t\tis_debug: Load a subset of data for faster testing\n\t\t\t\t\t\tis_train: \n\n\t'''\n\n\tdef __init__(self, data_path='/kaggle/input/svamp-dataset/data', dataset='mawps', datatype='train', max_length=30, is_debug=False, is_train=False, grade_info=False, type_info=False, challenge_info=False):\n\t\tif datatype=='train':\n\t\t\tfile_path = os.path.join(data_path, dataset, 'train.csv')\n\t\telif datatype=='dev':\n\t\t\tfile_path = os.path.join(data_path, dataset, 'dev.csv')\n\t\telse:\n\t\t\tfile_path = os.path.join(data_path, dataset, 'test.csv')\n\n\t\tif grade_info:\n\t\t\tself.grade_info = True\n\t\telse:\n\t\t\tself.grade_info = False\n\n\t\tif type_info:\n\t\t\tself.type_info = True\n\t\telse:\n\t\t\tself.type_info = False\n\n\t\tif challenge_info:\n\t\t\tself.challenge_info = True\n\t\telse:\n\t\t\tself.challenge_info = False\n\n\t\tfile_df= pd.read_csv(file_path)\n\n\t\tself.ques = file_df['Question'].values # np ndarray of size (#examples,)\n\t\tself.eqn = file_df['Equation'].values\n\t\tself.nums = file_df['Numbers'].values\n\t\tself.ans = file_df['Answer'].values\n\n\t\tif grade_info:\n\t\t\tself.grade = file_df['Grade'].values\n\n\t\tif type_info:\n\t\t\tself.type = file_df['Type'].values\n\n\t\tif challenge_info:\n\t\t\tself.type = file_df['Type'].values\n\t\t\tself.var_type = file_df['Variation Type'].values\n\t\t\tself.annotator = file_df['Annotator'].values\n\t\t\tself.alternate = file_df['Alternate'].values\n\n\t\tif is_debug:\n\t\t\tself.ques = self.ques[:5000:500]\n\t\t\tself.eqn = self.eqn[:5000:500]\n\n\t\tself.max_length = max_length\n\n\t\tif grade_info and type_info:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans, self.grade, self.type)\n\t\telif grade_info and not type_info:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans, self.grade)\n\t\telif type_info and not grade_info:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans, self.type)\n\t\telif challenge_info:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans, self.type, self.var_type, self.annotator, self.alternate)\n\t\telse:\n\t\t\tall_sents = zip(self.ques, self.eqn, self.nums, self.ans)\n\n\t\tif is_train:\n\t\t\tall_sents = sorted(all_sents, key = lambda x : len(x[0].split()))\n\n\t\tif grade_info and type_info:\n\t\t\tself.ques, self.eqn, self.nums, self.ans, self.grade, self.type = zip(*all_sents)\n\t\telif grade_info and not type_info:\n\t\t\tself.ques, self.eqn, self.nums, self.ans, self.grade = zip(*all_sents)\n\t\telif type_info and not grade_info:\n\t\t\tself.ques, self.eqn, self.nums, self.ans, self.type = zip(*all_sents)\n\t\telif challenge_info:\n\t\t\tself.ques, self.eqn, self.nums, self.ans, self.type, self.var_type, self.annotator, self.alternate = zip(*all_sents)\n\t\telse:\n\t\t\tself.ques, self.eqn, self.nums, self.ans = zip(*all_sents)\n\n\tdef __len__(self):\n\t\treturn len(self.ques)\n\n\tdef __getitem__(self, idx):\n\t\tques = self.process_string(str(self.ques[idx]))\n\t\teqn = self.process_string(str(self.eqn[idx]))\n\t\tnums = self.nums[idx]\n\t\tans = self.ans[idx]\n\t\t\n\t\tif self.grade_info and self.type_info:\n\t\t\tgrade = self.grade[idx]\n\t\t\ttype1 = self.type[idx]\n\t\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans, 'grade': grade, 'type': type1}\n\t\telif self.grade_info and not self.type_info:\n\t\t\tgrade = self.grade[idx]\n\t\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans, 'grade': grade}\n\t\telif self.type_info and not self.grade_info:\n\t\t\ttype1 = self.type[idx]\n\t\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans, 'type': type1}\n\t\telif self.challenge_info:\n\t\t\ttype1 = self.type[idx]\n\t\t\tvar_type = self.var_type[idx]\n\t\t\tannotator = self.annotator[idx]\n\t\t\talternate = self.alternate[idx]\n\t\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans, 'type': type1, \n\t\t\t\t\t'var_type': var_type, 'annotator': annotator, 'alternate': alternate}\n\n\t\treturn {'ques': self.curb_to_length(ques), 'eqn': self.curb_to_length(eqn), 'nums': nums, 'ans': ans}\n\n\tdef curb_to_length(self, string):\n\t\treturn ' '.join(string.strip().split()[:self.max_length])\n\n\tdef process_string(self, string):\n\t\t#string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n\t\tstring = re.sub(r\"\\'s\", \" 's\", string)\n\t\tstring = re.sub(r\"\\'ve\", \" 've\", string)\n\t\tstring = re.sub(r\"n\\'t\", \" n't\", string)\n\t\tstring = re.sub(r\"\\'re\", \" 're\", string)\n\t\tstring = re.sub(r\"\\'d\", \" 'd\", string)\n\t\tstring = re.sub(r\"\\'ll\", \" 'll\", string)\n\t\t#string = re.sub(r\",\", \" , \", string)\n\t\t#string = re.sub(r\"!\", \" ! \", string)\n\t\t#string = re.sub(r\"\\(\", \" ( \", string)\n\t\t#string = re.sub(r\"\\)\", \" ) \", string)\n\t\t#string = re.sub(r\"\\?\", \" ? \", string)\n\t\t#string = re.sub(r\"\\s{2,}\", \" \", string)\n\t\treturn string","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:55:14.786215Z","iopub.execute_input":"2024-11-17T15:55:14.786505Z","iopub.status.idle":"2024-11-17T15:55:14.813330Z","shell.execute_reply.started":"2024-11-17T15:55:14.786474Z","shell.execute_reply":"2024-11-17T15:55:14.812519Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## model.py","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\tdef __init__(self, d_model, dropout=0.1, max_len=5000):\n\t\tsuper(PositionalEncoding, self).__init__()\n\t\tself.dropout = nn.Dropout(p=dropout)\n\t\tself.scale = nn.Parameter(torch.ones(1)) # nn.Parameter causes the tensor to appear in the model.parameters()\n\n\t\tpe = torch.zeros(max_len, d_model)\n\t\tposition = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # max_len x 1\n\t\tdiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # torch.arange(0, d_model, 2) gives 2i\n\t\tpe[:, 0::2] = torch.sin(position * div_term) # all alternate columns 0 onwards\n\t\tpe[:, 1::2] = torch.cos(position * div_term) # all alternate columns 1 onwards\n\t\tpe = pe.unsqueeze(0).transpose(0, 1)\n\t\tself.register_buffer('pe', pe)\n\n\tdef forward(self, x):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tx (tensor): embeddings | size : [max_len x batch_size x d_model]\n\t\t\tReturns:\n\t\t\t\tz (tensor) : embeddings with positional encoding | size : [max_len x batch_size x d_model]\n\t\t'''\n\t\t\n\t\tx = x + self.scale * self.pe[:x.size(0), :]\n\t\tz = self.dropout(x)\n\t\treturn z\n\nclass TransformerModel(nn.Module):\n\tdef __init__(self, config, voc1, voc2, device, logger, EOS_tag = '</s>', SOS_tag = '<s>'):\n\t\tsuper(TransformerModel, self).__init__()\n\t\tself.config = config\n\t\tself.device = device\n\t\tself.voc1 = voc1\n\t\tself.voc2 = voc2\n\t\tself.EOS_tag = EOS_tag\n\t\tself.SOS_tag = SOS_tag\n\t\tself.EOS_token = voc2.get_id(EOS_tag)\n\t\tself.SOS_token = voc2.get_id(SOS_tag)\n\t\tself.logger = logger\n\n\t\tself.logger.debug('Initialising Embeddings.....')\n\n\t\tif self.config.embedding == 'bert':\n\t\t\tconfig.d_model = 768\n\t\t\tself.embedding1 = BertEncoder(self.config.emb_name, self.device, self.config.freeze_emb)\n\t\telif self.config.embedding == 'roberta':\n\t\t\tconfig.d_model = 768\n\t\t\tself.embedding1 = RobertaEncoder(self.config.emb_name, self.device, self.config.freeze_emb)\n\t\telif self.config.embedding == 'word2vec':\n\t\t\tconfig.d_model = 300\n\t\t\tself.embedding1  = nn.Embedding.from_pretrained(torch.FloatTensor(self._form_embeddings(self.config.word2vec_bin)), \n\t\t\t\t\t\t\t\tfreeze = self.config.freeze_emb)\n\t\telse:\n\t\t\tself.embedding1  = nn.Embedding(self.voc1.nwords, self.config.d_model)\n\t\t\tnn.init.uniform_(self.embedding1.weight, -1 * self.config.init_range, self.config.init_range)\n\n\t\tself.pos_embedding1 = PositionalEncoding(self.config.d_model, self.config.dropout)\n\n\t\tself.embedding2  = nn.Embedding(self.voc2.nwords, self.config.d_model)\n\t\tnn.init.uniform_(self.embedding2.weight, -1 * self.config.init_range, self.config.init_range)\n\t\t\n\t\tself.pos_embedding2 = PositionalEncoding(self.config.d_model, self.config.dropout)\n\n\t\tself.logger.debug('Embeddings initialised.....')\n\t\tself.logger.debug('Building Transformer Model.....')\n\n\t\tself.transformer = nn.Transformer(d_model=self.config.d_model, nhead=self.config.heads, \n\t\t\t\t\t\t\t\t\t\t\tnum_encoder_layers=self.config.encoder_layers, num_decoder_layers=self.config.decoder_layers, \n\t\t\t\t\t\t\t\t\t\t\tdim_feedforward=self.config.d_ff, dropout=self.config.dropout)\n\t\t\n\t\tself.fc_out = nn.Linear(self.config.d_model, self.voc2.nwords)\n\n\t\tself.logger.debug('Transformer Model Built.....')\n\n\t\tself.src_mask = None\n\t\tself.trg_mask = None\n\t\tself.memory_mask = None\n\n\t\tself.logger.debug('Initalizing Optimizer and Criterion...')\n\n\t\tself._initialize_optimizer()\n\n\t\tself.criterion = nn.CrossEntropyLoss() # nn.CrossEntropyLoss() does both F.log_softmax() and nn.NLLLoss() \n\n\t\tself.logger.info('All Model Components Initialized...')\n\n\tdef _form_embeddings(self, file_path):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tfile_path (string): path of file with word2vec weights\n\t\t\tReturns:\n\t\t\t\tweight_req (tensor) : embedding matrix | size : [voc1.nwords x d_model]\n\t\t'''\n\n\t\tweights_all = models.KeyedVectors.load_word2vec_format(file_path, limit=200000, binary=True)\n\t\tweight_req  = torch.randn(self.voc1.nwords, self.config.d_model)\n\t\tfor key, value in self.voc1.id2w.items():\n\t\t\tif value in weights_all:\n\t\t\t\tweight_req[key] = torch.FloatTensor(weights_all[value])\n\n\t\treturn weight_req\n\n\tdef _initialize_optimizer(self):\n\t\tself.params = list(self.embedding1.parameters()) + list(self.transformer.parameters()) + list(self.fc_out.parameters()) + \\\n\t\t\t\t\t\tlist(self.embedding2.parameters()) + list(self.pos_embedding1.parameters()) + list(self.pos_embedding2.parameters())\n\t\tself.non_emb_params = list(self.transformer.parameters()) + list(self.fc_out.parameters()) + list(self.embedding2.parameters()) + \\\n\t\t\t\t\t\t\t\tlist(self.pos_embedding1.parameters()) + list(self.pos_embedding2.parameters())\n\n\t\tif self.config.opt == 'adam':\n\t\t\tself.optimizer = optim.Adam(\n\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t{\"params\": self.non_emb_params, \"lr\": self.config.lr}]\n\t\t\t)\n\t\telif self.config.opt == 'adamw':\n\t\t\tself.optimizer = optim.AdamW(\n\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t{\"params\": self.non_emb_params, \"lr\": self.config.lr}]\n\t\t\t)\n\t\telif self.config.opt == 'adadelta':\n\t\t\tself.optimizer = optim.Adadelta(\n\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t{\"params\": self.non_emb_params, \"lr\": self.config.lr}]\n\t\t\t)\n\t\telif self.config.opt == 'asgd':\n\t\t\tself.optimizer = optim.ASGD(\n\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t{\"params\": self.non_emb_params, \"lr\": self.config.lr}]\n\t\t\t)\n\t\telse:\n\t\t\tself.optimizer = optim.SGD(\n\t\t\t\t[{\"params\": self.embedding1.parameters(), \"lr\": self.config.emb_lr},\n\t\t\t\t{\"params\": self.non_emb_params, \"lr\": self.config.lr}]\n\t\t\t)\n\n\tdef generate_square_subsequent_mask(self, sz):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tsz (integer): max_len of sequence in target without EOS i.e. (T-1)\n\t\t\tReturns:\n\t\t\t\tmask (tensor) : square mask | size : [T-1 x T-1]\n\t\t'''\n\n\t\tmask = torch.triu(torch.ones(sz, sz), 1)\n\t\tmask = mask.masked_fill(mask==1, float('-inf'))\n\t\treturn mask\n\n\tdef make_len_mask(self, inp):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tinp (tensor): input indices | size : [S x BS]\n\t\t\tReturns:\n\t\t\t\tmask (tensor) : pad mask | size : [BS x S]\n\t\t'''\n\n\t\tmask = (inp == -1).transpose(0, 1)\n\t\treturn mask\n\t\t# return (inp == self.EOS_token).transpose(0, 1)\n\n\tdef forward(self, ques, src, trg):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tques (list): raw source input | size : [BS]\n\t\t\t\tsrc (tensor): source indices | size : [S x BS]\n\t\t\t\ttrg (tensor): target indices | size : [T x BS]\n\t\t\tReturns:\n\t\t\t\toutput (tensor) : Network output | size : [T-1 x BS x voc2.nwords]\n\t\t'''\n\n\t\tif self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n\t\t\tself.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n\n\t\t# trg_mask when T-1 = 4: [When decoding for position i, only indexes with 0 in the ith row are attended over]\n\t\t# tensor([[0., -inf, -inf, -inf],\n\t\t# \t\t[0., 0., -inf, -inf],\n\t\t# \t\t[0., 0., 0., -inf],\n\t\t# \t\t[0., 0., 0., 0.],\n\n\t\tif self.config.embedding == 'bert' or self.config.embedding == 'roberta':\n\t\t\tsrc, src_tokens = self.embedding1(ques)\n\t\t\tsrc = src.transpose(0,1)\n\t\t\tif isinstance(src_tokens, list):\n\t\t\t\tsrc_tokens = torch.tensor(src_tokens).to(device)  # Convert to tensor and move to device if needed\n\t\t\t# src: Tensor [S x BS x d_model]\n# \t\t\tprint(src_tokens, src.shape)\n\t\t\tsrc_pad_mask = self.make_len_mask(src_tokens.transpose(0,1))\n\t\t\tsrc = self.pos_embedding1(src)\n\t\telse:\n\t\t\tsrc_pad_mask = self.make_len_mask(src)\n\t\t\tsrc = self.embedding1(src)\n\t\t\tsrc = self.pos_embedding1(src)\n\n\t\ttrg_pad_mask = self.make_len_mask(trg)\n\t\ttrg = self.embedding2(trg)\n\t\ttrg = self.pos_embedding2(trg)\n\n\t\toutput = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask, memory_mask=self.memory_mask,\n\t\t\t\t\t\t\t\t  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask, memory_key_padding_mask=src_pad_mask)\n\t\t\n\t\toutput = self.fc_out(output)\n\n\t\treturn output\n\n\tdef trainer(self, ques, input_seq1, input_seq2, config, device=None ,logger=None):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tques (list): raw source input | size : [BS]\n\t\t\t\tinput_seq1 (tensor): source indices | size : [S x BS]\n\t\t\t\tinput_seq2 (tensor): target indices | size : [T x BS]\n\t\t\tReturns:\n\t\t\t\tfin_loss (float) : Train Loss\n\t\t'''\n\n\t\tself.optimizer.zero_grad() # zero out gradients from previous backprop computations\n\n\t\toutput = self.forward(ques, input_seq1, input_seq2[:-1,:])\n\t\t# output: (T-1) x BS x voc2.nwords [T-1 because it predicts after start symbol]\n        \n\t\toutput_dim = output.shape[-1]\n\t\tself.loss = self.criterion(output.reshape(-1, output_dim), input_seq2[1:,:].reshape(-1))\n\n\t\tself.loss.backward()\n\t\tif self.config.max_grad_norm > 0:\n\t\t\ttorch.nn.utils.clip_grad_norm_(self.params, self.config.max_grad_norm)\n\t\tself.optimizer.step()\n\n\t\tfin_loss = self.loss.item()\n\n\t\treturn fin_loss\n\n\tdef greedy_decode(self, ques=None, input_seq1=None, input_seq2=None, input_len2 = None, validation=False):\n\t\t'''\n\t\t\tArgs:\n\t\t\t\tques (list): raw source input | size : [BS]\n\t\t\t\tinput_seq1 (tensor): source indices | size : [S x BS]\n\t\t\t\tinput_seq2 (tensor): target indices | size : [T x BS]\n\t\t\t\tinput_len2 (list): lengths of targets | size: [BS]\n\t\t\t\tvalidation (bool): whether validate\n\t\t\tReturns:\n\t\t\t\tif validation:\n\t\t\t\t\tvalidation loss (float): Validation loss\n\t\t\t\t\tdecoded_words (list): predicted equations | size : [BS x target_len]\n\t\t\t\telse:\n\t\t\t\t\tdecoded_words (list): predicted equations | size : [BS x target_len]\n\t\t'''\n\n\t\twith torch.no_grad():\n\t\t\tloss = 0.0\n\n\t\t\tif self.config.embedding == 'bert' or self.config.embedding == 'roberta':\n\t\t\t\tsrc, _ = self.embedding1(ques)\n\t\t\t\tsrc = src.transpose(0,1)\n\t\t\t\t# src: Tensor [S x BS x emb1_size]\n\t\t\t\tmemory = self.transformer.encoder(self.pos_embedding1(src))\n\t\t\telse: \n\t\t\t\tmemory = self.transformer.encoder(self.pos_embedding1(self.embedding1(input_seq1)))\n\t\t\t# memory: S x BS x d_model\n\n\t\t\tinput_list = [[self.SOS_token for i in range(input_seq1.size(1))]]\n\n\t\t\tdecoded_words = [[] for i in range(input_seq1.size(1))]\n\n\t\t\tif validation:\n\t\t\t\ttarget_len = max(input_len2)\n\t\t\telse:\n\t\t\t\ttarget_len = self.config.max_length\n\n\t\t\tfor step in range(target_len):\n\t\t\t\tdecoder_input = torch.LongTensor(input_list).to(self.device) # seq_len x bs\n\n\t\t\t\tdecoder_output = self.fc_out(self.transformer.decoder(self.pos_embedding2(self.embedding2(decoder_input)), memory)) # seq_len x bs x voc2.nwords\n\n\t\t\t\tif validation:\n\t\t\t\t\tloss += self.criterion(decoder_output[-1,:,:], input_seq2[step])\n\n\t\t\t\tout_tokens = decoder_output.argmax(2)[-1,:] # bs\n\n\t\t\t\tfor i in range(input_seq1.size(1)):\n\t\t\t\t\tif out_tokens[i].item() == self.EOS_token:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tdecoded_words[i].append(self.voc2.get_word(out_tokens[i].item()))\n\t\t\t\t\n\t\t\t\tinput_list.append(out_tokens.detach().tolist())\n\n\t\t\tif validation:\n\t\t\t\t\treturn loss/target_len, decoded_words\n\t\t\telse:\n\t\t\t\treturn decoded_words\n\ndef build_model(config, voc1, voc2, device, logger):\n\t'''\n\t\tArgs:\n\t\t\tconfig (dict): command line arguments\n\t\t\tvoc1 (object of class Voc1): vocabulary of source\n\t\t\tvoc2 (object of class Voc2): vocabulary of target\n\t\t\tdevice (torch.device): GPU device\n\t\t\tlogger (logger): logger variable to log messages\n\t\tReturns:\n\t\t\tmodel (object of class TransformerModel): model \n\t'''\n\n\tmodel = TransformerModel(config, voc1, voc2, device, logger)\n\tmodel = model.to(device)\n\n\treturn model\n\ndef train_model(model, train_dataloader, val_dataloader, voc1, voc2, device, config, logger, epoch_offset= 0, min_val_loss=float('inf'), \n\t\t\t\tmax_val_bleu=0.0, max_val_acc = 0.0, min_train_loss=float('inf'), max_train_acc = 0.0, best_epoch = 0, writer= None):\n\t'''\n\t\tArgs:\n\t\t\tmodel (object of class TransformerModel): model\n\t\t\ttrain_dataloader (object of class Dataloader): dataloader for train set\n\t\t\tval_dataloader (object of class Dataloader): dataloader for dev set\n\t\t\tvoc1 (object of class Voc1): vocabulary of source\n\t\t\tvoc2 (object of class Voc2): vocabulary of target\n\t\t\tdevice (torch.device): GPU device\n\t\t\tconfig (dict): command line arguments\n\t\t\tlogger (logger): logger variable to log messages\n\t\t\tepoch_offset (int): How many epochs of training already done\n\t\t\tmin_val_loss (float): minimum validation loss\n\t\t\tmax_val_bleu (float): maximum valiadtion bleu score\n\t\t\tmax_val_acc (float): maximum validation accuracy score\n\t\t\tmin_train_loss (float): minimum train loss\n\t\t\tmax_train_acc (float): maximum train accuracy\n\t\t\tbest_epoch (int): epoch with highest validation accuracy\n\t\t\twriter (object of class SummaryWriter): writer for Tensorboard\n\t\tReturns:\n\t\t\tmax_val_acc (float): maximum validation accuracy score\n\t'''\n\n\tif config.histogram and config.save_writer and writer:\n\t\tfor name, param in model.named_parameters():\n\t\t\twriter.add_histogram(name, param, epoch_offset)\n\t\n\testop_count=0\n\t\n\tfor epoch in range(1, config.epochs + 1):\n\t\tod = OrderedDict()\n\t\tod['Epoch'] = epoch + epoch_offset\n\t\tprint_log(logger, od)\n\n\t\tbatch_num = 1\n\t\ttrain_loss_epoch = 0.0\n\t\ttrain_acc_epoch = 0.0\n\t\ttrain_acc_epoch_cnt = 0.0\n\t\ttrain_acc_epoch_tot = 0.0\n\t\tval_loss_epoch = 0.0\n\n\t\tstart_time= time()\n\t\ttotal_batches = len(train_dataloader)\n\n\t\tfor data in train_dataloader:\n\t\t\tques = data['ques']\n\n\t\t\tsent1s = sents_to_idx(voc1, data['ques'], config.max_length, flag=0)\n\t\t\tsent2s = sents_to_idx(voc2, data['eqn'], config.max_length, flag=1)\n\t\t\tsent1_var, sent2_var, input_len1, input_len2  = process_batch(sent1s, sent2s, voc1, voc2, device)\n\n\t\t\tnums = data['nums']\n\t\t\tans = data['ans']\n\n\t\t\tmodel.train()\n\n\t\t\tloss = model.trainer(ques, sent1_var, sent2_var, config, device, logger)\n\t\t\ttrain_loss_epoch += loss\n\n\t\t\tif config.show_train_acc:\n\t\t\t\tmodel.eval()\n\n\t\t\t\t_, decoder_output = model.greedy_decode(ques, sent1_var, sent2_var, input_len2, validation=True)\n\t\t\t\ttemp_acc_cnt, temp_acc_tot, _ = cal_score(decoder_output, nums, ans)\n\t\t\t\ttrain_acc_epoch_cnt += temp_acc_cnt\n\t\t\t\ttrain_acc_epoch_tot += temp_acc_tot\n\n\t\t\tprint(\"Completed {} / {}...\".format(batch_num, total_batches), end = '\\r', flush = True)\n\t\t\tbatch_num+=1\n\n\t\ttrain_loss_epoch = train_loss_epoch / len(train_dataloader)\n\t\tif config.show_train_acc:\n\t\t\ttrain_acc_epoch = train_acc_epoch_cnt/train_acc_epoch_tot\n\t\telse:\n\t\t\ttrain_acc_epoch = 0.0\n\n\t\ttime_taken = (time() - start_time)/60.0\n\n\t\tif config.save_writer and writer:\n\t\t\twriter.add_scalar('loss/train_loss', train_loss_epoch, epoch + epoch_offset)\n\n\t\tlogger.debug('Training for epoch {} completed...\\nTime Taken: {}'.format(epoch, time_taken))\n\t\tlogger.debug('Starting Validation')\n\n\t\tval_bleu_epoch, val_loss_epoch, val_acc_epoch = run_validation(config=config, model=model, val_dataloader=val_dataloader, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvoc1=voc1, voc2=voc2, device=device, logger=logger, epoch_num = epoch)\n\n\t\tif train_loss_epoch < min_train_loss:\n\t\t\tmin_train_loss = train_loss_epoch\n\n\t\tif train_acc_epoch > max_train_acc:\n\t\t\tmax_train_acc = train_acc_epoch\n\n\t\tif val_bleu_epoch[0] > max_val_bleu:\n\t\t\tmax_val_bleu = val_bleu_epoch[0]\n\n\t\tif val_loss_epoch < min_val_loss:\n\t\t\tmin_val_loss = val_loss_epoch\n\n\t\tif val_acc_epoch > max_val_acc:\n\t\t\tmax_val_acc = val_acc_epoch\n\t\t\tbest_epoch = epoch + epoch_offset\n\n\t\t\tstate = {\n\t\t\t\t'epoch' : epoch + epoch_offset,\n\t\t\t\t'best_epoch': best_epoch,\n\t\t\t\t'model_state_dict': model.state_dict(),\n\t\t\t\t'voc1': model.voc1,\n\t\t\t\t'voc2': model.voc2,\n\t\t\t\t'optimizer_state_dict': model.optimizer.state_dict(),\n\t\t\t\t'train_loss_epoch' : train_loss_epoch,\n\t\t\t\t'min_train_loss' : min_train_loss,\n\t\t\t\t'train_acc_epoch' : train_acc_epoch,\n\t\t\t\t'max_train_acc' : max_train_acc,\n\t\t\t\t'val_loss_epoch' : val_loss_epoch,\n\t\t\t\t'min_val_loss' : min_val_loss,\n\t\t\t\t'val_acc_epoch' : val_acc_epoch,\n\t\t\t\t'max_val_acc' : max_val_acc,\n\t\t\t\t'val_bleu_epoch': val_bleu_epoch[0],\n\t\t\t\t'max_val_bleu': max_val_bleu\n\t\t\t}\n\t\t\tlogger.debug('Validation Bleu: {}'.format(val_bleu_epoch[0]))\n\n\t\t\tif config.save_model:\n\t\t\t\tsave_checkpoint(state, epoch + epoch_offset, logger, config.model_path, config.ckpt)\n\t\t\testop_count = 0\n\t\telse:\n\t\t\testop_count+=1\n\n\t\tif config.save_writer and writer:\n\t\t\twriter.add_scalar('loss/val_loss', val_loss_epoch, epoch + epoch_offset)\n\t\t\twriter.add_scalar('acc/val_score', val_score_epoch[0], epoch + epoch_offset)\n\n\t\tod = OrderedDict()\n\t\tod['Epoch'] = epoch + epoch_offset\n\t\tod['best_epoch'] = best_epoch\n\t\tod['train_loss_epoch'] = train_loss_epoch\n\t\tod['min_train_loss'] = min_train_loss\n\t\tod['val_loss_epoch']= val_loss_epoch\n\t\tod['min_val_loss']= min_val_loss\n\t\tod['train_acc_epoch'] = train_acc_epoch\n\t\tod['max_train_acc'] = max_train_acc\n\t\tod['val_acc_epoch'] = val_acc_epoch\n\t\tod['max_val_acc'] = max_val_acc\n\t\tod['val_bleu_epoch'] = val_bleu_epoch\n\t\tod['max_val_bleu'] = max_val_bleu\n\t\tprint_log(logger, od)\n\n\t\tif config.histogram and config.save_writer and writer:\n\t\t\tfor name, param in model.named_parameters():\n\t\t\t\twriter.add_histogram(name, param, epoch + epoch_offset)\n\n\t\tif estop_count >config.early_stopping:\n\t\t\tlogger.debug('Early Stopping at Epoch: {} after no improvement in {} epochs'.format(epoch, estop_count))\n\t\t\tbreak\n\n\tif config.save_writer:\n\t\twriter.export_scalars_to_json(os.path.join(config.board_path, 'all_scalars.json'))\n\t\twriter.close()\n\n\tlogger.info('Training Completed for {} epochs'.format(config.epochs))\n\n\tif config.results:\n\t\tstore_results(config, max_val_bleu, max_val_acc, min_val_loss, max_train_acc, min_train_loss, best_epoch)\n\t\tlogger.info('Scores saved at {}'.format(config.result_path))\n\n\treturn max_val_acc\n\ndef run_validation(config, model, val_dataloader, voc1, voc2, device, logger, epoch_num, validation = True):\n\t'''\n\t\tArgs:\n\t\t\tconfig (dict): command line arguments\n\t\t\tmodel (object of class TransformerModel): model\n\t\t\tval_dataloader (object of class Dataloader): dataloader for dev set\n\t\t\tvoc1 (object of class Voc1): vocabulary of source\n\t\t\tvoc2 (object of class Voc2): vocabulary of target\n\t\t\tdevice (torch.device): GPU device\n\t\t\tlogger (logger): logger variable to log messages\n\t\t\tepoch_num (int): Ongoing epoch number\n\t\t\tvalidation (bool): whether validating\n\t\tReturns:\n\t\t\tif config.mode == 'test':\n\t\t\t\tmax_test_acc (float): maximum test accuracy obtained\n\t\t\telse:\n\t\t\t\tval_bleu_epoch (float): validation bleu score for this epoch\n\t\t\t\tval_loss_epoch (float): va;iadtion loss for this epoch\n\t\t\t\tval_acc (float): validation accuracy score for this epoch\n\t'''\n\n\tbatch_num = 1\n\tval_loss_epoch = 0.0\n\tval_bleu_epoch = 0.0\n\tval_acc_epoch = 0.0\n\tval_acc_epoch_cnt = 0.0\n\tval_acc_epoch_tot = 0.0\n\n\tmodel.eval() # Set specific layers such as dropout to evaluation mode\n\n\trefs= []\n\thyps= []\n\n\tif config.mode == 'test':\n\t\tquestions, gen_eqns, act_eqns, scores = [], [], [], []\n\n\tdisplay_n = config.batch_size\n\n\twith open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n\t\tf_out.write('---------------------------------------\\n')\n\t\tf_out.write('Epoch: ' + str(epoch_num) + '\\n')\n\t\tf_out.write('---------------------------------------\\n')\n\ttotal_batches = len(val_dataloader)\n\tfor data in val_dataloader:\n\t\tsent1s = sents_to_idx(voc1, data['ques'], config.max_length, flag = 0)\n\t\tsent2s = sents_to_idx(voc2, data['eqn'], config.max_length, flag = 0)\n\t\tnums = data['nums']\n\t\tans = data['ans']\n\t\tif config.grade_disp:\n\t\t\tgrade = data['grade']\n\t\tif config.type_disp:\n\t\t\ttype1 = data['type']\n\t\tif config.challenge_disp:\n\t\t\ttype1 = data['type']\n\t\t\tvar_type = data['var_type']\n\t\t\tannotator = data['annotator']\n\t\t\talternate = data['alternate']\n\n\t\tques = data['ques']\n\n\t\tsent1_var, sent2_var, input_len1, input_len2 = process_batch(sent1s, sent2s, voc1, voc2, device)\n\n\t\tval_loss, decoder_output = model.greedy_decode(ques, sent1_var, sent2_var, input_len2, validation=True)\n\n\t\ttemp_acc_cnt, temp_acc_tot, disp_corr = cal_score(decoder_output, nums, ans)\n\t\tval_acc_epoch_cnt += temp_acc_cnt\n\t\tval_acc_epoch_tot += temp_acc_tot\n\n\t\tsent1s = idx_to_sents(voc1, sent1_var, no_eos= True)\n\t\tsent2s = idx_to_sents(voc2, sent2_var, no_eos= True)\n\n\t\trefs += [[' '.join(sent2s[i])] for i in range(sent2_var.size(1))]\n\t\thyps += [' '.join(decoder_output[i]) for i in range(sent1_var.size(1))]\n\n\t\tif config.mode == 'test':\n\t\t\tquestions+= data['ques']\n\t\t\tgen_eqns += [' '.join(decoder_output[i]) for i in range(sent1_var.size(1))]\n\t\t\tact_eqns += [' '.join(sent2s[i]) for i in range(sent2_var.size(1))]\n\t\t\tscores   += [cal_score([decoder_output[i]], [nums[i]], [ans[i]], [data['eqn'][i]])[0] for i in range(sent1_var.size(1))]\n\n\t\twith open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n\t\t\tf_out.write('Batch: ' + str(batch_num) + '\\n')\n\t\t\tf_out.write('---------------------------------------\\n')\n\t\t\tfor i in range(len(sent1s[:display_n])):\n\t\t\t\ttry:\n\t\t\t\t\tf_out.write('Example: ' + str(i) + '\\n')\n\t\t\t\t\tif config.grade_disp:\n\t\t\t\t\t\tf_out.write('Grade: ' + str(grade[i].item()) + '\\n')\n\t\t\t\t\tif config.type_disp:\n\t\t\t\t\t\tf_out.write('Type: ' + str(type1[i]) + '\\n')\n\t\t\t\t\tf_out.write('Source: ' + stack_to_string(sent1s[i]) + '\\n')\n\t\t\t\t\tf_out.write('Target: ' + stack_to_string(sent2s[i]) + '\\n')\n\t\t\t\t\tf_out.write('Generated: ' + stack_to_string(decoder_output[i]) + '\\n')\n\t\t\t\t\tif config.challenge_disp:\n\t\t\t\t\t\tf_out.write('Type: ' + str(type1[i]) + '\\n')\n\t\t\t\t\t\tf_out.write('Variation Type: ' + str(var_type[i]) + '\\n')\n\t\t\t\t\t\tf_out.write('Annotator: ' + str(annotator[i]) + '\\n')\n\t\t\t\t\t\tf_out.write('Alternate: ' + str(alternate[i].item()) + '\\n')\n\t\t\t\t\tif config.nums_disp:\n\t\t\t\t\t\tsrc_nums = 0\n\t\t\t\t\t\ttgt_nums = 0\n\t\t\t\t\t\tpred_nums = 0\n\t\t\t\t\t\tfor k in range(len(sent1s[i])):\n\t\t\t\t\t\t\tif sent1s[i][k][:6] == 'number':\n\t\t\t\t\t\t\t\tsrc_nums += 1\n\t\t\t\t\t\tfor k in range(len(sent2s[i])):\n\t\t\t\t\t\t\tif sent2s[i][k][:6] == 'number':\n\t\t\t\t\t\t\t\ttgt_nums += 1\n\t\t\t\t\t\tfor k in range(len(decoder_output[i])):\n\t\t\t\t\t\t\tif decoder_output[i][k][:6] == 'number':\n\t\t\t\t\t\t\t\tpred_nums += 1\n\t\t\t\t\t\tf_out.write('Numbers in question: ' + str(src_nums) + '\\n')\n\t\t\t\t\t\tf_out.write('Numbers in Target Equation: ' + str(tgt_nums) + '\\n')\n\t\t\t\t\t\tf_out.write('Numbers in Predicted Equation: ' + str(pred_nums) + '\\n')\n\t\t\t\t\tf_out.write('Result: ' + str(disp_corr[i]) + '\\n' + '\\n')\n\t\t\t\texcept:\n\t\t\t\t\tlogger.warning('Exception: Failed to generate')\n\t\t\t\t\tpdb.set_trace()\n\t\t\t\t\tbreak\n\t\t\tf_out.write('---------------------------------------\\n')\n\t\t\tf_out.close()\n\n\t\tif batch_num % config.display_freq == 0:\n\t\t\tfor i in range(len(sent1s[:display_n])):\n\t\t\t\ttry:\n\t\t\t\t\tod = OrderedDict()\n\t\t\t\t\tlogger.info('-------------------------------------')\n\t\t\t\t\tod['Source'] = ' '.join(sent1s[i])\n\n\t\t\t\t\tod['Target'] = ' '.join(sent2s[i])\n\n\t\t\t\t\tod['Generated'] = ' '.join(decoder_output[i])\n\t\t\t\t\tprint_log(logger, od)\n\t\t\t\t\tlogger.info('-------------------------------------')\n\t\t\t\texcept:\n\t\t\t\t\tlogger.warning('Exception: Failed to generate')\n\t\t\t\t\tpdb.set_trace()\n\t\t\t\t\tbreak\n\n\t\tval_loss_epoch += val_loss\n\t\tprint(\"Completed {} / {}...\".format(batch_num, total_batches), end = '\\r', flush = True)\n\t\tbatch_num += 1\n\n\tval_bleu_epoch = bleu_scorer(refs, hyps)\n\tif config.mode == 'test':\n\t\tresults_df = pd.DataFrame([questions, act_eqns, gen_eqns, scores]).transpose()\n\t\tresults_df.columns = ['Question', 'Actual Equation', 'Generated Equation', 'Score']\n\t\tcsv_file_path = os.path.join(config.outputs_path, config.dataset+'.csv')\n\t\tresults_df.to_csv(csv_file_path, index = False)\n\t\treturn sum(scores)/len(scores)\n\n\tval_acc_epoch = val_acc_epoch_cnt/val_acc_epoch_tot\n\n\treturn val_bleu_epoch, val_loss_epoch/len(val_dataloader), val_acc_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:55:14.814762Z","iopub.execute_input":"2024-11-17T15:55:14.815127Z","iopub.status.idle":"2024-11-17T15:55:14.911751Z","shell.execute_reply.started":"2024-11-17T15:55:14.815085Z","shell.execute_reply":"2024-11-17T15:55:14.910889Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Main.py","metadata":{}},{"cell_type":"code","source":"global log_folder\nglobal model_folder\nglobal result_folder\nglobal data_path\nglobal board_path\n\nlog_folder = 'logs'\nmodel_folder = 'models'\noutputs_folder = 'outputs'\nresult_folder = './out/'\ndata_path = '/kaggle/input/svamp-dataset/data'\nboard_path = './runs/'\n\ndef load_data(config, logger):\n\t'''\n\t\tLoads the data from the datapath in torch dataset form\n\n\t\tArgs:\n\t\t\tconfig (dict) : configuration/args\n\t\t\tlogger (logger) : logger object for logging\n\n\t\tReturns:\n\t\t\tdataloader(s) \n\t'''\n\t\n\tif config.mode == 'train':\n\t\tlogger.debug('Loading Training Data...')\n\n\t\t'''Load Datasets'''\n\t\ttrain_set = TextDataset(data_path=data_path, dataset=config.dataset,\n\t\t\t\t\t\t\t\tdatatype='train', max_length=config.max_length, is_debug=config.debug, is_train=True)\n\t\tval_set = TextDataset(data_path=data_path, dataset=config.dataset,\n\t\t\t\t\t\t\t  datatype='dev', max_length=config.max_length, is_debug=config.debug, grade_info=config.grade_disp, \n\t\t\t\t\t\t\t  type_info=config.type_disp, challenge_info=config.challenge_disp)\n\n\t\t'''In case of sort by length, write a different case with shuffle=False '''\n\t\ttrain_dataloader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True, num_workers=5)\n\t\tval_dataloader = DataLoader(val_set, batch_size=config.batch_size, shuffle=True, num_workers=5)\n\n\t\ttrain_size = len(train_dataloader) * config.batch_size\n\t\tval_size = len(val_dataloader)* config.batch_size\n\t\t\n\t\tmsg = 'Training and Validation Data Loaded:\\nTrain Size: {}\\nVal Size: {}'.format(train_size, val_size)\n\t\tlogger.info(msg)\n\n\t\treturn train_dataloader, val_dataloader\n\n\telif config.mode == 'test':\n\t\tlogger.debug('Loading Test Data...')\n\n\t\ttest_set = TextDataset(data_path=data_path, dataset=config.dataset,\n\t\t\t\t\t\t\t   datatype='test', max_length=config.max_length, is_debug=config.debug)\n\t\ttest_dataloader = DataLoader(\n\t\t\ttest_set, batch_size=config.batch_size, shuffle=True, num_workers=5)\n\n\t\tlogger.info('Test Data Loaded...')\n\t\treturn test_dataloader\n\n\telse:\n\t\tlogger.critical('Invalid Mode Specified')\n\t\traise Exception('{} is not a valid mode'.format(config.mode))\n\n\n'''read arguments'''\n\nkaggle_args = {\n    'debug': False,\n    'mode': 'train',\n    'gpu': 0,\n    'dropout': 0.1,\n    'heads': 4,\n    'encoder_layers': 1,\n    'decoder_layers': 1,\n    'd_model': 768,\n    'd_ff': 256,\n    'lr': 0.0001,\n    'emb_lr': 1e-5,\n    'batch_size': 16,\n    'epochs': 10,\n    'embedding': 'roberta',\n    'emb_name': 'roberta-base',\n    'mawps_vocab': True,\n    'dataset': 'mawps-asdiv-a_svamp',\n    'run_name': 'mawps_try1',\n}\n\n\n\n\nconfig =  parse_arguments(kaggle_args)\n\nmode = config.mode\nif mode == 'train':\n    is_train = True\nelse:\n    is_train = False\n\n''' Set seed for reproducibility'''\nnp.random.seed(config.seed)\ntorch.manual_seed(config.seed)\nrandom.seed(config.seed)\n\n'''GPU initialization'''\ndevice = gpu_init_pytorch(config.gpu)\n\nif config.full_cv:\n    global data_path \n    data_name = config.dataset\n    data_path = data_path + data_name + '/'\n    config.val_result_path = os.path.join(result_folder, 'CV_results_{}.json'.format(data_name))\n    fold_acc_score = 0.0\n    folds_scores = []\n    for z in range(5):\n        run_name = config.run_name + '_fold' + str(z)\n        config.dataset = 'fold' + str(z)\n        config.log_path = os.path.join(log_folder, run_name)\n        config.model_path = os.path.join(model_folder, run_name)\n        config.board_path = os.path.join(board_path, run_name)\n        config.outputs_path = os.path.join(outputs_folder, run_name)\n\n        vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n        vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n        config_file = os.path.join(config.model_path, 'config.p')\n        log_file = os.path.join(config.log_path, 'log.txt')\n\n        if config.results:\n            config.result_path = os.path.join(result_folder, 'val_results_{}_{}.json'.format(data_name, config.dataset))\n\n        if is_train:\n            create_save_directories(config.log_path)\n            create_save_directories(config.model_path)\n            create_save_directories(config.outputs_path)\n        else:\n            create_save_directories(config.log_path)\n            create_save_directories(config.result_path)\n\n        logger = get_logger(run_name, log_file, logging.DEBUG)\n        writer = SummaryWriter(config.board_path)\n\n        logger.debug('Created Relevant Directories')\n        logger.info('Experiment Name: {}'.format(config.run_name))\n\n        '''Read Files and create/load Vocab'''\n        if is_train:\n            train_dataloader, val_dataloader = load_data(config, logger)\n\n            logger.debug('Creating Vocab...')\n\n            voc1 = Voc1()\n            voc1.create_vocab_dict(config, train_dataloader)\n\n            # Removed\n            # voc1.add_to_vocab_dict(config, val_dataloader)\n\n            voc2 = Voc2(config)\n            voc2.create_vocab_dict(config, train_dataloader)\n\n            # Removed\n            # voc2.add_to_vocab_dict(config, val_dataloader)\n\n            logger.info('Vocab Created with number of words : {}'.format(voc1.nwords))\n\n            with open(vocab1_path, 'wb') as f:\n                pickle.dump(voc1, f, protocol=pickle.HIGHEST_PROTOCOL)\n            with open(vocab2_path, 'wb') as f:\n                pickle.dump(voc2, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n            logger.info('Vocab saved at {}'.format(vocab1_path))\n\n        else:\n            test_dataloader = load_data(config, logger)\n            logger.info('Loading Vocab File...')\n\n            with open(vocab1_path, 'rb') as f:\n                voc1 = pickle.load(f)\n            with open(vocab2_path, 'rb') as f:\n                voc2 = pickle.load(f)\n\n            logger.info('Vocab Files loaded from {}\\nNumber of Words: {}'.format(vocab1_path, voc1.nwords))\n\n        # TO DO : Load Existing Checkpoints here\n        checkpoint = get_latest_checkpoint(config.model_path, logger)\n\n        if is_train:\n            model = build_model(config=config, voc1=voc1, voc2=voc2, device=device, logger=logger)\n\n            logger.info('Initialized Model')\n\n            if checkpoint == None:\n                min_val_loss = torch.tensor(float('inf')).item()\n                min_train_loss = torch.tensor(float('inf')).item()\n                max_val_bleu = 0.0\n                max_val_acc = 0.0\n                max_train_acc = 0.0\n                best_epoch = 0\n                epoch_offset = 0\n            else:\n                epoch_offset, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2 = \\\n                                                                    load_checkpoint(model, config.mode, checkpoint, logger, device)\n\n            with open(config_file, 'wb') as f:\n                pickle.dump(vars(config), f, protocol=pickle.HIGHEST_PROTOCOL)\n\n            logger.debug('Config File Saved')\n\n            logger.info('Starting Training Procedure')\n            max_val_acc = train_model(model, train_dataloader, val_dataloader, voc1, voc2, device, config, logger, \n                        epoch_offset, min_val_loss, max_val_bleu, max_val_acc, min_train_loss, max_train_acc, best_epoch, writer)\n\n        else:\n            gpu = config.gpu\n            mode = config.mode\n            dataset = config.dataset\n            batch_size = config.batch_size\n            with open(config_file, 'rb') as f:\n                config = AttrDict(pickle.load(f))\n                config.gpu = gpu\n                config.mode = mode\n                config.dataset = dataset\n                config.batch_size = batch_size\n\n            with open(config_file, 'rb') as f:\n                config = AttrDict(pickle.load(f))\n                config.gpu = gpu\n\n            model = build_model(config=config, voc1=voc1, voc2=voc2, device=device, logger=logger)\n\n            epoch_offset, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2 = \\\n                                                                    load_checkpoint(model, config.mode, checkpoint, logger, device)\n\n            logger.info('Prediction from')\n            od = OrderedDict()\n            od['epoch'] = ep_offset\n            od['min_train_loss'] = min_train_loss\n            od['min_val_loss'] = min_val_loss\n            od['max_train_acc'] = max_train_acc\n            od['max_val_acc'] = max_val_acc\n            od['max_val_bleu'] = max_val_bleu\n            od['best_epoch'] = best_epoch\n            print_log(logger, od)\n\n            test_acc_epoch = run_validation(config, model, test_dataloader, voc1, voc2, device, logger, 0)\n            logger.info('Accuracy: {}'.format(test_acc_epoch))\n\n        fold_acc_score += max_val_acc\n        folds_scores.append(max_val_acc)\n\n    fold_acc_score = fold_acc_score/5\n    store_val_results(config, fold_acc_score, folds_scores)\n    logger.info('Final Val score: {}'.format(fold_acc_score))\n\nelse:\n    '''Run Config files/paths'''\n    run_name = config.run_name\n    config.log_path = os.path.join(log_folder, run_name)\n    config.model_path = os.path.join(model_folder, run_name)\n    config.board_path = os.path.join(board_path, run_name)\n    config.outputs_path = os.path.join(outputs_folder, run_name)\n\n    vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n    vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n    config_file = os.path.join(config.model_path, 'config.p')\n    log_file = os.path.join(config.log_path, 'log.txt')\n\n    if config.results:\n        config.result_path = os.path.join(result_folder, 'val_results_{}.json'.format(config.dataset))\n\n    if is_train:\n        create_save_directories(config.log_path)\n        create_save_directories(config.model_path)\n        create_save_directories(config.outputs_path)\n    else:\n        create_save_directories(config.log_path)\n        create_save_directories(config.result_path)\n\n    logger = get_logger(run_name, log_file, logging.DEBUG)\n    writer = SummaryWriter(config.board_path)\n\n    logger.debug('Created Relevant Directories')\n    logger.info('Experiment Name: {}'.format(config.run_name))\n\n    '''Read Files and create/load Vocab'''\n    if is_train:\n        train_dataloader, val_dataloader = load_data(config, logger)\n\n        logger.debug('Creating Vocab...')\n\n        voc1 = Voc1()\n        voc1.create_vocab_dict(config, train_dataloader)\n\n        # Removed\n        # voc1.add_to_vocab_dict(config, val_dataloader)\n\n        voc2 = Voc2(config)\n        voc2.create_vocab_dict(config, train_dataloader)\n\n        # Removed\n        # voc2.add_to_vocab_dict(config, val_dataloader)\n\n        logger.info('Vocab Created with number of words : {}'.format(voc1.nwords))\n\n        with open(vocab1_path, 'wb') as f:\n            pickle.dump(voc1, f, protocol=pickle.HIGHEST_PROTOCOL)\n        with open(vocab2_path, 'wb') as f:\n            pickle.dump(voc2, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.info('Vocab saved at {}'.format(vocab1_path))\n\n    else:\n        test_dataloader = load_data(config, logger)\n        logger.info('Loading Vocab File...')\n\n        with open(vocab1_path, 'rb') as f:\n            voc1 = pickle.load(f)\n        with open(vocab2_path, 'rb') as f:\n            voc2 = pickle.load(f)\n\n        logger.info('Vocab Files loaded from {}\\nNumber of Words: {}'.format(vocab1_path, voc1.nwords))\n\n    # Load Existing Checkpoints here\n    checkpoint = get_latest_checkpoint(config.model_path, logger)\n\n    if is_train:\n        model = build_model(config=config, voc1=voc1, voc2=voc2, device=device, logger=logger)\n\n        logger.info('Initialized Model')\n\n        if checkpoint == None:\n            min_val_loss = torch.tensor(float('inf')).item()\n            min_train_loss = torch.tensor(float('inf')).item()\n            max_val_bleu = 0.0\n            max_val_acc = 0.0\n            max_train_acc = 0.0\n            best_epoch = 0\n            epoch_offset = 0\n        else:\n            epoch_offset, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2 = \\\n                                                                load_checkpoint(model, config.mode, checkpoint, logger, device)\n\n        with open(config_file, 'wb') as f:\n            pickle.dump(vars(config), f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Config File Saved')\n\n        logger.info('Starting Training Procedure')\n        train_model(model, train_dataloader, val_dataloader, voc1, voc2, device, config, logger, \n                    epoch_offset, min_val_loss, max_val_bleu, max_val_acc, min_train_loss, max_train_acc, best_epoch, writer)\n\n    else:\n        gpu = config.gpu\n        mode = config.mode\n        dataset = config.dataset\n        batch_size = config.batch_size\n        with open(config_file, 'rb') as f:\n            config = AttrDict(pickle.load(f))\n            config.gpu = gpu\n            config.mode = mode\n            config.dataset = dataset\n            config.batch_size = batch_size\n\n        with open(config_file, 'rb') as f:\n            config = AttrDict(pickle.load(f))\n            config.gpu = gpu\n\n        model = build_model(config=config, voc1=voc1, voc2=voc2, device=device, logger=logger)\n\n        epoch_offset, min_train_loss, min_val_loss, max_train_acc, max_val_acc, max_val_bleu, best_epoch, voc1, voc2 = \\\n                                                                load_checkpoint(model, config.mode, checkpoint, logger, device)\n\n        logger.info('Prediction from')\n        od = OrderedDict()\n        od['epoch'] = ep_offset\n        od['min_train_loss'] = min_train_loss\n        od['min_val_loss'] = min_val_loss\n        od['max_train_acc'] = max_train_acc\n        od['max_val_acc'] = max_val_acc\n        od['max_val_bleu'] = max_val_bleu\n        od['best_epoch'] = best_epoch\n        print_log(logger, od)\n\n        test_acc_epoch = run_validation(config, model, test_dataloader, voc1, voc2, device, logger, 0)\n        logger.info('Accuracy: {}'.format(test_acc_epoch))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:00:47.885422Z","iopub.execute_input":"2024-11-17T17:00:47.885873Z","iopub.status.idle":"2024-11-17T17:08:36.796681Z","shell.execute_reply.started":"2024-11-17T17:00:47.885815Z","shell.execute_reply":"2024-11-17T17:08:36.795215Z"}},"outputs":[{"name":"stderr","text":"2024-11-17 17:00:47,948 | DEBUG | 1821039644.py: 279 : <module>() ::\t Created Relevant Directories\n2024-11-17 17:00:47,948 | DEBUG | 1821039644.py: 279 : <module>() ::\t Created Relevant Directories\n2024-11-17 17:00:47,951 | INFO | 1821039644.py: 280 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-17 17:00:47,951 | INFO | 1821039644.py: 280 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-17 17:00:47,952 | DEBUG | 1821039644.py: 27 : load_data() ::\t Loading Training Data...\n2024-11-17 17:00:47,952 | DEBUG | 1821039644.py: 27 : load_data() ::\t Loading Training Data...\n2024-11-17 17:00:48,011 | INFO | 1821039644.py: 44 : load_data() ::\t Training and Validation Data Loaded:\nTrain Size: 3152\nVal Size: 1008\n2024-11-17 17:00:48,011 | INFO | 1821039644.py: 44 : load_data() ::\t Training and Validation Data Loaded:\nTrain Size: 3152\nVal Size: 1008\n2024-11-17 17:00:48,013 | DEBUG | 1821039644.py: 286 : <module>() ::\t Creating Vocab...\n2024-11-17 17:00:48,013 | DEBUG | 1821039644.py: 286 : <module>() ::\t Creating Vocab...\n2024-11-17 17:00:48,860 | INFO | 1821039644.py: 300 : <module>() ::\t Vocab Created with number of words : 4023\n2024-11-17 17:00:48,860 | INFO | 1821039644.py: 300 : <module>() ::\t Vocab Created with number of words : 4023\n2024-11-17 17:00:48,865 | INFO | 1821039644.py: 307 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-17 17:00:48,865 | INFO | 1821039644.py: 307 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-17 17:00:48,867 | INFO | 2446051103.py: 261 : get_latest_checkpoint() ::\t Checkpoint found with epoch number : 1\n2024-11-17 17:00:48,867 | INFO | 2446051103.py: 261 : get_latest_checkpoint() ::\t Checkpoint found with epoch number : 1\n2024-11-17 17:00:48,869 | DEBUG | 2446051103.py: 262 : get_latest_checkpoint() ::\t Checkpoint found at : models/mawps_try1/model_1.pt\n2024-11-17 17:00:48,869 | DEBUG | 2446051103.py: 262 : get_latest_checkpoint() ::\t Checkpoint found at : models/mawps_try1/model_1.pt\n2024-11-17 17:00:48,870 | DEBUG | 2551679145.py: 40 : __init__() ::\t Initialising Embeddings.....\n2024-11-17 17:00:48,870 | DEBUG | 2551679145.py: 40 : __init__() ::\t Initialising Embeddings.....\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2024-11-17 17:00:49,431 | DEBUG | 2551679145.py: 63 : __init__() ::\t Embeddings initialised.....\n2024-11-17 17:00:49,431 | DEBUG | 2551679145.py: 63 : __init__() ::\t Embeddings initialised.....\n2024-11-17 17:00:49,433 | DEBUG | 2551679145.py: 64 : __init__() ::\t Building Transformer Model.....\n2024-11-17 17:00:49,433 | DEBUG | 2551679145.py: 64 : __init__() ::\t Building Transformer Model.....\n2024-11-17 17:00:49,558 | DEBUG | 2551679145.py: 72 : __init__() ::\t Transformer Model Built.....\n2024-11-17 17:00:49,558 | DEBUG | 2551679145.py: 72 : __init__() ::\t Transformer Model Built.....\n2024-11-17 17:00:49,560 | DEBUG | 2551679145.py: 78 : __init__() ::\t Initalizing Optimizer and Criterion...\n2024-11-17 17:00:49,560 | DEBUG | 2551679145.py: 78 : __init__() ::\t Initalizing Optimizer and Criterion...\n2024-11-17 17:00:49,565 | INFO | 2551679145.py: 84 : __init__() ::\t All Model Components Initialized...\n2024-11-17 17:00:49,565 | INFO | 2551679145.py: 84 : __init__() ::\t All Model Components Initialized...\n2024-11-17 17:00:49,768 | INFO | 1821039644.py: 326 : <module>() ::\t Initialized Model\n2024-11-17 17:00:49,768 | INFO | 1821039644.py: 326 : <module>() ::\t Initialized Model\n2024-11-17 17:00:51,119 | INFO | 2446051103.py: 308 : load_checkpoint() ::\t Successfully Loaded Checkpoint from models/mawps_try1/model_1.pt, with epoch number: 1 for train\n2024-11-17 17:00:51,119 | INFO | 2446051103.py: 308 : load_checkpoint() ::\t Successfully Loaded Checkpoint from models/mawps_try1/model_1.pt, with epoch number: 1 for train\n2024-11-17 17:00:51,201 | DEBUG | 1821039644.py: 343 : <module>() ::\t Config File Saved\n2024-11-17 17:00:51,201 | DEBUG | 1821039644.py: 343 : <module>() ::\t Config File Saved\n2024-11-17 17:00:51,203 | INFO | 1821039644.py: 345 : <module>() ::\t Starting Training Procedure\n2024-11-17 17:00:51,203 | INFO | 1821039644.py: 345 : <module>() ::\t Starting Training Procedure\n2024-11-17 17:00:51,204 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 2\t\n2024-11-17 17:00:51,204 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 2\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:01:31,041 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 1 completed...\nTime Taken: 0.6639152566591898\n2024-11-17 17:01:31,041 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 1 completed...\nTime Taken: 0.6639152566591898\n2024-11-17 17:01:31,043 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:01:31,043 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:01:35,957 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.25329970086839887\n2024-11-17 17:01:35,957 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.25329970086839887\n2024-11-17 17:01:35,959 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_2.pt\n2024-11-17 17:01:35,959 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_2.pt\n2024-11-17 17:01:38,225 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.5478257794821928\t\n min train loss: 0.5478257794821928\t\n val loss epoch: 1.746546983718872\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.49936265137029956\t\n max train acc: 0.49936265137029956\t\n val acc epoch: 0.16\t\n max val acc: 0.16\t\n val bleu epoch: (0.25329970086839887, [0.7104096008348552, 0.32509707024355805, 0.18330605564648117, 0.09723889555822329], 1.0, 1.1039746543778801, 3833, 3472)\t\n max val bleu: 0.25329970086839887\t\n2024-11-17 17:01:38,225 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.5478257794821928\t\n min train loss: 0.5478257794821928\t\n val loss epoch: 1.746546983718872\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.49936265137029956\t\n max train acc: 0.49936265137029956\t\n val acc epoch: 0.16\t\n max val acc: 0.16\t\n val bleu epoch: (0.25329970086839887, [0.7104096008348552, 0.32509707024355805, 0.18330605564648117, 0.09723889555822329], 1.0, 1.1039746543778801, 3833, 3472)\t\n max val bleu: 0.25329970086839887\t\n2024-11-17 17:01:38,227 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 3\t\n2024-11-17 17:01:38,227 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 3\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:02:17,462 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 2 completed...\nTime Taken: 0.6538920442263285\n2024-11-17 17:02:17,462 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 2 completed...\nTime Taken: 0.6538920442263285\n2024-11-17 17:02:17,464 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:02:17,464 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:02:22,402 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.23957236046792776\n2024-11-17 17:02:22,402 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.23957236046792776\n2024-11-17 17:02:22,404 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_3.pt\n2024-11-17 17:02:22,404 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_3.pt\n2024-11-17 17:02:24,687 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.38764385894167847\t\n min train loss: 0.38764385894167847\t\n val loss epoch: 2.2493715286254883\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.6606118546845124\t\n max train acc: 0.6606118546845124\t\n val acc epoch: 0.201\t\n max val acc: 0.201\t\n val bleu epoch: (0.23957236046792776, [0.7259673886590411, 0.3235767127693792, 0.17875770507349456, 0.07844905320108206], 1.0, 1.1834677419354838, 4109, 3472)\t\n max val bleu: 0.25329970086839887\t\n2024-11-17 17:02:24,687 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.38764385894167847\t\n min train loss: 0.38764385894167847\t\n val loss epoch: 2.2493715286254883\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.6606118546845124\t\n max train acc: 0.6606118546845124\t\n val acc epoch: 0.201\t\n max val acc: 0.201\t\n val bleu epoch: (0.23957236046792776, [0.7259673886590411, 0.3235767127693792, 0.17875770507349456, 0.07844905320108206], 1.0, 1.1834677419354838, 4109, 3472)\t\n max val bleu: 0.25329970086839887\t\n2024-11-17 17:02:24,689 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 4\t\n2024-11-17 17:02:24,689 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 4\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:03:03,917 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 3 completed...\nTime Taken: 0.6537924766540527\n2024-11-17 17:03:03,917 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 3 completed...\nTime Taken: 0.6537924766540527\n2024-11-17 17:03:03,919 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:03:03,919 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:03:08,858 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.2659616140623929\n2024-11-17 17:03:08,858 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.2659616140623929\n2024-11-17 17:03:08,860 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_4.pt\n2024-11-17 17:03:08,860 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_4.pt\n2024-11-17 17:03:11,138 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.2940902719415989\t\n min train loss: 0.2940902719415989\t\n val loss epoch: 2.2732460498809814\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.751434034416826\t\n max train acc: 0.751434034416826\t\n val acc epoch: 0.236\t\n max val acc: 0.236\t\n val bleu epoch: (0.2659616140623929, [0.7161691542288557, 0.3400662251655629, 0.20544554455445543, 0.1], 1.0, 1.1578341013824884, 4020, 3472)\t\n max val bleu: 0.2659616140623929\t\n2024-11-17 17:03:11,138 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.2940902719415989\t\n min train loss: 0.2940902719415989\t\n val loss epoch: 2.2732460498809814\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.751434034416826\t\n max train acc: 0.751434034416826\t\n val acc epoch: 0.236\t\n max val acc: 0.236\t\n val bleu epoch: (0.2659616140623929, [0.7161691542288557, 0.3400662251655629, 0.20544554455445543, 0.1], 1.0, 1.1578341013824884, 4020, 3472)\t\n max val bleu: 0.2659616140623929\t\n2024-11-17 17:03:11,140 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 5\t\n2024-11-17 17:03:11,140 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 5\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:03:50,726 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 4 completed...\nTime Taken: 0.6597361922264099\n2024-11-17 17:03:50,726 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 4 completed...\nTime Taken: 0.6597361922264099\n2024-11-17 17:03:50,728 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:03:50,728 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:03:55,630 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.2839332879099206\n2024-11-17 17:03:55,630 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.2839332879099206\n2024-11-17 17:03:55,632 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_5.pt\n2024-11-17 17:03:55,632 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_5.pt\n2024-11-17 17:03:57,894 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 5\t\n best epoch: 5\t\n train loss epoch: 0.23652177177331773\t\n min train loss: 0.23652177177331773\t\n val loss epoch: 2.3999016284942627\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.8170809432759719\t\n max train acc: 0.8170809432759719\t\n val acc epoch: 0.238\t\n max val acc: 0.238\t\n val bleu epoch: (0.2839332879099206, [0.7239795918367347, 0.3506849315068493, 0.21770833333333334, 0.11758360302049622], 1.0, 1.1290322580645162, 3920, 3472)\t\n max val bleu: 0.2839332879099206\t\n2024-11-17 17:03:57,894 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 5\t\n best epoch: 5\t\n train loss epoch: 0.23652177177331773\t\n min train loss: 0.23652177177331773\t\n val loss epoch: 2.3999016284942627\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.8170809432759719\t\n max train acc: 0.8170809432759719\t\n val acc epoch: 0.238\t\n max val acc: 0.238\t\n val bleu epoch: (0.2839332879099206, [0.7239795918367347, 0.3506849315068493, 0.21770833333333334, 0.11758360302049622], 1.0, 1.1290322580645162, 3920, 3472)\t\n max val bleu: 0.2839332879099206\t\n2024-11-17 17:03:57,896 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 6\t\n2024-11-17 17:03:57,896 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 6\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:04:37,300 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 5 completed...\nTime Taken: 0.6567135532697042\n2024-11-17 17:04:37,300 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 5 completed...\nTime Taken: 0.6567135532697042\n2024-11-17 17:04:37,302 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:04:37,302 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:04:42,256 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.28077336632122357\n2024-11-17 17:04:42,256 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.28077336632122357\n2024-11-17 17:04:42,258 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_6.pt\n2024-11-17 17:04:42,258 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_6.pt\n2024-11-17 17:04:44,541 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 6\t\n best epoch: 6\t\n train loss epoch: 0.20443111632567673\t\n min train loss: 0.20443111632567673\t\n val loss epoch: 2.7854697704315186\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.8448056086679414\t\n max train acc: 0.8448056086679414\t\n val acc epoch: 0.246\t\n max val acc: 0.246\t\n val bleu epoch: (0.28077336632122357, [0.7308823529411764, 0.34155844155844156, 0.21682692307692308, 0.11481481481481481], 1.0, 1.1751152073732718, 4080, 3472)\t\n max val bleu: 0.2839332879099206\t\n2024-11-17 17:04:44,541 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 6\t\n best epoch: 6\t\n train loss epoch: 0.20443111632567673\t\n min train loss: 0.20443111632567673\t\n val loss epoch: 2.7854697704315186\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.8448056086679414\t\n max train acc: 0.8448056086679414\t\n val acc epoch: 0.246\t\n max val acc: 0.246\t\n val bleu epoch: (0.28077336632122357, [0.7308823529411764, 0.34155844155844156, 0.21682692307692308, 0.11481481481481481], 1.0, 1.1751152073732718, 4080, 3472)\t\n max val bleu: 0.2839332879099206\t\n2024-11-17 17:04:44,543 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 7\t\n2024-11-17 17:04:44,543 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 7\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:05:24,233 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 6 completed...\nTime Taken: 0.6614709297815958\n2024-11-17 17:05:24,233 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 6 completed...\nTime Taken: 0.6614709297815958\n2024-11-17 17:05:24,235 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:05:24,235 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:05:29,053 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 7\t\n best epoch: 6\t\n train loss epoch: 0.1727958616852609\t\n min train loss: 0.1727958616852609\t\n val loss epoch: 2.601456642150879\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.8741236456341619\t\n max train acc: 0.8741236456341619\t\n val acc epoch: 0.24\t\n max val acc: 0.246\t\n val bleu epoch: (0.2847784956323363, [0.7307395001288328, 0.3592502603262756, 0.21903242955874536, 0.1143827859569649], 1.0, 1.117799539170507, 3881, 3472)\t\n max val bleu: 0.2847784956323363\t\n2024-11-17 17:05:29,053 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 7\t\n best epoch: 6\t\n train loss epoch: 0.1727958616852609\t\n min train loss: 0.1727958616852609\t\n val loss epoch: 2.601456642150879\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.8741236456341619\t\n max train acc: 0.8741236456341619\t\n val acc epoch: 0.24\t\n max val acc: 0.246\t\n val bleu epoch: (0.2847784956323363, [0.7307395001288328, 0.3592502603262756, 0.21903242955874536, 0.1143827859569649], 1.0, 1.117799539170507, 3881, 3472)\t\n max val bleu: 0.2847784956323363\t\n2024-11-17 17:05:29,055 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 8\t\n2024-11-17 17:05:29,055 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 8\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:06:08,569 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 7 completed...\nTime Taken: 0.6585436900456746\n2024-11-17 17:06:08,569 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 7 completed...\nTime Taken: 0.6585436900456746\n2024-11-17 17:06:08,570 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:06:08,570 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:06:13,549 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.31613595108986964\n2024-11-17 17:06:13,549 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.31613595108986964\n2024-11-17 17:06:13,551 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_8.pt\n2024-11-17 17:06:13,551 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_8.pt\n2024-11-17 17:06:15,830 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.15512653779673394\t\n min train loss: 0.15512653779673394\t\n val loss epoch: 2.721817970275879\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.8891013384321224\t\n max train acc: 0.8891013384321224\t\n val acc epoch: 0.278\t\n max val acc: 0.278\t\n val bleu epoch: (0.31613595108986964, [0.7489384288747346, 0.37463872832369943, 0.248868778280543, 0.14304291287386217], 1.0, 1.0852534562211982, 3768, 3472)\t\n max val bleu: 0.31613595108986964\t\n2024-11-17 17:06:15,830 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.15512653779673394\t\n min train loss: 0.15512653779673394\t\n val loss epoch: 2.721817970275879\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.8891013384321224\t\n max train acc: 0.8891013384321224\t\n val acc epoch: 0.278\t\n max val acc: 0.278\t\n val bleu epoch: (0.31613595108986964, [0.7489384288747346, 0.37463872832369943, 0.248868778280543, 0.14304291287386217], 1.0, 1.0852534562211982, 3768, 3472)\t\n max val bleu: 0.31613595108986964\t\n2024-11-17 17:06:15,832 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 9\t\n2024-11-17 17:06:15,832 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 9\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:06:55,311 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 8 completed...\nTime Taken: 0.6579514384269715\n2024-11-17 17:06:55,311 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 8 completed...\nTime Taken: 0.6579514384269715\n2024-11-17 17:06:55,313 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:06:55,313 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:07:00,156 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 9\t\n best epoch: 8\t\n train loss epoch: 0.13637186423757203\t\n min train loss: 0.13637186423757203\t\n val loss epoch: 3.273540735244751\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.9053537284894837\t\n max train acc: 0.9053537284894837\t\n val acc epoch: 0.265\t\n max val acc: 0.278\t\n val bleu epoch: (0.30168934168220213, [0.7394259818731118, 0.3633916554508748, 0.23782961460446247, 0.12962962962962962], 1.0, 1.1440092165898617, 3972, 3472)\t\n max val bleu: 0.31613595108986964\t\n2024-11-17 17:07:00,156 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 9\t\n best epoch: 8\t\n train loss epoch: 0.13637186423757203\t\n min train loss: 0.13637186423757203\t\n val loss epoch: 3.273540735244751\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.9053537284894837\t\n max train acc: 0.9053537284894837\t\n val acc epoch: 0.265\t\n max val acc: 0.278\t\n val bleu epoch: (0.30168934168220213, [0.7394259818731118, 0.3633916554508748, 0.23782961460446247, 0.12962962962962962], 1.0, 1.1440092165898617, 3972, 3472)\t\n max val bleu: 0.31613595108986964\t\n2024-11-17 17:07:00,159 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 10\t\n2024-11-17 17:07:00,159 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 10\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:07:39,531 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 9 completed...\nTime Taken: 0.6561801274617513\n2024-11-17 17:07:39,531 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 9 completed...\nTime Taken: 0.6561801274617513\n2024-11-17 17:07:39,533 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:07:39,533 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:07:44,402 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.32880545620188834\n2024-11-17 17:07:44,402 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.32880545620188834\n2024-11-17 17:07:44,404 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_10.pt\n2024-11-17 17:07:44,404 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_10.pt\n2024-11-17 17:07:46,820 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.12682309005938963\t\n min train loss: 0.12682309005938963\t\n val loss epoch: 2.7805018424987793\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.9098151688973869\t\n max train acc: 0.9098151688973869\t\n val acc epoch: 0.279\t\n max val acc: 0.279\t\n val bleu epoch: (0.32880545620188834, [0.743293991416309, 0.39919354838709675, 0.26851851851851855, 0.14670255720053835], 1.0, 1.0737327188940091, 3728, 3472)\t\n max val bleu: 0.32880545620188834\t\n2024-11-17 17:07:46,820 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.12682309005938963\t\n min train loss: 0.12682309005938963\t\n val loss epoch: 2.7805018424987793\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.9098151688973869\t\n max train acc: 0.9098151688973869\t\n val acc epoch: 0.279\t\n max val acc: 0.279\t\n val bleu epoch: (0.32880545620188834, [0.743293991416309, 0.39919354838709675, 0.26851851851851855, 0.14670255720053835], 1.0, 1.0737327188940091, 3728, 3472)\t\n max val bleu: 0.32880545620188834\t\n2024-11-17 17:07:46,822 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 11\t\n2024-11-17 17:07:46,822 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 11\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 197 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:08:26,315 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 10 completed...\nTime Taken: 0.6581767002741495\n2024-11-17 17:08:26,315 | DEBUG | 2551679145.py: 387 : train_model() ::\t Training for epoch 10 completed...\nTime Taken: 0.6581767002741495\n2024-11-17 17:08:26,317 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n2024-11-17 17:08:26,317 | DEBUG | 2551679145.py: 388 : train_model() ::\t Starting Validation\n","output_type":"stream"},{"name":"stdout","text":"Completed 63 / 63...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-17 17:08:31,184 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.30719756400070075\n2024-11-17 17:08:31,184 | DEBUG | 2551679145.py: 427 : train_model() ::\t Validation Bleu: 0.30719756400070075\n2024-11-17 17:08:31,186 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_11.pt\n2024-11-17 17:08:31,186 | INFO | 2446051103.py: 236 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model_11.pt\n2024-11-17 17:08:33,541 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 11\t\n best epoch: 11\t\n train loss epoch: 0.11646150475542823\t\n min train loss: 0.11646150475542823\t\n val loss epoch: 3.5236055850982666\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.9225621414913958\t\n max train acc: 0.9225621414913958\t\n val acc epoch: 0.295\t\n max val acc: 0.295\t\n val bleu epoch: (0.30719756400070075, [0.7432264479244345, 0.3893483294740324, 0.2481463173504696, 0.1240234375], 1.0, 1.1586981566820276, 4023, 3472)\t\n max val bleu: 0.32880545620188834\t\n2024-11-17 17:08:33,541 | INFO | 2446051103.py: 502 : print_log() ::\t \n Epoch: 11\t\n best epoch: 11\t\n train loss epoch: 0.11646150475542823\t\n min train loss: 0.11646150475542823\t\n val loss epoch: 3.5236055850982666\t\n min val loss: 1.6727910041809082\t\n train acc epoch: 0.9225621414913958\t\n max train acc: 0.9225621414913958\t\n val acc epoch: 0.295\t\n max val acc: 0.295\t\n val bleu epoch: (0.30719756400070075, [0.7432264479244345, 0.3893483294740324, 0.2481463173504696, 0.1240234375], 1.0, 1.1586981566820276, 4023, 3472)\t\n max val bleu: 0.32880545620188834\t\n2024-11-17 17:08:33,543 | INFO | 2551679145.py: 466 : train_model() ::\t Training Completed for 10 epochs\n2024-11-17 17:08:33,543 | INFO | 2551679145.py: 466 : train_model() ::\t Training Completed for 10 epochs\n","output_type":"stream"},{"name":"stdout","text":"--Return--\nNone\n> \u001b[0;32m/tmp/ipykernel_31/2446051103.py\u001b[0m(549)\u001b[0;36mstore_results\u001b[0;34m()\u001b[0m\n\u001b[0;32m    547 \u001b[0;31m                        \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 549 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mstore_val_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  exit\n"}],"execution_count":86},{"cell_type":"code","source":"# import re\n\n# def modify_sentence_and_extract_numbers(sentence):\n#     # Find all numbers in the sentence\n#     numbers = re.findall(r'\\d+', sentence)\n    \n#     # Create a modified sentence by replacing each number with \"number0\", \"number1\", etc.\n#     modified_sentence = sentence\n#     for i, num in enumerate(numbers):\n#         modified_sentence = modified_sentence.replace(num, f'number{i}', 1)\n    \n#     # Convert the numbers from strings to integers\n#     numbers = [int(num) for num in numbers]\n    \n#     return modified_sentence, numbers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:56:12.340451Z","iopub.execute_input":"2024-11-17T15:56:12.340878Z","iopub.status.idle":"2024-11-17T15:56:12.347422Z","shell.execute_reply.started":"2024-11-17T15:56:12.340808Z","shell.execute_reply":"2024-11-17T15:56:12.346368Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# # Custom input problem\n# custom_problem = \"Bryan took a look at his books as well . If Bryan has 56 books in each of his 9 bookshelves , how many books does he have in total ?\"\n# custom_problem, numbers = modify_sentence_and_extract_numbers(custom_problem)\n\n# run_name = config.run_name\n# config.model_path = os.path.join(model_folder, run_name)\n\n# vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n# config_file = os.path.join(config.model_path, 'config.p')\n\n# print(vocab1_path, config.max_length)\n\n# with open(vocab1_path, 'rb') as f:\n# \tvoc1 = pickle.load(f)\n\n# device = gpu_init_pytorch(config.gpu)\n\n# # Convert input problem to indices using your vocabulary (voc1)\n# input_problem = sents_to_idx(voc1, [custom_problem], config.max_length)\n\n# def process_batch(sent1s, voc1, device):\n# \tinput_len1 = [len(s) for s in sent1s]\n# \tmax_length_1 = max(input_len1)\n\n# \tsent1s_padded = [pad_seq(s, max_length_1, voc1) for s in sent1s]\n\n# \t# Convert to [Max_len X Batch]\n# \tsent1_var = Variable(torch.LongTensor(sent1s_padded)).transpose(0, 1)\n\n# \tsent1_var = sent1_var.to(device)\n\n# \treturn sent1_var, input_len1\n\n# # Process the batch for input\n# sent1_var, input_len1 = process_batch(input_problem, voc1, device)\n\n# # Generate the predicted output using greedy decoding\n# decoder_output = model.greedy_decode([custom_problem], sent1_var, None, input_len1, None)\n\n# # Print the generated equation\n# print(\"Generated Equation: \", ' '.join(decoder_output[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:56:12.912044Z","iopub.execute_input":"2024-11-17T15:56:12.912772Z","iopub.status.idle":"2024-11-17T15:56:13.068306Z","shell.execute_reply.started":"2024-11-17T15:56:12.912732Z","shell.execute_reply":"2024-11-17T15:56:13.067410Z"}},"outputs":[{"name":"stdout","text":"models/mawps_try1/vocab1.p 80\nGenerated Equation:  * number0 number1\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# def cal_score(outputs, num):\n# \tfor i in range(len(outputs)):\n# \t\top = stack_to_string(outputs[i])\n\n# \t\tpred = ans_evaluator(op, num)\n\n# \treturn pred\n\n# answer = cal_score(decoder_output, numbers)\n# print(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:56:07.605358Z","iopub.status.idle":"2024-11-17T15:56:07.605702Z","shell.execute_reply.started":"2024-11-17T15:56:07.605534Z","shell.execute_reply":"2024-11-17T15:56:07.605551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the CSV file\nfile_path = \"/kaggle/input/svamp-dataset/data/mawps-asdiv-a_svamp/dev.csv\"\ndata = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:08:45.004872Z","iopub.execute_input":"2024-11-17T17:08:45.005815Z","iopub.status.idle":"2024-11-17T17:08:45.040180Z","shell.execute_reply.started":"2024-11-17T17:08:45.005753Z","shell.execute_reply":"2024-11-17T17:08:45.039266Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"print(len(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:08:48.910375Z","iopub.execute_input":"2024-11-17T17:08:48.910769Z","iopub.status.idle":"2024-11-17T17:08:48.916413Z","shell.execute_reply.started":"2024-11-17T17:08:48.910731Z","shell.execute_reply":"2024-11-17T17:08:48.915328Z"}},"outputs":[{"name":"stdout","text":"1000\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"# import pandas as pd\n\n# def generate_full_question(question, numbers):\n#     for i, num in enumerate(numbers):\n#         placeholder = f\"number{i}\"\n#         question = question.replace(placeholder, str(num))\n#     return question\n\n# def convert_eqn(equation, numbers):\n#     for i, num in enumerate(numbers):\n#         placeholder = f\"number{i}\"\n#         equation = equation.replace(placeholder, str(num))\n#     return equation\n\n\n# # Function to write evaluation results into a file\n# def write_to_file(filename, data):\n#     with open(filename, 'w') as f:\n#         for line in data:\n#             f.write(line + '\\n')\n\n# ct = 0\n\n# # Iterate over the rows\n# for index, row in data.iterrows():\n#     ques = row['Question']\n#     ans = row['Answer']\n#     true_eq = row['Equation']\n#     num_str = row['Numbers']\n#     num_list = list(map(int, num_str.split()))\n\n#     sent1s = sents_to_idx(voc1, ques, config.max_length, flag = 0)\n#     sent2s = sents_to_idx(voc2, true_eq, config.max_length, flag = 0)\n#     nums = num_str\n    \n#     sent1_var, sent2_var, input_len1, input_len2 = process_batch(sent1s, sent2s, voc1, voc2, device)\n#     val_loss, decoder_output = model.greedy_decode(ques, sent1_var, sent2_var, input_len2, validation=True)\n#     print(decoder_output)\n\n    \n#     # Print the variables or process them as needed\n#     print(f\"Ques: {generate_full_question(ques, num_list)}\")\n#     print(f\"Ans: {ans}\")\n#     print(f\"True Equation: {convert_eqn(true_eq, num_list)}\")\n#     # print(f\"Model Equation: {convert_eqn(decoder_output, num_list)}\")\n#     print(\"-\" * 50)  \n#     ct += 1\n#     if ct >= 5:\n#         break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T16:27:53.988364Z","iopub.execute_input":"2024-11-17T16:27:53.989233Z","iopub.status.idle":"2024-11-17T16:27:53.994471Z","shell.execute_reply.started":"2024-11-17T16:27:53.989190Z","shell.execute_reply":"2024-11-17T16:27:53.993552Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def generate_full_question(question, numbers):\n    for i, num in enumerate(numbers):\n        placeholder = f\"number{i}\"\n        question = question.replace(placeholder, str(num))\n    return question\n\ndef convert_eqn(equation, numbers):\n    for i, num in enumerate(numbers):\n        placeholder = f\"number{i}\"\n        equation = equation.replace(placeholder, str(num))\n    return equation\n\n\n# Function to write evaluation results into a file\ndef write_to_file(filename, data):\n    with open(filename, 'w') as f:\n        for line in data:\n            f.write(line + '\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:08:54.849583Z","iopub.execute_input":"2024-11-17T17:08:54.850002Z","iopub.status.idle":"2024-11-17T17:08:54.856949Z","shell.execute_reply.started":"2024-11-17T17:08:54.849963Z","shell.execute_reply":"2024-11-17T17:08:54.856008Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"# Open the file in write mode\nwith open(\"transformers_eval.txt\", \"w\") as file:\n    for data in val_dataloader:\n        sent1s = sents_to_idx(voc1, data['ques'], config.max_length, flag=0)\n        sent2s = sents_to_idx(voc2, data['eqn'], config.max_length, flag=0)\n        nums = data['nums']\n        ans = data['ans']\n        ques = data['ques']\n        true_eqn = data['eqn']\n        sent1_var, sent2_var, input_len1, input_len2 = process_batch(sent1s, sent2s, voc1, voc2, device)\n\n        val_loss, decoder_output = model.greedy_decode(ques, sent1_var, sent2_var, input_len2, validation=True)\n\n        for i in range(len(ques)):\n            num_list = list(map(int, nums[i].split()))\n            question_text = generate_full_question(ques[i], num_list)\n            true_given_eqn = convert_eqn(true_eqn[i], num_list)\n            decoded_eqn = ' '.join(decoder_output[i])\n            model_eqn_text = convert_eqn(decoded_eqn, num_list)\n            result_comparison = \"Correct\" if (true_eqn[i] == decoded_eqn) else \"Incorrect\"\n\n            # Write to file\n            file.write(f\"Question: {question_text}\\n\")\n            file.write(f\"Answer: {ans[i]}\\n\")\n            file.write(f\"True Equation: {true_given_eqn}\\n\")\n            file.write(f\"Model Equation: {model_eqn_text}\\n\")\n            file.write(f\"Predicted Result: {result_comparison}\\n\")\n            file.write(\"-\" * 100 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:08:57.899294Z","iopub.execute_input":"2024-11-17T17:08:57.899685Z","iopub.status.idle":"2024-11-17T17:09:01.157769Z","shell.execute_reply.started":"2024-11-17T17:08:57.899647Z","shell.execute_reply":"2024-11-17T17:09:01.156443Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"import torch\ntorch.save(model, '/kaggle/working/entire_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:09:46.579017Z","iopub.execute_input":"2024-11-17T17:09:46.579431Z","iopub.status.idle":"2024-11-17T17:09:49.044471Z","shell.execute_reply.started":"2024-11-17T17:09:46.579388Z","shell.execute_reply":"2024-11-17T17:09:49.043500Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"!zip -r /kaggle/working/kaggle_working_dir.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:10:05.430040Z","iopub.execute_input":"2024-11-17T17:10:05.430432Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/models/ (stored 0%)\n  adding: kaggle/working/models/mawps_try1/ (stored 0%)\n  adding: kaggle/working/models/mawps_try1/vocab1.p (deflated 50%)\n  adding: kaggle/working/models/mawps_try1/config.p (deflated 36%)\n  adding: kaggle/working/models/mawps_try1/model_3.pt","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}