{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9525411,"sourceType":"datasetVersion","datasetId":5800301},{"sourceId":9912417,"sourceType":"datasetVersion","datasetId":6090767}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nfrom transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\nimport pdb\nfrom torch.nn import functional\nfrom gensim import models\nfrom copy import deepcopy\nimport re\n\nimport logging\nfrom glob import glob\nfrom torch.autograd import Variable\nimport numpy as np\nimport os\nimport sys\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport json\n\nimport random\nimport copy\nimport nltk\nimport argparse\nimport ast\n\nimport time\nimport torch.optim\nfrom collections import OrderedDict\ntry:\n\timport cPickle as pickle\nexcept ImportError:\n\timport pickle\n\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:40:04.332543Z","iopub.execute_input":"2024-11-15T11:40:04.333088Z","iopub.status.idle":"2024-11-15T11:40:04.341601Z","shell.execute_reply.started":"2024-11-15T11:40:04.333035Z","shell.execute_reply":"2024-11-15T11:40:04.340758Z"}},"outputs":[],"execution_count":112},{"cell_type":"markdown","source":"## Components","metadata":{}},{"cell_type":"code","source":"#########################################\n# contextual_embeddings.py\n#########################################\n\nclass BertEncoder(nn.Module):\n\tdef __init__(self, bert_model = 'bert-base-uncased',device = 'cuda:0 ', freeze_bert = False):\n\t\tsuper(BertEncoder, self).__init__()\n\t\tself.bert_layer = BertModel.from_pretrained(bert_model)\n\t\tself.bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n\t\tself.device = device\n\t\t\n\t\tif freeze_bert:\n\t\t\tfor p in self.bert_layer.parameters():\n\t\t\t\tp.requires_grad = False\n\t\t\n\tdef bertify_input(self, sentences):\n\t\t'''\n\t\tPreprocess the input sentences using bert tokenizer and converts them to a torch tensor containing token ids\n\n\t\t'''\n\t\t#Tokenize the input sentences for feeding into BERT\n\t\tall_tokens  = [['[CLS]'] + self.bert_tokenizer.tokenize(sentence) + ['[SEP]'] for sentence in sentences]\n\t\t\n\t\t#Pad all the sentences to a maximum length\n\t\tinput_lengths = [len(tokens) for tokens in all_tokens]\n\t\tmax_length    = max(input_lengths)\n\t\tpadded_tokens = [tokens + ['[PAD]' for _ in range(max_length - len(tokens))] for tokens in all_tokens]\n\n\t\t#Convert tokens to token ids\n\t\ttoken_ids = torch.tensor([self.bert_tokenizer.convert_tokens_to_ids(tokens) for tokens in padded_tokens]).to(self.device)\n\n\t\t#Obtain attention masks\n\t\tpad_token = self.bert_tokenizer.convert_tokens_to_ids('[PAD]')\n\t\tattn_masks = (token_ids != pad_token).long()\n\n\t\treturn token_ids, attn_masks, input_lengths\n\n\tdef forward(self, sentences):\n\t\t'''\n\t\tFeed the batch of sentences to a BERT encoder to obtain contextualized representations of each token\n\t\t'''\n\t\t#Preprocess sentences\n\t\ttoken_ids, attn_masks, input_lengths = self.bertify_input(sentences)\n\n\t\t#Feed through bert\n\t\tcont_reps, _ = self.bert_layer(token_ids, attention_mask = attn_masks)\n\n\t\treturn cont_reps, input_lengths\n\nclass RobertaEncoder(nn.Module):\n\tdef __init__(self, roberta_model = 'roberta-base', device = 'cuda:0 ', freeze_roberta = False):\n\t\tsuper(RobertaEncoder, self).__init__()\n\t\tself.roberta_layer = RobertaModel.from_pretrained(roberta_model)\n\t\tself.roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model)\n\t\tself.device = device\n\t\t\n\t\tif freeze_roberta:\n\t\t\tfor p in self.roberta_layer.parameters():\n\t\t\t\tp.requires_grad = False\n\t\t\n\tdef robertify_input(self, sentences):\n\t\t'''\n\t\tPreprocess the input sentences using roberta tokenizer and converts them to a torch tensor containing token ids\n\n\t\t'''\n\t\t# Tokenize the input sentences for feeding into RoBERTa\n\t\tall_tokens  = [['<s>'] + self.roberta_tokenizer.tokenize(sentence) + ['</s>'] for sentence in sentences]\n\t\t\n\t\t# Pad all the sentences to a maximum length\n\t\tinput_lengths = [len(tokens) for tokens in all_tokens]\n\t\tmax_length    = max(input_lengths)\n\t\tpadded_tokens = [tokens + ['<pad>' for _ in range(max_length - len(tokens))] for tokens in all_tokens]\n\n\t\t# Convert tokens to token ids\n\t\ttoken_ids = torch.tensor([self.roberta_tokenizer.convert_tokens_to_ids(tokens) for tokens in padded_tokens]).to(self.device)\n\n\t\t# Obtain attention masks\n\t\tpad_token = self.roberta_tokenizer.convert_tokens_to_ids('<pad>')\n\t\tattn_masks = (token_ids != pad_token).long()\n\n\t\treturn token_ids, attn_masks, input_lengths\n\n\tdef forward(self, sentences):\n\t\t'''\n\t\tFeed the batch of sentences to a RoBERTa encoder to obtain contextualized representations of each token\n\t\t'''\n\t\t# Preprocess sentences\n\t\ttoken_ids, attn_masks, input_lengths = self.robertify_input(sentences)\n\n\t\t# Feed through RoBERTa\n\t\toutput = self.roberta_layer(token_ids, attention_mask = attn_masks)\n        \n\t\tcont_reps = output.last_hidden_state\n\n\t\treturn cont_reps, input_lengths\n    \n#########################################\n# masked_cross_entropy.py\n#########################################\n\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.data.max()\n    batch_size = sequence_length.size(0)\n    seq_range = torch.arange(0, max_len).long()\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    if sequence_length.is_cuda:\n        seq_range_expand = seq_range_expand.cuda()\n    seq_length_expand = (sequence_length.unsqueeze(1).expand_as(seq_range_expand))\n    return seq_range_expand < seq_length_expand\n\n\ndef masked_cross_entropy(logits, target, length):\n    if torch.cuda.is_available():\n        length = torch.LongTensor(length).cuda()\n    else:\n        length = torch.LongTensor(length)\n    \"\"\"\n    Args:\n        logits: A Variable containing a FloatTensor of size\n            (batch, max_len, num_classes) which contains the\n            unnormalized probability for each class.\n        target: A Variable containing a LongTensor of size\n            (batch, max_len) which contains the index of the true\n            class for each corresponding step.\n        length: A Variable containing a LongTensor of size (batch,)\n            which contains the length of each data in a batch.\n    Returns:\n        loss: An average loss value masked by the length.\n    \"\"\"\n\n    # logits_flat: (batch * max_len, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # log_probs_flat: (batch * max_len, num_classes)\n    log_probs_flat = functional.log_softmax(logits_flat, dim=1)\n    # target_flat: (batch * max_len, 1)\n    target_flat = target.view(-1, 1)\n    # losses_flat: (batch * max_len, 1)\n    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n\n    # losses: (batch, max_len)\n    losses = losses_flat.view(*target.size())\n    # mask: (batch, max_len)\n    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n    losses = losses * mask.float()\n    loss = losses.sum() / length.float().sum()\n    # if loss.item() > 10:\n    #     print(losses, target)\n    return loss\n\n\ndef masked_cross_entropy_without_logit(logits, target, length):\n    if torch.cuda.is_available():\n        length = torch.LongTensor(length).cuda()\n    else:\n        length = torch.LongTensor(length)\n    \"\"\"\n    Args:\n        logits: A Variable containing a FloatTensor of size\n            (batch, max_len, num_classes) which contains the\n            unnormalized probability for each class.\n        target: A Variable containing a LongTensor of size\n            (batch, max_len) which contains the index of the true\n            class for each corresponding step.\n        length: A Variable containing a LongTensor of size (batch,)\n            which contains the length of each data in a batch.\n    Returns:\n        loss: An average loss value masked by the length.\n    \"\"\"\n\n    # logits_flat: (batch * max_len, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n\n    # log_probs_flat: (batch * max_len, num_classes)\n    log_probs_flat = torch.log(logits_flat + 1e-12)\n\n    # target_flat: (batch * max_len, 1)\n    target_flat = target.view(-1, 1)\n    # losses_flat: (batch * max_len, 1)\n    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n\n    # losses: (batch, max_len)\n    losses = losses_flat.view(*target.size())\n\n    # mask: (batch, max_len)\n    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n    losses = losses * mask.float()\n    loss = losses.sum() / length.float().sum()\n    # if loss.item() > 10:\n    #     print(losses, target)\n    return loss\n\n#########################################\n# models.py\n#########################################\n\nclass Embedding(nn.Module):\n\tdef __init__(self, config, input_lang, input_size, embedding_size, dropout=0.5):\n\t\tsuper(Embedding, self).__init__()\n\n\t\tself.config = config\n\t\tself.input_lang = input_lang\n\t\tself.input_size = input_size\n\t\tself.embedding_size = embedding_size\n\n\t\tif self.config.embedding == 'word2vec':\n\t\t\tself.config.embedding_size = 300\n\t\t\tself.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self._form_embeddings(self.config.word2vec_bin)), freeze = self.config.freeze_emb)\n\t\telse:\n\t\t\tself.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n\t\tself.em_dropout = nn.Dropout(dropout)\n\n\tdef _form_embeddings(self, file_path):\n\t\tweights_all = models.KeyedVectors.load_word2vec_format(file_path, limit=200000, binary=True)\n\t\tweight_req  = torch.randn(self.input_size, self.config.embedding_size)\n\t\tfor temp_ind in range(len(self.input_lang.index2word)):\n\t\t\tvalue = self.input_lang.index2word[temp_ind]\n\t\t\tif value in weights_all:\n\t\t\t\tweight_req[temp_ind] = torch.FloatTensor(weights_all[value])\n\t\t# for key, value in self.voc1.id2w.items():\n\t\t# \tif value in weights_all:\n\t\t# \t\tweight_req[key] = torch.FloatTensor(weights_all[value])\n\n\t\treturn weight_req\n\n\tdef forward(self, input_seqs):\n\t\tembedded = self.embedding(input_seqs)  # S x B x E\n\t\tembedded = self.em_dropout(embedded)\n\t\treturn embedded\n\nclass EncoderRNN(nn.Module):\n\t# def __init__(self, input_size, embedding_size, hidden_size, n_layers=2, dropout=0.5):\n\tdef __init__(self, embedding_size, hidden_size, n_layers=2, dropout=0.5):\n\t\tsuper(EncoderRNN, self).__init__()\n\n\t\t# self.input_size = input_size\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\t\tself.n_layers = n_layers\n\t\tself.dropout = dropout\n\n\t\t# self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n\t\t# self.em_dropout = nn.Dropout(dropout)\n\t\tself.gru = nn.GRU(embedding_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n\n\t# def forward(self, input_seqs, input_lengths, hidden=None):\n\tdef forward(self, embedded, input_lengths, hidden=None):\n\t\t# Note: we run this all at once (over multiple batches of multiple sequences)\n\t\t# embedded = self.embedding(input_seqs)  # S x B x E\n\t\t# embedded = self.em_dropout(embedded)\n\t\tpacked = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n\t\toutputs, hidden = self.gru(packed, hidden)\n\t\toutputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n\t\toutputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n\t\t# S x B x H\n\t\treturn outputs, hidden\n\n\nclass Attn(nn.Module):\n\tdef __init__(self, hidden_size):\n\t\tsuper(Attn, self).__init__()\n\t\tself.hidden_size = hidden_size\n\t\tself.attn = nn.Linear(hidden_size * 2, hidden_size)\n\t\tself.score = nn.Linear(hidden_size, 1, bias=False)\n\t\tself.softmax = nn.Softmax(dim=1)\n\n\tdef forward(self, hidden, encoder_outputs, seq_mask=None):\n\t\tmax_len = encoder_outputs.size(0)\n\t\trepeat_dims = [1] * hidden.dim()\n\t\trepeat_dims[0] = max_len\n\t\thidden = hidden.repeat(*repeat_dims)  # S x B x H\n\t\t# For each position of encoder outputs\n\t\tthis_batch_size = encoder_outputs.size(1)\n\t\tenergy_in = torch.cat((hidden, encoder_outputs), 2).view(-1, 2 * self.hidden_size)\n\t\tattn_energies = self.score(torch.tanh(self.attn(energy_in)))  # (S x B) x 1\n\t\tattn_energies = attn_energies.squeeze(1)\n\t\tattn_energies = attn_energies.view(max_len, this_batch_size).transpose(0, 1)  # B x S\n\t\tif seq_mask is not None:\n\t\t\tattn_energies = attn_energies.masked_fill_(seq_mask, -1e12)\n\t\tattn_energies = self.softmax(attn_energies)\n\t\t# Normalize energies to weights in range 0 to 1, resize to B x 1 x S\n\t\treturn attn_energies.unsqueeze(1)\n\n\nclass AttnDecoderRNN(nn.Module):\n\tdef __init__(\n\t\t\tself, hidden_size, embedding_size, input_size, output_size, n_layers=2, dropout=0.5):\n\t\tsuper(AttnDecoderRNN, self).__init__()\n\n\t\t# Keep for reference\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\t\tself.input_size = input_size\n\t\tself.output_size = output_size\n\t\tself.n_layers = n_layers\n\t\tself.dropout = dropout\n\n\t\t# Define layers\n\t\tself.em_dropout = nn.Dropout(dropout)\n\t\tself.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n\t\tself.gru = nn.GRU(hidden_size + embedding_size, hidden_size, n_layers, dropout=dropout)\n\t\tself.concat = nn.Linear(hidden_size * 2, hidden_size)\n\t\tself.out = nn.Linear(hidden_size, output_size)\n\t\t# Choose attention model\n\t\tself.attn = Attn(hidden_size)\n\n\tdef forward(self, input_seq, last_hidden, encoder_outputs, seq_mask):\n\t\t# Get the embedding of the current input word (last output word)\n\t\tbatch_size = input_seq.size(0)\n\t\tembedded = self.embedding(input_seq)\n\t\tembedded = self.em_dropout(embedded)\n\t\tembedded = embedded.view(1, batch_size, self.embedding_size)  # S=1 x B x N\n\n\t\t# Calculate attention from current RNN state and all encoder outputs;\n\t\t# apply to encoder outputs to get weighted average\n\t\tattn_weights = self.attn(last_hidden[-1].unsqueeze(0), encoder_outputs, seq_mask)\n\t\tcontext = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B x S=1 x N\n\n\t\t# Get current hidden state from input word and last hidden state\n\t\trnn_output, hidden = self.gru(torch.cat((embedded, context.transpose(0, 1)), 2), last_hidden)\n\n\t\t# Attentional vector using the RNN hidden state and context vector\n\t\t# concatenated together (Luong eq. 5)\n\t\toutput = self.out(torch.tanh(self.concat(torch.cat((rnn_output.squeeze(0), context.squeeze(1)), 1))))\n\n\t\t# Return final output, hidden state\n\t\treturn output, hidden\n\n\nclass TreeNode:  # the class save the tree node\n\tdef __init__(self, embedding, left_flag=False):\n\t\tself.embedding = embedding\n\t\tself.left_flag = left_flag\n\n\nclass Score(nn.Module):\n\tdef __init__(self, input_size, hidden_size):\n\t\tsuper(Score, self).__init__()\n\t\tself.input_size = input_size\n\t\tself.hidden_size = hidden_size\n\t\tself.attn = nn.Linear(hidden_size + input_size, hidden_size)\n\t\tself.score = nn.Linear(hidden_size, 1, bias=False)\n\n\tdef forward(self, hidden, num_embeddings, num_mask=None):\n\t\tmax_len = num_embeddings.size(1)\n\t\trepeat_dims = [1] * hidden.dim()\n\t\trepeat_dims[1] = max_len\n\t\thidden = hidden.repeat(*repeat_dims)  # B x O x H\n\t\t# For each position of encoder outputs\n\t\tthis_batch_size = num_embeddings.size(0)\n\t\tenergy_in = torch.cat((hidden, num_embeddings), 2).view(-1, self.input_size + self.hidden_size)\n\t\tscore = self.score(torch.tanh(self.attn(energy_in)))  # (B x O) x 1\n\t\tscore = score.squeeze(1)\n\t\tscore = score.view(this_batch_size, -1)  # B x O\n\t\tif num_mask is not None:\n\t\t\tscore = score.masked_fill_(num_mask, -1e12)\n\t\treturn score\n\n\nclass TreeAttn(nn.Module):\n\tdef __init__(self, input_size, hidden_size):\n\t\tsuper(TreeAttn, self).__init__()\n\t\tself.input_size = input_size\n\t\tself.hidden_size = hidden_size\n\t\tself.attn = nn.Linear(hidden_size + input_size, hidden_size)\n\t\tself.score = nn.Linear(hidden_size, 1)\n\n\tdef forward(self, hidden, encoder_outputs, seq_mask=None):\n\t\tmax_len = encoder_outputs.size(0)\n\n\t\trepeat_dims = [1] * hidden.dim()\n\t\trepeat_dims[0] = max_len\n\t\thidden = hidden.repeat(*repeat_dims)  # S x B x H\n\t\tthis_batch_size = encoder_outputs.size(1)\n\n\t\tenergy_in = torch.cat((hidden, encoder_outputs), 2).view(-1, self.input_size + self.hidden_size)\n\n\t\tscore_feature = torch.tanh(self.attn(energy_in))\n\t\tattn_energies = self.score(score_feature)  # (S x B) x 1\n\t\tattn_energies = attn_energies.squeeze(1)\n\t\tattn_energies = attn_energies.view(max_len, this_batch_size).transpose(0, 1)  # B x S\n\t\tif seq_mask is not None:\n\t\t\tattn_energies = attn_energies.masked_fill_(seq_mask, -1e12)\n\t\tattn_energies = nn.functional.softmax(attn_energies, dim=1)  # B x S\n\n\t\treturn attn_energies.unsqueeze(1)\n\n\nclass EncoderSeq(nn.Module):\n\t# def __init__(self, input_size, embedding_size, hidden_size, n_layers=2, dropout=0.5):\n\tdef __init__(self, cell_type, embedding_size, hidden_size, n_layers=2, dropout=0.5):\n\t\tsuper(EncoderSeq, self).__init__()\n\n\t\t# self.input_size = input_size\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\t\tself.n_layers = n_layers\n\t\tself.dropout = dropout\n\n\t\t# self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n\t\t# self.em_dropout = nn.Dropout(dropout)\n\n\t\tif cell_type == 'lstm':\n\t\t\tself.rnn = nn.LSTM(self.embedding_size, self.hidden_size,\n\t\t\t\t\t\t\t   num_layers=self.n_layers,\n\t\t\t\t\t\t\t   dropout=(0 if self.n_layers == 1 else self.dropout),\n\t\t\t\t\t\t\t   bidirectional=True)\n\t\telif cell_type == 'gru':\n\t\t\tself.rnn = nn.GRU(embedding_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n\t\telse:\n\t\t\tself.rnn = nn.RNN(self.embedding_size, self.hidden_size,\n\t\t\t\t\t\t\t  num_layers=self.n_layers,\n\t\t\t\t\t\t\t  nonlinearity='tanh',\t\t\t\t\t\t\t# ['relu', 'tanh']\n\t\t\t\t\t\t\t  dropout=(0 if self.n_layers == 1 else self.dropout),\n\t\t\t\t\t\t\t  bidirectional=True)\t\t\n\n\t# def forward(self, input_seqs, input_lengths, hidden=None):\n\tdef forward(self, embedded, input_lengths, orig_idx, hidden=None):\n\t\t# Note: we run this all at once (over multiple batches of multiple sequences)\n\t\t# embedded = self.embedding(input_seqs)  # S x B x E\n\t\t# embedded = self.em_dropout(embedded)\n\t\tpacked = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n\t\tpade_hidden = hidden\n\t\t# pade_outputs, pade_hidden = self.gru_pade(packed, pade_hidden)\n\t\tpade_outputs, pade_hidden = self.rnn(packed, pade_hidden)\n\t\tpade_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(pade_outputs)\n\n\t\tif orig_idx is not None:\n\t\t\tpade_outputs = pade_outputs.index_select(1, orig_idx)\n\n\t\tproblem_output = pade_outputs[-1, :, :self.hidden_size] + pade_outputs[0, :, self.hidden_size:]\n\t\tpade_outputs = pade_outputs[:, :, :self.hidden_size] + pade_outputs[:, :, self.hidden_size:]  # S x B x H\n\t\treturn pade_outputs, problem_output\n\n\nclass Prediction(nn.Module):\n\t# a seq2tree decoder with Problem aware dynamic encoding\n\n\tdef __init__(self, hidden_size, op_nums, input_size, dropout=0.5):\n\t\tsuper(Prediction, self).__init__()\n\n\t\t# Keep for reference\n\t\tself.hidden_size = hidden_size\n\t\tself.input_size = input_size\n\t\tself.op_nums = op_nums\n\n\t\t# Define layers\n\t\tself.dropout = nn.Dropout(dropout)\n\n\t\tself.embedding_weight = nn.Parameter(torch.randn(1, input_size, hidden_size))\n\n\t\t# for Computational symbols and Generated numbers\n\t\tself.concat_l = nn.Linear(hidden_size, hidden_size)\n\t\tself.concat_r = nn.Linear(hidden_size * 2, hidden_size)\n\t\tself.concat_lg = nn.Linear(hidden_size, hidden_size)\n\t\tself.concat_rg = nn.Linear(hidden_size * 2, hidden_size)\n\n\t\tself.ops = nn.Linear(hidden_size * 2, op_nums)\n\n\t\tself.attn = TreeAttn(hidden_size, hidden_size)\n\t\tself.score = Score(hidden_size * 2, hidden_size)\n\n\tdef forward(self, node_stacks, left_childs, encoder_outputs, num_pades, padding_hidden, seq_mask, mask_nums):\n\t\tcurrent_embeddings = []\n\n\t\tfor st in node_stacks:\n\t\t\tif len(st) == 0:\n\t\t\t\tcurrent_embeddings.append(padding_hidden)\n\t\t\telse:\n\t\t\t\tcurrent_node = st[-1]\n\t\t\t\tcurrent_embeddings.append(current_node.embedding)\n\n\t\tcurrent_node_temp = []\n\t\tfor l, c in zip(left_childs, current_embeddings):\n\t\t\tif l is None:\n\t\t\t\tc = self.dropout(c)\n\t\t\t\tg = torch.tanh(self.concat_l(c))\n\t\t\t\tt = torch.sigmoid(self.concat_lg(c))\n\t\t\t\tcurrent_node_temp.append(g * t)\n\t\t\telse:\n\t\t\t\tld = self.dropout(l)\n\t\t\t\tc = self.dropout(c)\n\t\t\t\tg = torch.tanh(self.concat_r(torch.cat((ld, c), 1)))\n\t\t\t\tt = torch.sigmoid(self.concat_rg(torch.cat((ld, c), 1)))\n\t\t\t\tcurrent_node_temp.append(g * t)\n\n\t\tcurrent_node = torch.stack(current_node_temp)\n\n\t\tcurrent_embeddings = self.dropout(current_node)\n\n\t\tcurrent_attn = self.attn(current_embeddings.transpose(0, 1), encoder_outputs, seq_mask)\n\t\tcurrent_context = current_attn.bmm(encoder_outputs.transpose(0, 1))  # B x 1 x N\n\n\t\t# the information to get the current quantity\n\t\tbatch_size = current_embeddings.size(0)\n\t\t# predict the output (this node corresponding to output(number or operator)) with PADE\n\n\t\trepeat_dims = [1] * self.embedding_weight.dim()\n\t\trepeat_dims[0] = batch_size\n\t\tembedding_weight = self.embedding_weight.repeat(*repeat_dims)  # B x input_size x N\n\t\tembedding_weight = torch.cat((embedding_weight, num_pades), dim=1)  # B x O x N\n\n\t\tleaf_input = torch.cat((current_node, current_context), 2)\n\t\tleaf_input = leaf_input.squeeze(1)\n\t\tleaf_input = self.dropout(leaf_input)\n\n\t\t# p_leaf = nn.functional.softmax(self.is_leaf(leaf_input), 1)\n\t\t# max pooling the embedding_weight\n\t\tembedding_weight_ = self.dropout(embedding_weight)\n\t\tnum_score = self.score(leaf_input.unsqueeze(1), embedding_weight_, mask_nums)\n\n\t\t# num_score = nn.functional.softmax(num_score, 1)\n\n\t\top = self.ops(leaf_input)\n\n\t\t# return p_leaf, num_score, op, current_embeddings, current_attn\n\n\t\treturn num_score, op, current_node, current_context, embedding_weight\n\n\nclass GenerateNode(nn.Module):\n\tdef __init__(self, hidden_size, op_nums, embedding_size, dropout=0.5):\n\t\tsuper(GenerateNode, self).__init__()\n\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\n\t\tself.embeddings = nn.Embedding(op_nums, embedding_size)\n\t\tself.em_dropout = nn.Dropout(dropout)\n\t\tself.generate_l = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\t\tself.generate_r = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\t\tself.generate_lg = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\t\tself.generate_rg = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\n\tdef forward(self, node_embedding, node_label, current_context):\n\t\tnode_label_ = self.embeddings(node_label)\n\t\tnode_label = self.em_dropout(node_label_)\n\t\tnode_embedding = node_embedding.squeeze(1)\n\t\tcurrent_context = current_context.squeeze(1)\n\t\tnode_embedding = self.em_dropout(node_embedding)\n\t\tcurrent_context = self.em_dropout(current_context)\n\n\t\tl_child = torch.tanh(self.generate_l(torch.cat((node_embedding, current_context, node_label), 1)))\n\t\tl_child_g = torch.sigmoid(self.generate_lg(torch.cat((node_embedding, current_context, node_label), 1)))\n\t\tr_child = torch.tanh(self.generate_r(torch.cat((node_embedding, current_context, node_label), 1)))\n\t\tr_child_g = torch.sigmoid(self.generate_rg(torch.cat((node_embedding, current_context, node_label), 1)))\n\t\tl_child = l_child * l_child_g\n\t\tr_child = r_child * r_child_g\n\t\treturn l_child, r_child, node_label_\n\n\nclass Merge(nn.Module):\n\tdef __init__(self, hidden_size, embedding_size, dropout=0.5):\n\t\tsuper(Merge, self).__init__()\n\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\n\t\tself.em_dropout = nn.Dropout(dropout)\n\t\tself.merge = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\t\tself.merge_g = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\n\tdef forward(self, node_embedding, sub_tree_1, sub_tree_2):\n\t\tsub_tree_1 = self.em_dropout(sub_tree_1)\n\t\tsub_tree_2 = self.em_dropout(sub_tree_2)\n\t\tnode_embedding = self.em_dropout(node_embedding)\n\n\t\tsub_tree = torch.tanh(self.merge(torch.cat((node_embedding, sub_tree_1, sub_tree_2), 1)))\n\t\tsub_tree_g = torch.sigmoid(self.merge_g(torch.cat((node_embedding, sub_tree_1, sub_tree_2), 1)))\n\t\tsub_tree = sub_tree * sub_tree_g\n\t\treturn sub_tree\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:40:04.567918Z","iopub.execute_input":"2024-11-15T11:40:04.568271Z","iopub.status.idle":"2024-11-15T11:40:04.657620Z","shell.execute_reply.started":"2024-11-15T11:40:04.568239Z","shell.execute_reply":"2024-11-15T11:40:04.656645Z"}},"outputs":[],"execution_count":113},{"cell_type":"markdown","source":"## utils","metadata":{}},{"cell_type":"code","source":"#########################################\n# expressions_transfer.py\n#########################################\n\n# An expression tree node\nclass Et:\n    # Constructor to create a node\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\n# Returns root of constructed tree for given postfix expression\ndef construct_exp_tree(postfix):\n    stack = []\n\n    # Traverse through every character of input expression\n    for char in postfix:\n\n        # if operand, simply push into stack\n        if char not in [\"+\", \"-\", \"*\", \"/\", \"^\"]:\n            t = Et(char)\n            stack.append(t)\n        # Operator\n        else:\n            # Pop two top nodes\n            t = Et(char)\n            t1 = stack.pop()\n            t2 = stack.pop()\n\n            # make them children\n            t.right = t1\n            t.left = t2\n\n            # Add this subexpression to stack\n            stack.append(t)\n    # Only element  will be the root of expression tree\n    t = stack.pop()\n    return t\n\n\ndef from_infix_to_postfix(expression):\n    st = list()\n    res = list()\n    priority = {\"+\": 0, \"-\": 0, \"*\": 1, \"/\": 1, \"^\": 2}\n    for e in expression:\n        if e in [\"(\", \"[\"]:\n            st.append(e)\n        elif e == \")\":\n            c = st.pop()\n            while c != \"(\":\n                res.append(c)\n                c = st.pop()\n        elif e == \"]\":\n            c = st.pop()\n            while c != \"[\":\n                res.append(c)\n                c = st.pop()\n        elif e in priority:\n            while len(st) > 0 and st[-1] not in [\"(\", \"[\"] and priority[e] <= priority[st[-1]]:\n                res.append(st.pop())\n            st.append(e)\n        else:\n            res.append(e)\n    while len(st) > 0:\n        res.append(st.pop())\n    return res\n\n\ndef from_infix_to_prefix(expression):\n    st = list()\n    res = list()\n    priority = {\"+\": 0, \"-\": 0, \"*\": 1, \"/\": 1, \"^\": 2}\n    expression = deepcopy(expression)\n    expression.reverse()\n    for e in expression:\n        if e in [\")\", \"]\"]:\n            st.append(e)\n        elif e == \"(\":\n            c = st.pop()\n            while c != \")\":\n                res.append(c)\n                c = st.pop()\n        elif e == \"[\":\n            c = st.pop()\n            while c != \"]\":\n                res.append(c)\n                c = st.pop()\n        elif e in priority:\n            while len(st) > 0 and st[-1] not in [\")\", \"]\"] and priority[e] < priority[st[-1]]:\n                res.append(st.pop())\n            st.append(e)\n        else:\n            res.append(e)\n    while len(st) > 0:\n        res.append(st.pop())\n    res.reverse()\n    return res\n\n\ndef out_expression_list(test, output_lang, num_list, num_stack=None):\n    max_index = output_lang.n_words\n    res = []\n    for i in test:\n        # if i == 0:\n        #     return res\n        if i < max_index - 1:\n            idx = output_lang.index2word[i]\n            if idx[0] == \"N\":\n                if int(idx[1:]) >= len(num_list):\n                    return None\n                res.append(num_list[int(idx[1:])])\n            else:\n                res.append(idx)\n        else:\n            pos_list = num_stack.pop()\n            c = num_list[pos_list[0]]\n            res.append(c)\n    return res\n\n\ndef compute_postfix_expression(post_fix):\n    st = list()\n    operators = [\"+\", \"-\", \"^\", \"*\", \"/\"]\n    for p in post_fix:\n        if p not in operators:\n            pos = re.search(\"\\d+\\(\", p)\n            if pos:\n                st.append(eval(p[pos.start(): pos.end() - 1] + \"+\" + p[pos.end() - 1:]))\n            elif p[-1] == \"%\":\n                    st.append(float(p[:-1]) / 100)\n            else:\n                st.append(eval(p))\n        elif p == \"+\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a + b)\n        elif p == \"*\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a * b)\n        elif p == \"*\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a * b)\n        elif p == \"/\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            if a == 0:\n                return None\n            st.append(b / a)\n        elif p == \"-\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(b - a)\n        elif p == \"^\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a ** b)\n        else:\n            return None\n    if len(st) == 1:\n        return st.pop()\n    return None\n\n\ndef compute_prefix_expression(pre_fix):\n    st = list()\n    operators = [\"+\", \"-\", \"^\", \"*\", \"/\"]\n    pre_fix = deepcopy(pre_fix)\n    pre_fix.reverse()\n    for p in pre_fix:\n        if p not in operators:\n            pos = re.search(\"\\d+\\(\", p)\n            if pos:\n                st.append(eval(p[pos.start(): pos.end() - 1] + \"+\" + p[pos.end() - 1:]))\n            elif p[-1] == \"%\":\n                st.append(float(p[:-1]) / 100)\n            else:\n                st.append(eval(p))\n        elif p == \"+\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a + b)\n        elif p == \"*\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a * b)\n        elif p == \"*\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a * b)\n        elif p == \"/\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            if b == 0:\n                return None\n            st.append(a / b)\n        elif p == \"-\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a - b)\n        elif p == \"^\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            if float(eval(b)) != 2.0 or float(eval(b)) != 3.0:\n                return None\n            st.append(a ** b)\n        else:\n            return None\n    if len(st) == 1:\n        return st.pop()\n    return None\n\n#########################################\n# helper.py\n#########################################\n\ndef gpu_init_pytorch(gpu_num):\n\t'''\n\t\tInitialize GPU\n\t'''\n\ttorch.cuda.set_device(int(gpu_num))\n\tdevice = torch.device(\"cuda:{}\".format(\n\t\tgpu_num) if torch.cuda.is_available() else \"cpu\")\n\treturn device\n\ndef create_save_directories(path):\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\ndef stack_to_string(stack):\n\top = \"\"\n\tfor i in stack:\n\t\tif op == \"\":\n\t\t\top = op + i\n\t\telse:\n\t\t\top = op + ' ' + i\n\treturn op\n\ndef index_batch_to_words(input_batch, input_length, lang):\n\t'''\n\t\tArgs:\n\t\t\tinput_batch: List of BS x Max_len\n\t\t\tinput_length: List of BS\n\t\tReturn:\n\t\t\tcontextual_input: List of BS\n\t'''\n\tcontextual_input = []\n\tfor i in range(len(input_batch)):\n\t\tcontextual_input.append(stack_to_string(sentence_from_indexes(lang, input_batch[i][:input_length[i]])))\n\n\treturn contextual_input\n\ndef sort_by_len(seqs, input_len, device=None, dim=1):\n\torig_idx = list(range(seqs.size(dim)))\n\t# pdb.set_trace()\n\n\t# Index by which sorting needs to be done\n\tsorted_idx = sorted(orig_idx, key=lambda k: input_len[k], reverse=True)\n\tsorted_idx= torch.LongTensor(sorted_idx)\n\tif device:\n\t\tsorted_idx = sorted_idx.to(device)\n\n\tsorted_seqs = seqs.index_select(1, sorted_idx)\n\tsorted_lens=  [input_len[i] for i in sorted_idx]\n\n\t# For restoring original order\n\torig_idx = sorted(orig_idx, key=lambda k: sorted_idx[k])\n\torig_idx = torch.LongTensor(orig_idx)\n\tif device:\n\t\torig_idx = orig_idx.to(device)\n\t\t# sorted_lens = torch.LongTensor(sorted_lens).to(device)\n\treturn sorted_seqs, sorted_lens, orig_idx\n\ndef save_checkpoint(state, epoch, logger, model_path, ckpt):\n\t'''\n\t\tSaves the model state along with epoch number. The name format is important for \n\t\tthe load functions. Don't mess with it.\n\n\t\tArgs:\n\t\t\tmodel state\n\t\t\tepoch number\n\t\t\tlogger variable\n\t\t\tdirectory to save models\n\t\t\tcheckpoint name\n\t'''\n\tckpt_path = os.path.join(model_path, '{}.pt'.format(ckpt))\n\tlogger.info('Saving Checkpoint at : {}'.format(ckpt_path))\n\ttorch.save(state, ckpt_path)\n\ndef load_checkpoint(config, embedding, encoder, predict, generate, merge, mode, ckpt_path, logger, device,\n\t\t\t\t\tembedding_optimizer = None, encoder_optimizer = None, predict_optimizer = None, generate_optimizer = None, merge_optimizer = None,\n\t\t\t\t\tembedding_scheduler = None, encoder_scheduler = None, predict_scheduler = None, generate_scheduler = None, merge_scheduler = None\n\t\t\t\t\t):\n\tcheckpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n\n\tembedding.load_state_dict(checkpoint['embedding_state_dict'])\n\tencoder.load_state_dict(checkpoint['encoder_state_dict'])\n\tpredict.load_state_dict(checkpoint['predict_state_dict'])\n\tgenerate.load_state_dict(checkpoint['generate_state_dict'])\n\tmerge.load_state_dict(checkpoint['merge_state_dict'])\n\n\tif mode == 'train':\n\t\tembedding_optimizer.load_state_dict(checkpoint['embedding_optimizer_state_dict'])\n\t\tencoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n\t\tpredict_optimizer.load_state_dict(checkpoint['predict_optimizer_state_dict'])\n\t\tgenerate_optimizer.load_state_dict(checkpoint['generate_optimizer_state_dict'])\n\t\tmerge_optimizer.load_state_dict(checkpoint['merge_optimizer_state_dict'])\n\n\t\tembedding_scheduler.load_state_dict(checkpoint['embedding_scheduler_state_dict'])\n\t\tencoder_scheduler.load_state_dict(checkpoint['encoder_scheduler_state_dict'])\n\t\tpredict_scheduler.load_state_dict(checkpoint['predict_scheduler_state_dict'])\n\t\tgenerate_scheduler.load_state_dict(checkpoint['generate_scheduler_state_dict'])\n\t\tmerge_scheduler.load_state_dict(checkpoint['merge_scheduler_state_dict'])\n\n\tstart_epoch = checkpoint['epoch']\n\tmin_train_loss  = checkpoint['min_train_loss']\n\tmax_train_acc = checkpoint['max_train_acc']\n\tmax_val_acc = checkpoint['max_val_acc']\n\tequation_acc = checkpoint['equation_acc']\n\tbest_epoch = checkpoint['best_epoch']\n\tgenerate_nums = checkpoint['generate_nums']\n\n\tembedding.to(device)\n\tencoder.to(device)\n\tpredict.to(device)\n\tgenerate.to(device)\n\tmerge.to(device)\n\n\tlogger.info('Successfully Loaded Checkpoint from {}, with epoch number: {} for {}'.format(ckpt_path, start_epoch, mode))\n\n\tif mode == 'train':\n\t\tembedding.train()\n\t\tencoder.train()\n\t\tpredict.train()\n\t\tgenerate.train()\n\t\tmerge.train()\n\telse:\n\t\tembedding.eval()\n\t\tencoder.eval()\n\t\tpredict.eval()\n\t\tgenerate.eval()\n\t\tmerge.eval()\t\t\n\n\treturn start_epoch, min_train_loss, max_train_acc, max_val_acc, equation_acc, best_epoch, generate_nums\n\ndef get_latest_checkpoint(model_path, logger):\n\t'''\n\t\tLooks for the checkpoint with highest epoch number in the directory \"model_path\" \n\n\t\tArgs:\n\t\t\tmodel_path: including the run_name\n\t\t\tlogger variable: to log messages\n\t\tReturns:\n\t\t\tcheckpoint: path to the latest checkpoint \n\t'''\n\n\tckpts = glob('{}/*.pt'.format(model_path))\n\tckpts = sorted(ckpts)\n\n\tif len(ckpts) == 0:\n\t\tlogger.warning('No Checkpoints Found')\n\n\t\treturn None\n\telse:\n\t\t#pdb.set_trace()\n\t\t#latest_epoch = max([int(x.split('_')[-1].split('.')[0]) for x in ckpts])\n\t\t#ckpts = sorted(ckpts, key= lambda x: int(x.split('_')[-1].split('.')[0]) , reverse=True )\n\t\tckpt_path = ckpts[0]\n\t\t#logger.info('Checkpoint found with epoch number : {}'.format(latest_epoch))\n\t\tlogger.debug('Checkpoint found at : {}'.format(ckpt_path))\n\n\t\treturn ckpt_path\n\n#########################################\n# logger.py\n#########################################\n\n'''Logging Modules'''\n\ndef get_logger(name, log_file_path='./logs/temp.log', logging_level=logging.INFO, log_format='%(asctime)s | %(levelname)s | %(filename)s: %(lineno)s : %(funcName)s() ::\\t %(message)s'):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging_level)\n\tformatter = logging.Formatter(log_format)\n\n\tfile_handler = logging.FileHandler(log_file_path, mode='w')\n\tfile_handler.setLevel(logging_level)\n\tfile_handler.setFormatter(formatter)\n\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging_level)\n\tstream_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(stream_handler)\n\n\treturn logger\n\ndef print_log(logger, dict):\n\tstring = ''\n\tfor key, value in dict.items():\n\t\tstring += '\\n {}: {}\\t'.format(key.replace('_', ' '), value)\n\tlogger.info(string)\n\ndef store_results(config, max_train_acc, max_val_acc, eq_acc, min_train_loss, best_epoch):\n\ttry:\n\t\twith open(config.result_path) as f:\n\t\t\tres_data =json.load(f)\n\texcept:\n\t\tres_data = {}\n\ttry:\n\t\tmin_train_loss = min_train_loss.item()\n\texcept:\n\t\tpass\n\t# try:\n\t# \tmin_val_loss = min_val_loss.item()\n\t# except:\n\t# \tpass\n\ttry:\n\t\tdata= {'run name' : str(config.run_name)\n\t\t, 'max val acc': str(max_val_acc)\n\t\t, 'equation acc': str(eq_acc)\n\t\t, 'max train acc': str(max_train_acc)\n\t\t, 'min train loss': str(min_train_loss)\n\t\t, 'best epoch': str(best_epoch)\n\t\t, 'epochs' : config.epochs\n\t\t, 'dataset' : config.dataset\n\t\t, 'embedding': config.embedding\n\t\t, 'embedding_size' : config.embedding_size\n\t\t, 'embedding_lr': config.emb_lr\n\t\t, 'freeze_emb': config.freeze_emb\n\t\t, 'cell_type' : config.cell_type\n\t\t, 'hidden_size' : config.hidden_size\n\t\t, 'depth' : config.depth\n\t\t, 'lr' : config.lr\n\t\t, 'batch_size' : config.batch_size\n\t\t, 'dropout' : config.dropout\n\t\t}\n\t\tres_data[str(config.run_name)] = data\n\n\t\twith open(config.result_path, 'w', encoding='utf-8') as f:\n\t\t\tjson.dump(res_data, f, ensure_ascii= False, indent= 4)\n\texcept:\n\t\tpdb.set_trace()\n\ndef store_val_results(config, acc_score, folds_scores):\n\ttry:\n\t\twith open(config.val_result_path) as f:\n\t\t\tres_data = json.load(f)\n\texcept:\n\t\tres_data = {}\n\n\ttry:\n\t\tdata= {'run_name' : str(config.run_name)\n\t\t, '5-fold avg acc score' : str(acc_score)\n\t\t, 'Fold0 acc' : folds_scores[0]\n\t\t, 'Fold1 acc' : folds_scores[1]\n\t\t, 'Fold2 acc' : folds_scores[2]\n\t\t, 'Fold3 acc' : folds_scores[3]\n\t\t, 'Fold4 acc' : folds_scores[4]\n\t\t, 'epochs' : config.epochs\n\t\t, 'embedding': config.embedding\n\t\t, 'embedding_size' : config.embedding_size\n\t\t, 'embedding_lr': config.emb_lr\n\t\t, 'freeze_emb': config.freeze_emb\n\t\t, 'cell_type' : config.cell_type\n\t\t, 'hidden_size' : config.hidden_size\n\t\t, 'depth' : config.depth\n\t\t, 'lr' : config.lr\n\t\t, 'batch_size' : config.batch_size\n\t\t, 'dropout' : config.dropout\n\t\t}\n\t\tres_data[str(config.run_name)] = data\n\n\t\twith open(config.val_result_path, 'w', encoding='utf-8') as f:\n\t\t\tjson.dump(res_data, f, ensure_ascii= False, indent= 4)\n\texcept:\n\t\tpdb.set_trace()\n        \n\n#########################################\n# pre_data.py\n#########################################\n\nPAD_token = 0\n\nclass Lang:\n\t\"\"\"\n\tclass to save the vocab and two dict: the word->index and index->word\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.word2index = {}\n\t\tself.word2count = {}\n\t\tself.index2word = []\n\t\tself.n_words = 0  # Count word tokens\n\t\tself.num_start = 0\n\n\tdef add_sen_to_vocab(self, sentence):  # add words of sentence to vocab\n\t\tfor word in sentence:\n\t\t\tif re.search(\"N\\d+|NUM|\\d+\", word):\n\t\t\t\tcontinue\n\t\t\tif word not in self.index2word:\n\t\t\t\tself.word2index[word] = self.n_words\n\t\t\t\tself.word2count[word] = 1\n\t\t\t\tself.index2word.append(word)\n\t\t\t\tself.n_words += 1\n\t\t\telse:\n\t\t\t\tself.word2count[word] += 1\n\n\tdef trim(self, logger, min_count):  # trim words below a certain count threshold\n\t\tkeep_words = []\n\n\t\tfor k, v in self.word2count.items():\n\t\t\tif v >= min_count:\n\t\t\t\tkeep_words.append(k)\n\n\t\tlogger.debug('keep_words {} / {} = {}'.format(len(keep_words), len(self.index2word), len(keep_words) / len(self.index2word)))\n\n\t\t# Reinitialize dictionaries\n\t\tself.word2index = {}\n\t\t# self.word2count = {}\n\t\tself.index2word = []\n\t\tself.n_words = 0  # Count default tokens\n\n\t\tfor word in keep_words:\n\t\t\tself.word2index[word] = self.n_words\n\t\t\tself.index2word.append(word)\n\t\t\tself.n_words += 1\n\n\tdef build_input_lang(self, logger, trim_min_count):  # build the input lang vocab and dict\n\t\tif trim_min_count > 0:\n\t\t\tself.trim(logger, trim_min_count)\n\t\t\tself.index2word = [\"PAD\", \"NUM\", \"UNK\"] + self.index2word\n\t\telse:\n\t\t\tself.index2word = [\"PAD\", \"NUM\"] + self.index2word\n\t\tself.word2index = {}\n\t\tself.n_words = len(self.index2word)\n\t\tfor i, j in enumerate(self.index2word):\n\t\t\tself.word2index[j] = i\n\n\tdef build_output_lang(self, generate_num, copy_nums):  # build the output lang vocab and dict\n\t\tself.index2word = [\"PAD\", \"EOS\"] + self.index2word + generate_num + [\"N\" + str(i) for i in range(copy_nums)] +\\\n\t\t\t\t\t\t  [\"SOS\", \"UNK\"]\n\t\tself.n_words = len(self.index2word)\n\t\tfor i, j in enumerate(self.index2word):\n\t\t\tself.word2index[j] = i\n\n\tdef build_output_lang_for_tree(self, generate_num, copy_nums):  # build the output lang vocab and dict\n\t\tself.num_start = len(self.index2word)\n\n\t\tself.index2word = self.index2word + generate_num + [\"N\" + str(i) for i in range(copy_nums)] + [\"UNK\"]\n\t\tself.n_words = len(self.index2word)\n\n\t\tfor i, j in enumerate(self.index2word):\n\t\t\tself.word2index[j] = i\n\ndef load_raw_data(data_path, dataset, is_train = True):  # load the data to list(dict())\n\ttrain_ls = None\n\tif is_train:\n\t\ttrain_path = os.path.join(data_path, dataset, 'train.csv')\n\t\ttrain_df = pd.read_csv(train_path)\n\t\ttrain_ls = train_df.to_dict('records')\n\n\tdev_path = os.path.join(data_path, dataset, 'dev.csv')\n\tdev_df = pd.read_csv(dev_path)\n\tdev_ls = dev_df.to_dict('records')\n\n\treturn train_ls, dev_ls\n\n# remove the superfluous brackets\ndef remove_brackets(x):\n\ty = x\n\tif x[0] == \"(\" and x[-1] == \")\":\n\t\tx = x[1:-1]\n\t\tflag = True\n\t\tcount = 0\n\t\tfor s in x:\n\t\t\tif s == \")\":\n\t\t\t\tcount -= 1\n\t\t\t\tif count < 0:\n\t\t\t\t\tflag = False\n\t\t\t\t\tbreak\n\t\t\telif s == \"(\":\n\t\t\t\tcount += 1\n\t\tif flag:\n\t\t\treturn x\n\treturn y\n\ndef transfer_num(train_ls, dev_ls, chall=False):  # transfer num into \"NUM\"\n\tprint(\"Transfer numbers...\")\n\tdev_pairs = []\n\tgenerate_nums = []\n\tgenerate_nums_dict = {}\n\tcopy_nums = 0\n\n\tif train_ls != None:\n\t\ttrain_pairs = []\n\t\tfor d in train_ls:\n\t\t\t# nums = []\n\t\t\tnums = d['Numbers'].split()\n\t\t\tinput_seq = []\n\t\t\tseg = nltk.word_tokenize(d[\"Question\"].strip())\n\t\t\tequation = d[\"Equation\"].split()\n\n\t\t\tnumz = ['0','1','2','3','4','5','6','7','8','9']\n\t\t\topz = ['+', '-', '*', '/']\n\t\t\tidxs = []\n\t\t\tfor s in range(len(seg)):\n\t\t\t\tif len(seg[s]) >= 7 and seg[s][:6] == \"number\" and seg[s][6] in numz:\n\t\t\t\t\tinput_seq.append(\"NUM\")\n\t\t\t\t\tidxs.append(s)\n\t\t\t\telse:\n\t\t\t\t\tinput_seq.append(seg[s])\n\t\t\tif copy_nums < len(nums):\n\t\t\t\tcopy_nums = len(nums)\n\n\t\t\tout_seq = []\n\t\t\tfor e1 in equation:\n\t\t\t\tif len(e1) >= 7 and e1[:6] == \"number\":\n\t\t\t\t\tout_seq.append('N'+e1[6:])\n\t\t\t\telif e1 not in opz:\n\t\t\t\t\tgenerate_nums.append(e1)\n\t\t\t\t\tif e1 not in generate_nums_dict:\n\t\t\t\t\t\tgenerate_nums_dict[e1] = 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tgenerate_nums_dict[e1] += 1\n\t\t\t\t\tout_seq.append(e1)\n\t\t\t\telse:\n\t\t\t\t\tout_seq.append(e1)\n\n\t\t\ttrain_pairs.append((input_seq, out_seq, nums, idxs))\n\telse:\n\t\ttrain_pairs = None\n\n\tfor d in dev_ls:\n\t\t# nums = []\n\t\tnums = d['Numbers'].split()\n\t\tinput_seq = []\n\t\tseg = nltk.word_tokenize(d[\"Question\"].strip())\n\t\tequation = d[\"Equation\"].split()\n\n\t\tnumz = ['0','1','2','3','4','5','6','7','8','9']\n\t\topz = ['+', '-', '*', '/']\n\t\tidxs = []\n\t\tfor s in range(len(seg)):\n\t\t\tif len(seg[s]) >= 7 and seg[s][:6] == \"number\" and seg[s][6] in numz:\n\t\t\t\tinput_seq.append(\"NUM\")\n\t\t\t\tidxs.append(s)\n\t\t\telse:\n\t\t\t\tinput_seq.append(seg[s])\n\t\tif copy_nums < len(nums):\n\t\t\tcopy_nums = len(nums)\n\n\t\tout_seq = []\n\t\tfor e1 in equation:\n\t\t\tif len(e1) >= 7 and e1[:6] == \"number\":\n\t\t\t\tout_seq.append('N'+e1[6:])\n\t\t\telif e1 not in opz:\n\t\t\t\tgenerate_nums.append(e1)\n\t\t\t\tif e1 not in generate_nums_dict:\n\t\t\t\t\tgenerate_nums_dict[e1] = 1\n\t\t\t\telse:\n\t\t\t\t\tgenerate_nums_dict[e1] += 1\n\t\t\t\tout_seq.append(e1)\n\t\t\telse:\n\t\t\t\tout_seq.append(e1)\n\t\tif chall:\n\t\t\tdev_pairs.append((input_seq, out_seq, nums, idxs, d['Type'], d['Variation Type'], d['Annotator'], d['Alternate']))\n\t\telse:\n\t\t\tdev_pairs.append((input_seq, out_seq, nums, idxs))\n\n\ttemp_g = []\n\tfor g in generate_nums_dict:\n\t\tif generate_nums_dict[g] >= 5:\n\t\t\ttemp_g.append(g)\n\treturn train_pairs, dev_pairs, temp_g, copy_nums\n\n# Return a list of indexes, one for each word in the sentence, plus EOS\ndef indexes_from_sentence(lang, sentence, tree=False):\n\tres = []\n\tfor word in sentence:\n\t\tif len(word) == 0:\n\t\t\tcontinue\n\t\tif word in lang.word2index:\n\t\t\tres.append(lang.word2index[word])\n\t\telse:\n\t\t\tres.append(lang.word2index[\"UNK\"])\n\tif \"EOS\" in lang.index2word and not tree:\n\t\tres.append(lang.word2index[\"EOS\"])\n\treturn res\n\ndef sentence_from_indexes(lang, indexes):\n\tsent = []\n\tfor ind in indexes:\n\t\tsent.append(lang.index2word[ind])\n\treturn sent\n\ndef prepare_data(config, logger, pairs_trained, pairs_tested, trim_min_count, generate_nums, copy_nums, input_lang=None, output_lang=None, tree=False):\n\tif input_lang == None:\n\t\tinput_lang = Lang()\n\tif output_lang == None:\n\t\toutput_lang = Lang()\n\n\ttest_pairs = []\n\ttrain_pairs = None\n\n\tif pairs_trained != None:\n\t\ttrain_pairs = []\n\t\tfor pair in pairs_trained:\n\t\t\tif not tree:\n\t\t\t\tinput_lang.add_sen_to_vocab(pair[0])\n\t\t\t\toutput_lang.add_sen_to_vocab(pair[1])\n\t\t\telif pair[-1]:\n\t\t\t\tinput_lang.add_sen_to_vocab(pair[0])\n\t\t\t\toutput_lang.add_sen_to_vocab(pair[1])\n\n\tif config.embedding == 'bert' or config.embedding == 'roberta':\n\t\tfor pair in pairs_tested:\n\t\t\tif not tree:\n\t\t\t\tinput_lang.add_sen_to_vocab(pair[0])\n\t\t\telif pair[-1]:\n\t\t\t\tinput_lang.add_sen_to_vocab(pair[0])\n\n\tif pairs_trained != None:\n\n\t\tinput_lang.build_input_lang(logger, trim_min_count)\n\t\tif tree:\n\t\t\toutput_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n\t\telse:\n\t\t\toutput_lang.build_output_lang(generate_nums, copy_nums)\n\n\t\tfor pair in pairs_trained:\n\t\t\tnum_stack = []\n\t\t\tfor word in pair[1]: # For each token in equation\n\t\t\t\ttemp_num = []\n\t\t\t\tflag_not = True\n\t\t\t\tif word not in output_lang.index2word: # If token is not in output vocab\n\t\t\t\t\tflag_not = False\n\t\t\t\t\tfor i, j in enumerate(pair[2]):\n\t\t\t\t\t\tif j == word:\n\t\t\t\t\t\t\ttemp_num.append(i) # Append number list index of token not in output vocab\n\n\t\t\t\tif not flag_not and len(temp_num) != 0: # Equation has an unknown token and it is a number present in number list (could be default number with freq < 5)\n\t\t\t\t\tnum_stack.append(temp_num)\n\t\t\t\tif not flag_not and len(temp_num) == 0: # Equation has an unknown token but it is not a number from number list\n\t\t\t\t\tnum_stack.append([_ for _ in range(len(pair[2]))])\n\n\t\t\tnum_stack.reverse()\n\t\t\tinput_cell = indexes_from_sentence(input_lang, pair[0])\n\t\t\toutput_cell = indexes_from_sentence(output_lang, pair[1], tree)\n\t\t\ttrain_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n\t\t\t\t\t\t\t\tpair[2], pair[3], num_stack))\n\n\tlogger.debug('Indexed {} words in input language, {} words in output'.format(input_lang.n_words, output_lang.n_words))\n\n\tfor pair in pairs_tested:\n\t\tnum_stack = []\n\t\tfor word in pair[1]:\n\t\t\ttemp_num = []\n\t\t\tflag_not = True\n\t\t\tif word not in output_lang.index2word:\n\t\t\t\tflag_not = False\n\t\t\t\tfor i, j in enumerate(pair[2]):\n\t\t\t\t\tif j == word:\n\t\t\t\t\t\ttemp_num.append(i)\n\n\t\t\tif not flag_not and len(temp_num) != 0:\n\t\t\t\tnum_stack.append(temp_num)\n\t\t\tif not flag_not and len(temp_num) == 0:\n\t\t\t\tnum_stack.append([_ for _ in range(len(pair[2]))])\n\n\t\tnum_stack.reverse()\n\t\tinput_cell = indexes_from_sentence(input_lang, pair[0])\n\t\toutput_cell = indexes_from_sentence(output_lang, pair[1], tree)\n\t\tif config.challenge_disp:\n\t\t\ttest_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n\t\t\t\t\t\t   pair[2], pair[3], num_stack, pair[4], pair[5], pair[6], pair[7]))\n\t\telse:\n\t\t\ttest_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n\t\t\t\t\t\t   pair[2], pair[3], num_stack))\n\n\treturn input_lang, output_lang, train_pairs, test_pairs\n\n# Pad a with the PAD symbol\ndef pad_seq(seq, seq_len, max_length):\n\tseq += [PAD_token for _ in range(max_length - seq_len)]\n\treturn seq\n\n# prepare the batches\ndef prepare_train_batch(pairs_to_batch, batch_size):\n\tpairs = copy.deepcopy(pairs_to_batch)\n\trandom.shuffle(pairs)  # shuffle the pairs\n\tpos = 0\n\tinput_lengths = []\n\toutput_lengths = []\n\tnums_batches = []\n\tbatches = []\n\tinput_batches = []\n\toutput_batches = []\n\tnum_stack_batches = []  # save the num stack which\n\tnum_pos_batches = []\n\tnum_size_batches = []\n\twhile pos + batch_size < len(pairs):\n\t\tbatches.append(pairs[pos:pos+batch_size])\n\t\tpos += batch_size\n\tbatches.append(pairs[pos:])\n\n\tfor batch in batches:\n\t\tbatch = sorted(batch, key=lambda tp: tp[1], reverse=True)\n\t\tinput_length = []\n\t\toutput_length = []\n\t\tfor _, i, _, j, _, _, _ in batch:\n\t\t\tinput_length.append(i)\n\t\t\toutput_length.append(j)\n\t\tinput_lengths.append(input_length)\n\t\toutput_lengths.append(output_length)\n\t\tinput_len_max = input_length[0]\n\t\toutput_len_max = max(output_length)\n\t\tinput_batch = []\n\t\toutput_batch = []\n\t\tnum_batch = []\n\t\tnum_stack_batch = []\n\t\tnum_pos_batch = []\n\t\tnum_size_batch = []\n\t\tfor i, li, j, lj, num, num_pos, num_stack in batch:\n\t\t\tnum_batch.append(len(num))\n\t\t\tinput_batch.append(pad_seq(i, li, input_len_max))\n\t\t\toutput_batch.append(pad_seq(j, lj, output_len_max))\n\t\t\tnum_stack_batch.append(num_stack)\n\t\t\tnum_pos_batch.append(num_pos)\n\t\t\tnum_size_batch.append(len(num_pos))\n\t\tinput_batches.append(input_batch)\n\t\tnums_batches.append(num_batch)\n\t\toutput_batches.append(output_batch)\n\t\tnum_stack_batches.append(num_stack_batch)\n\t\tnum_pos_batches.append(num_pos_batch)\n\t\tnum_size_batches.append(num_size_batch)\n\treturn input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches\n\ndef get_num_stack(eq, output_lang, num_pos):\n\tnum_stack = []\n\tfor word in eq:\n\t\ttemp_num = []\n\t\tflag_not = True\n\t\tif word not in output_lang.index2word:\n\t\t\tflag_not = False\n\t\t\tfor i, j in enumerate(num_pos):\n\t\t\t\tif j == word:\n\t\t\t\t\ttemp_num.append(i)\n\t\tif not flag_not and len(temp_num) != 0:\n\t\t\tnum_stack.append(temp_num)\n\t\tif not flag_not and len(temp_num) == 0:\n\t\t\tnum_stack.append([_ for _ in range(len(num_pos))])\n\tnum_stack.reverse()\n\treturn num_stack","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:40:04.776410Z","iopub.execute_input":"2024-11-15T11:40:04.776840Z","iopub.status.idle":"2024-11-15T11:40:04.903488Z","shell.execute_reply.started":"2024-11-15T11:40:04.776802Z","shell.execute_reply":"2024-11-15T11:40:04.902481Z"}},"outputs":[],"execution_count":114},{"cell_type":"markdown","source":"## args.py","metadata":{}},{"cell_type":"code","source":"### Add Early Stopping ###\n\ndef build_parser():\n\t# Data loading parameters\n\tparser = argparse.ArgumentParser(description='Run Single sequence model')\n\n\tparser.add_argument('-mode', type=str, default='train', choices=['train', 'test'], help='Modes: train, test')\n\n\t# Run Config\n\tparser.add_argument('-run_name', type=str, default='debug', help='run name for logs')\n\tparser.add_argument('-dataset', type=str, default='mawps_fold0', help='Dataset')\n\tparser.add_argument('-outputs', dest='outputs', action='store_true', help='Show full validation outputs')\n\tparser.add_argument('-no-outputs', dest='outputs', action='store_false', help='Do not show full validation outputs')\n\tparser.set_defaults(outputs=True)\n\tparser.add_argument('-results', dest='results', action='store_true', help='Store results')\n\tparser.add_argument('-no-results', dest='results', action='store_false', help='Do not store results')\n\tparser.set_defaults(results=True)\n\n\t# Meta Attributes\n\t# parser.add_argument('-vocab_size', type=int, default=30000, help='Vocabulary size to consider')\n\tparser.add_argument('-trim_threshold', type=int, default=1, help='Remove words with frequency less than this from vocab')\n\n\t# Device Configuration\n\tparser.add_argument('-gpu', type=int, default=2, help='Specify the gpu to use')\n\tparser.add_argument('-seed', type=int, default=6174, help='Default seed to set')\n\tparser.add_argument('-logging', type=int, default=1, help='Set to 0 if you do not require logging')\n\tparser.add_argument('-ckpt', type=str, default='model', help='Checkpoint file name')\n\tparser.add_argument('-save_model', dest='save_model',action='store_true', help='To save the model')\n\tparser.add_argument('-no-save_model', dest='save_model', action='store_false', help='Dont save the model')\n\tparser.set_defaults(save_model=False)\n\t# parser.add_argument('-log_fmt', type=str, default='%(asctime)s | %(levelname)s | %(name)s | %(message)s', help='Specify format of the logger')\n\n\t# Model parameters\n\t# parser.add_argument('-cell_type', type=str, default='gru', help='RNN cell for encoder, default: gru')\n\tparser.add_argument('-embedding', type=str, default='roberta', choices=['bert', 'roberta', 'word2vec', 'random'], help='Embeddings')\n\tparser.add_argument('-emb_name', type=str, default='roberta-base', choices=['bert-base-uncased', 'roberta-base'], help='Which pre-trained model')\n\tparser.add_argument('-embedding_size', type=int, default=768, help='Embedding dimensions of inputs')\n\tparser.add_argument('-emb_lr', type=float, default=8e-6, help='Larning rate to train embeddings')\n\tparser.add_argument('-freeze_emb', dest='freeze_emb', action='store_true', help='Freeze embedding weights')\n\tparser.add_argument('-no-freeze_emb', dest='freeze_emb', action='store_false', help='Train embedding weights')\n\tparser.set_defaults(freeze_emb=False)\n\tparser.add_argument('-word2vec_bin', type=str, default='/datadrive/global_files/GoogleNews-vectors-negative300.bin', help='Binary file of word2vec')\n\n\tparser.add_argument('-cell_type', type=str, default='lstm', help='RNN cell for encoder and decoder, default: lstm')\n\tparser.add_argument('-hidden_size', type=int, default=512, help='Number of hidden units in each layer')\n\tparser.add_argument('-depth', type=int, default=2, help='Number of layers in each encoder')\n\tparser.add_argument('-lr', type=float, default=0.0008, help='Learning rate')\n\tparser.add_argument('-batch_size', type=int, default=4, help='Batch size')\n\tparser.add_argument('-weight_decay', type=float, default=1e-5, help='Weight Decay')\n\tparser.add_argument('-beam_size', type=float, default=5, help='Beam Size')\n\tparser.add_argument('-epochs', type=int, default=50, help='Maximum # of training epochs')\t\n\tparser.add_argument('-dropout', type=float, default=0.5, help= 'Dropout probability for input/output/state units (0.0: no dropout)')\n\t\n\t# parser.add_argument('-max_length', type=int, default=100, help='Specify max decode steps: Max length string to output')\n\t# parser.add_argument('-init_range', type=float, default=0.08, help='Initialization range for seq2seq model')\n\t# parser.add_argument('-bidirectional', dest='bidirectional', action='store_true', help='Bidirectionality in LSTMs')\n\t# parser.add_argument('-no-bidirectional', dest='bidirectional', action='store_false', help='Bidirectionality in LSTMs')\n\t# parser.set_defaults(bidirectional=False)\n\t\n\t# parser.add_argument('-max_grad_norm', type=float, default=0.25, help='Clip gradients to this norm')\n\t# parser.add_argument('-opt', type=str, default='adam', choices=['adam', 'adadelta', 'sgd', 'asgd'], help='Optimizer for training')\n\n\t# parser.add_argument('-grade_disp', dest='grade_disp', action='store_true', help='Display grade information in validation outputs')\n\t# parser.add_argument('-no-grade_disp', dest='grade_disp', action='store_false', help='Don\\'t display grade information')\n\t# parser.set_defaults(grade_disp=True)\n\t# parser.add_argument('-type_disp', dest='type_disp', action='store_true', help='Display Type information in validation outputs')\n\t# parser.add_argument('-no-type_disp', dest='type_disp', action='store_false', help='Don\\'t display Type information')\n\t# parser.set_defaults(type_disp=True)\n\tparser.add_argument('-nums_disp', dest='nums_disp', action='store_true', help='Display number of numbers information in validation outputs')\n\tparser.add_argument('-no-nums_disp', dest='nums_disp', action='store_false', help='Don\\'t display number of numbers information')\n\tparser.set_defaults(nums_disp=True)\n\tparser.add_argument('-challenge_disp', dest='challenge_disp', action='store_true', help='Display information in validation outputs')\n\tparser.add_argument('-no-challenge_disp', dest='challenge_disp', action='store_false', help='Don\\'t display information')\n\tparser.set_defaults(challenge_disp=False)\n\n\tparser.add_argument('-show_train_acc', dest='show_train_acc', action='store_true', help='Calculate the train accuracy')\n\tparser.add_argument('-no-show_train_acc', dest='show_train_acc', action='store_false', help='Don\\'t calculate the train accuracy')\n\tparser.set_defaults(show_train_acc=True)\n\n\tparser.add_argument('-full_cv', dest='full_cv', action='store_true', help='5-fold CV')\n\tparser.add_argument('-no-full_cv', dest='full_cv', action='store_false', help='No 5-fold CV')\n\tparser.set_defaults(full_cv=False)\n\n\tparser.add_argument('-len_generate_nums', type=int, default=0, help='store length of generate_nums')\n\tparser.add_argument('-copy_nums', type=int, default=0, help='store copy_nums')\n\t\n\treturn parser\n\n\ndef parse_arguments(arg_dict=None):\n    parser = build_parser()\n    if arg_dict:\n        # Override default values with provided dictionary values\n        args = parser.parse_args([])\n        for key, value in arg_dict.items():\n            setattr(args, key, value)\n        return args\n    else:\n        return parser.parse_args()  # If no dictionary is provided, use default command line arguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:40:04.905777Z","iopub.execute_input":"2024-11-15T11:40:04.906236Z","iopub.status.idle":"2024-11-15T11:40:04.930034Z","shell.execute_reply.started":"2024-11-15T11:40:04.906190Z","shell.execute_reply":"2024-11-15T11:40:04.929039Z"}},"outputs":[],"execution_count":115},{"cell_type":"markdown","source":"## train & evaluate","metadata":{}},{"cell_type":"code","source":"MAX_OUTPUT_LENGTH = 45\nMAX_INPUT_LENGTH = 120\nUSE_CUDA = torch.cuda.is_available()\n\nclass Beam:  # the class save the beam node\n\tdef __init__(self, score, input_var, hidden, all_output):\n\t\tself.score = score\n\t\tself.input_var = input_var\n\t\tself.hidden = hidden\n\t\tself.all_output = all_output\n\ndef time_since(s):  # compute time\n\tm = math.floor(s / 60)\n\ts -= m * 60\n\th = math.floor(m / 60)\n\tm -= h * 60\n\treturn '%dh %dm %ds' % (h, m, s)\n\ndef generate_rule_mask(decoder_input, nums_batch, word2index, batch_size, nums_start, copy_nums, generate_nums,\n\t\t\t\t\t   english):\n\trule_mask = torch.FloatTensor(batch_size, nums_start + copy_nums).fill_(-float(\"1e12\"))\n\tif english:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + \\\n\t\t\t\t\t  [word2index[\"(\"]] + generate_nums\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start:\n\t\t\t\tres += [word2index[\")\"], word2index[\"+\"], word2index[\"-\"],\n\t\t\t\t\t\tword2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] in generate_nums:\n\t\t\t\tres += [word2index[\")\"], word2index[\"+\"], word2index[\"-\"],\n\t\t\t\t\t\tword2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] == word2index[\"(\"]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] +\\\n\t\t\t\t  [word2index[\"(\"]] + generate_nums\n\t\t\telif decoder_input[i] == word2index[\")\"]:\n\t\t\t\tres += [word2index[\")\"], word2index[\"+\"], word2index[\"-\"],\n\t\t\t\t\t\tword2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + [word2index[\"(\"]] + generate_nums\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\telse:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + \\\n\t\t\t\t\t  [word2index[\"[\"], word2index[\"(\"]] + generate_nums\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [word2index[\"]\"], word2index[\")\"], word2index[\"+\"],\n\t\t\t\t\t\tword2index[\"-\"], word2index[\"/\"], word2index[\"^\"],\n\t\t\t\t\t\tword2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] == word2index[\"[\"] or decoder_input[i] == word2index[\"(\"]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] +\\\n\t\t\t\t  [word2index[\"(\"]] + generate_nums\n\t\t\telif decoder_input[i] == word2index[\")\"]:\n\t\t\t\tres += [word2index[\"]\"], word2index[\")\"], word2index[\"+\"],\n\t\t\t\t\t\tword2index[\"-\"], word2index[\"/\"], word2index[\"^\"],\n\t\t\t\t\t\tword2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"]\"]:\n\t\t\t\tres += [word2index[\"+\"], word2index[\"*\"], word2index[\"-\"], word2index[\"/\"], word2index[\"EOS\"]]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"],\n\t\t\t\t\t\t\t\t\t  word2index[\"*\"], word2index[\"^\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] +\\\n\t\t\t\t  [word2index[\"[\"], word2index[\"(\"]] + generate_nums\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\treturn rule_mask\n\n\ndef generate_pre_tree_seq_rule_mask(decoder_input, nums_batch, word2index, batch_size, nums_start, copy_nums,\n\t\t\t\t\t\t\t\t\tgenerate_nums, english):\n\trule_mask = torch.FloatTensor(batch_size, nums_start + copy_nums).fill_(-float(\"1e12\"))\n\tif english:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t  [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\telse:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t  [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"^\"]]\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"],\n\t\t\t\t\t\tword2index[\"^\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"],\n\t\t\t\t\t\t\t\t\t  word2index[\"^\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"^\"]]\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\treturn rule_mask\n\n\ndef generate_post_tree_seq_rule_mask(decoder_input, nums_batch, word2index, batch_size, nums_start, copy_nums,\n\t\t\t\t\t\t\t\t\t generate_nums, english):\n\trule_mask = torch.FloatTensor(batch_size, nums_start + copy_nums).fill_(-float(\"1e12\"))\n\tif english:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums +\\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\telse:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"^\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"],\n\t\t\t\t\t\t\t\t\t  word2index[\"^\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"^\"],\n\t\t\t\t\t\tword2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\treturn rule_mask\n\ndef generate_tree_input(target, decoder_output, nums_stack_batch, num_start, unk):\n\t# when the decoder input is copied num but the num has two pos, chose the max\n\ttarget_input = copy.deepcopy(target)\n\tfor i in range(len(target)):\n\t\tif target[i] == unk:\n\t\t\tnum_stack = nums_stack_batch[i].pop()\n\t\t\tmax_score = -float(\"1e12\")\n\t\t\tfor num in num_stack:\n\t\t\t\tif decoder_output[i, num_start + num] > max_score:\n\t\t\t\t\ttarget[i] = num + num_start\n\t\t\t\t\tmax_score = decoder_output[i, num_start + num]\n\t\tif target_input[i] >= num_start:\n\t\t\ttarget_input[i] = 0\n\treturn torch.LongTensor(target), torch.LongTensor(target_input)\n\ndef generate_decoder_input(target, decoder_output, nums_stack_batch, num_start, unk):\n\t# when the decoder input is copied num but the num has two pos, chose the max\n\tif USE_CUDA:\n\t\tdecoder_output = decoder_output.cpu()\n\tfor i in range(target.size(0)):\n\t\tif target[i] == unk:\n\t\t\tnum_stack = nums_stack_batch[i].pop()\n\t\t\tmax_score = -float(\"1e12\")\n\t\t\tfor num in num_stack:\n\t\t\t\tif decoder_output[i, num_start + num] > max_score:\n\t\t\t\t\ttarget[i] = num + num_start\n\t\t\t\t\tmax_score = decoder_output[i, num_start + num]\n\treturn target\n\ndef mask_num(encoder_outputs, decoder_input, embedding_size, nums_start, copy_nums, num_pos):\n\t# mask the decoder input number and return the mask tensor and the encoder position Hidden vector\n\tup_num_start = decoder_input >= nums_start\n\tdown_num_end = decoder_input < (nums_start + copy_nums)\n\tnum_mask = up_num_start == down_num_end\n\tnum_mask_encoder = num_mask < 1\n\tnum_mask_encoder = num_mask_encoder.unsqueeze(1)  # ByteTensor size: B x 1\n\trepeat_dims = [1] * num_mask_encoder.dim()\n\trepeat_dims[1] = embedding_size\n\tnum_mask_encoder = num_mask_encoder.repeat(*repeat_dims)  # B x 1 -> B x Decoder_embedding_size\n\n\tall_embedding = encoder_outputs.transpose(0, 1).contiguous()\n\tall_embedding = all_embedding.view(-1, encoder_outputs.size(2))  # S x B x H -> (B x S) x H\n\tindices = decoder_input - nums_start\n\tindices = indices * num_mask.long()  # 0 or the num pos in sentence\n\tindices = indices.tolist()\n\tfor k in range(len(indices)):\n\t\tindices[k] = num_pos[k][indices[k]]\n\tindices = torch.LongTensor(indices)\n\tif USE_CUDA:\n\t\tindices = indices.cuda()\n\tbatch_size = decoder_input.size(0)\n\tsen_len = encoder_outputs.size(0)\n\tbatch_num = torch.LongTensor(range(batch_size))\n\tbatch_num = batch_num * sen_len\n\tif USE_CUDA:\n\t\tbatch_num = batch_num.cuda()\n\tindices = batch_num + indices\n\tnum_encoder = all_embedding.index_select(0, indices)\n\treturn num_mask, num_encoder, num_mask_encoder\n\ndef out_equation(test, output_lang, num_list, num_stack=None):\n\ttest = test[:-1]\n\tmax_index = len(output_lang.index2word) - 1\n\ttest_str = \"\"\n\tfor i in test:\n\t\tif i < max_index:\n\t\t\tc = output_lang.index2word[i]\n\t\t\tif c == \"^\":\n\t\t\t\ttest_str += \"**\"\n\t\t\telif c == \"[\":\n\t\t\t\ttest_str += \"(\"\n\t\t\telif c == \"]\":\n\t\t\t\ttest_str += \")\"\n\t\t\telif c[0] == \"N\":\n\t\t\t\tif int(c[1:]) >= len(num_list):\n\t\t\t\t\treturn None\n\t\t\t\tx = num_list[int(c[1:])]\n\t\t\t\tif x[-1] == \"%\":\n\t\t\t\t\ttest_str += \"(\" + x[:-1] + \"/100\" + \")\"\n\t\t\t\telse:\n\t\t\t\t\ttest_str += x\n\t\t\telse:\n\t\t\t\ttest_str += c\n\t\telse:\n\t\t\tif len(num_stack) == 0:\n\t\t\t\tprint(test_str, num_list)\n\t\t\t\treturn \"\"\n\t\t\tn_pos = num_stack.pop()\n\t\t\ttest_str += num_list[n_pos[0]]\n\treturn test_str\n\ndef compute_prefix_tree_result(test_res, test_tar, output_lang, num_list, num_stack):\n\t# print(test_res, test_tar)\n\n\tif len(num_stack) == 0 and test_res == test_tar:\n\t\treturn True, True, test_res, test_tar\n\ttest = out_expression_list(test_res, output_lang, num_list)\n\ttar = out_expression_list(test_tar, output_lang, num_list, copy.deepcopy(num_stack))\n\t# print(test, tar)\n\tif test is None:\n\t\treturn False, False, test, tar\n\tif test == tar:\n\t\treturn True, True, test, tar\n\ttry:\n\t\tif abs(compute_prefix_expression(test) - compute_prefix_expression(tar)) < 1e-4:\n\t\t\treturn True, False, test, tar\n\t\telse:\n\t\t\treturn False, False, test, tar\n\texcept:\n\t\treturn False, False, test, tar\n\ndef compute_postfix_tree_result(test_res, test_tar, output_lang, num_list, num_stack):\n\t# print(test_res, test_tar)\n\n\tif len(num_stack) == 0 and test_res == test_tar:\n\t\treturn True, True, test_res, test_tar\n\ttest = out_expression_list(test_res, output_lang, num_list)\n\ttar = out_expression_list(test_tar, output_lang, num_list, copy.deepcopy(num_stack))\n\t# print(test, tar)\n\tif test is None:\n\t\treturn False, False, test, tar\n\tif test == tar:\n\t\treturn True, True, test, tar\n\ttry:\n\t\tif abs(compute_postfix_expression(test) - compute_postfix_expression(tar)) < 1e-4:\n\t\t\treturn True, False, test, tar\n\t\telse:\n\t\t\treturn False, False, test, tar\n\texcept:\n\t\treturn False, False, test, tar\n\ndef compute_result(test_res, test_tar, output_lang, num_list, num_stack):\n\tif len(num_stack) == 0 and test_res == test_tar:\n\t\treturn True, True\n\ttest = out_equation(test_res, output_lang, num_list)\n\ttar = out_equation(test_tar, output_lang, num_list, copy.deepcopy(num_stack))\n\tif test is None:\n\t\treturn False, False\n\tif test == tar:\n\t\treturn True, True\n\ttry:\n\t\tif abs(eval(test) - eval(tar)) < 1e-4:\n\t\t\treturn True, False\n\t\telse:\n\t\t\treturn False, False\n\texcept:\n\t\treturn False, False\n\ndef get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size, hidden_size):\n\tindices = list()\n\tsen_len = encoder_outputs.size(0)\n\tmasked_index = []\n\ttemp_1 = [1 for _ in range(hidden_size)]\n\ttemp_0 = [0 for _ in range(hidden_size)]\n\tfor b in range(batch_size):\n\t\tfor i in num_pos[b]:\n\t\t\tindices.append(i + b * sen_len)\n\t\t\tmasked_index.append(temp_0)\n\t\tindices += [0 for _ in range(len(num_pos[b]), num_size)]\n\t\tmasked_index += [temp_1 for _ in range(len(num_pos[b]), num_size)]\n\tindices = torch.LongTensor(indices)\n\tmasked_index = torch.BoolTensor(masked_index)\n\tmasked_index = masked_index.view(batch_size, num_size, hidden_size)\n\tif USE_CUDA:\n\t\tindices = indices.cuda()\n\t\tmasked_index = masked_index.cuda()\n\tall_outputs = encoder_outputs.transpose(0, 1).contiguous()\n\tall_embedding = all_outputs.view(-1, encoder_outputs.size(2))  # S x B x H -> (B x S) x H\n\tall_num = all_embedding.index_select(0, indices)\n\tall_num = all_num.view(batch_size, num_size, hidden_size)\n\treturn all_num.masked_fill_(masked_index, 0.0)\n\ndef train_attn(input_batch, input_length, target_batch, target_length, num_batch, nums_stack_batch, copy_nums,\n\t\t\t   generate_nums, encoder, decoder, encoder_optimizer, decoder_optimizer, output_lang, clip=0,\n\t\t\t   use_teacher_forcing=1, beam_size=1, english=False):\n\tseq_mask = []\n\tmax_len = max(input_length)\n\tfor i in input_length:\n\t\tseq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n\tseq_mask = torch.BoolTensor(seq_mask)\n\n\tnum_start = output_lang.n_words - copy_nums - 2\n\tunk = output_lang.word2index[\"UNK\"]\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).transpose(0, 1)\n\ttarget = torch.LongTensor(target_batch).transpose(0, 1)\n\n\tbatch_size = len(input_length)\n\n\tencoder.train()\n\tdecoder.train()\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\tseq_mask = seq_mask.cuda()\n\n\t# Zero gradients of both optimizers\n\tencoder_optimizer.zero_grad()\n\tdecoder_optimizer.zero_grad()\n\t# Run words through encoder\n\tencoder_outputs, encoder_hidden = encoder(input_var, input_length, None)\n\n\t# Prepare input and output variables\n\tdecoder_input = torch.LongTensor([output_lang.word2index[\"SOS\"]] * batch_size)\n\n\tdecoder_hidden = encoder_hidden[:decoder.n_layers]  # Use last (forward) hidden state from encoder\n\n\tmax_target_length = max(target_length)\n\tall_decoder_outputs = torch.zeros(max_target_length, batch_size, decoder.output_size)\n\n\t# Move new Variables to CUDA\n\tif USE_CUDA:\n\t\tall_decoder_outputs = all_decoder_outputs.cuda()\n\n\tif random.random() < use_teacher_forcing:\n\t\t# Run through decoder one time step at a time\n\t\tfor t in range(max_target_length):\n\t\t\tif USE_CUDA:\n\t\t\t\tdecoder_input = decoder_input.cuda()\n\n\t\t\tdecoder_output, decoder_hidden = decoder(\n\t\t\t\tdecoder_input, decoder_hidden, encoder_outputs, seq_mask)\n\t\t\tall_decoder_outputs[t] = decoder_output\n\t\t\tdecoder_input = generate_decoder_input(\n\t\t\t\ttarget[t], decoder_output, nums_stack_batch, num_start, unk)\n\t\t\ttarget[t] = decoder_input\n\telse:\n\t\tbeam_list = list()\n\t\tscore = torch.zeros(batch_size)\n\t\tif USE_CUDA:\n\t\t\tscore = score.cuda()\n\t\tbeam_list.append(Beam(score, decoder_input, decoder_hidden, all_decoder_outputs))\n\t\t# Run through decoder one time step at a time\n\t\tfor t in range(max_target_length):\n\t\t\tbeam_len = len(beam_list)\n\t\t\tbeam_scores = torch.zeros(batch_size, decoder.output_size * beam_len)\n\t\t\tall_hidden = torch.zeros(decoder_hidden.size(0), batch_size * beam_len, decoder_hidden.size(2))\n\t\t\tall_outputs = torch.zeros(max_target_length, batch_size * beam_len, decoder.output_size)\n\t\t\tif USE_CUDA:\n\t\t\t\tbeam_scores = beam_scores.cuda()\n\t\t\t\tall_hidden = all_hidden.cuda()\n\t\t\t\tall_outputs = all_outputs.cuda()\n\n\t\t\tfor b_idx in range(len(beam_list)):\n\t\t\t\tdecoder_input = beam_list[b_idx].input_var\n\t\t\t\tdecoder_hidden = beam_list[b_idx].hidden\n\n\t\t\t\trule_mask = generate_rule_mask(decoder_input, num_batch, output_lang.word2index, batch_size,\n\t\t\t\t\t\t\t\t\t\t\t   num_start, copy_nums, generate_nums, english)\n\t\t\t\tif USE_CUDA:\n\t\t\t\t\trule_mask = rule_mask.cuda()\n\t\t\t\t\tdecoder_input = decoder_input.cuda()\n\n\t\t\t\tdecoder_output, decoder_hidden = decoder(\n\t\t\t\t\tdecoder_input, decoder_hidden, encoder_outputs, seq_mask)\n\n\t\t\t\tscore = f.log_softmax(decoder_output, dim=1) + rule_mask\n\t\t\t\tbeam_score = beam_list[b_idx].score\n\t\t\t\tbeam_score = beam_score.unsqueeze(1)\n\t\t\t\trepeat_dims = [1] * beam_score.dim()\n\t\t\t\trepeat_dims[1] = score.size(1)\n\t\t\t\tbeam_score = beam_score.repeat(*repeat_dims)\n\t\t\t\tscore += beam_score\n\t\t\t\tbeam_scores[:, b_idx * decoder.output_size: (b_idx + 1) * decoder.output_size] = score\n\t\t\t\tall_hidden[:, b_idx * batch_size:(b_idx + 1) * batch_size, :] = decoder_hidden\n\n\t\t\t\tbeam_list[b_idx].all_output[t] = decoder_output\n\t\t\t\tall_outputs[:, batch_size * b_idx: batch_size * (b_idx + 1), :] = \\\n\t\t\t\t\tbeam_list[b_idx].all_output\n\t\t\ttopv, topi = beam_scores.topk(beam_size, dim=1)\n\t\t\tbeam_list = list()\n\n\t\t\tfor k in range(beam_size):\n\t\t\t\ttemp_topk = topi[:, k]\n\t\t\t\ttemp_input = temp_topk % decoder.output_size\n\t\t\t\ttemp_input = temp_input.data\n\t\t\t\tif USE_CUDA:\n\t\t\t\t\ttemp_input = temp_input.cpu()\n\t\t\t\ttemp_beam_pos = temp_topk / decoder.output_size\n\n\t\t\t\tindices = torch.LongTensor(range(batch_size))\n\t\t\t\tif USE_CUDA:\n\t\t\t\t\tindices = indices.cuda()\n\t\t\t\tindices += temp_beam_pos * batch_size\n\n\t\t\t\ttemp_hidden = all_hidden.index_select(1, indices)\n\t\t\t\ttemp_output = all_outputs.index_select(1, indices)\n\n\t\t\t\tbeam_list.append(Beam(topv[:, k], temp_input, temp_hidden, temp_output))\n\t\tall_decoder_outputs = beam_list[0].all_output\n\n\t\tfor t in range(max_target_length):\n\t\t\ttarget[t] = generate_decoder_input(\n\t\t\t\ttarget[t], all_decoder_outputs[t], nums_stack_batch, num_start, unk)\n\t# Loss calculation and backpropagation\n\n\tif USE_CUDA:\n\t\ttarget = target.cuda()\n\n\tloss = masked_cross_entropy(\n\t\tall_decoder_outputs.transpose(0, 1).contiguous(),  # -> batch x seq\n\t\ttarget.transpose(0, 1).contiguous(),  # -> batch x seq\n\t\ttarget_length\n\t)\n\n\tloss.backward()\n\treturn_loss = loss.item()\n\n\t# Clip gradient norms\n\tif clip:\n\t\ttorch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n\t\ttorch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n\n\t# Update parameters with optimizers\n\tencoder_optimizer.step()\n\tdecoder_optimizer.step()\n\n\treturn return_loss\n\ndef evaluate_attn(input_seq, input_length, num_list, copy_nums, generate_nums, encoder, decoder, output_lang,\n\t\t\t\t  beam_size=1, english=False, max_length=MAX_OUTPUT_LENGTH):\n\tseq_mask = torch.BoolTensor(1, input_length).fill_(0)\n\tnum_start = output_lang.n_words - copy_nums - 2\n\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_seq).unsqueeze(1)\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\tseq_mask = seq_mask.cuda()\n\n\t# Set to not-training mode to disable dropout\n\tencoder.eval()\n\tdecoder.eval()\n\n\t# Run through encoder\n\tencoder_outputs, encoder_hidden = encoder(input_var, [input_length], None)\n\n\t# Create starting vectors for decoder\n\tdecoder_input = torch.LongTensor([output_lang.word2index[\"SOS\"]])  # SOS\n\tdecoder_hidden = encoder_hidden[:decoder.n_layers]  # Use last (forward) hidden state from encoder\n\tbeam_list = list()\n\tscore = 0\n\tbeam_list.append(Beam(score, decoder_input, decoder_hidden, []))\n\n\t# Run through decoder\n\tfor di in range(max_length):\n\t\ttemp_list = list()\n\t\tbeam_len = len(beam_list)\n\t\tfor xb in beam_list:\n\t\t\tif int(xb.input_var[0]) == output_lang.word2index[\"EOS\"]:\n\t\t\t\ttemp_list.append(xb)\n\t\t\t\tbeam_len -= 1\n\t\tif beam_len == 0:\n\t\t\treturn beam_list[0].all_output\n\t\tbeam_scores = torch.zeros(decoder.output_size * beam_len)\n\t\thidden_size_0 = decoder_hidden.size(0)\n\t\thidden_size_2 = decoder_hidden.size(2)\n\t\tall_hidden = torch.zeros(beam_len, hidden_size_0, 1, hidden_size_2)\n\t\tif USE_CUDA:\n\t\t\tbeam_scores = beam_scores.cuda()\n\t\t\tall_hidden = all_hidden.cuda()\n\t\tall_outputs = []\n\t\tcurrent_idx = -1\n\n\t\tfor b_idx in range(len(beam_list)):\n\t\t\tdecoder_input = beam_list[b_idx].input_var\n\t\t\tif int(decoder_input[0]) == output_lang.word2index[\"EOS\"]:\n\t\t\t\tcontinue\n\t\t\tcurrent_idx += 1\n\t\t\tdecoder_hidden = beam_list[b_idx].hidden\n\n\t\t\t# rule_mask = generate_rule_mask(decoder_input, [num_list], output_lang.word2index,\n\t\t\t#                                1, num_start, copy_nums, generate_nums, english)\n\t\t\tif USE_CUDA:\n\t\t\t\t# rule_mask = rule_mask.cuda()\n\t\t\t\tdecoder_input = decoder_input.cuda()\n\n\t\t\tdecoder_output, decoder_hidden = decoder(\n\t\t\t\tdecoder_input, decoder_hidden, encoder_outputs, seq_mask)\n\t\t\t# score = f.log_softmax(decoder_output, dim=1) + rule_mask.squeeze()\n\t\t\tscore = f.log_softmax(decoder_output, dim=1)\n\t\t\tscore += beam_list[b_idx].score\n\t\t\tbeam_scores[current_idx * decoder.output_size: (current_idx + 1) * decoder.output_size] = score\n\t\t\tall_hidden[current_idx] = decoder_hidden\n\t\t\tall_outputs.append(beam_list[b_idx].all_output)\n\t\ttopv, topi = beam_scores.topk(beam_size)\n\n\t\tfor k in range(beam_size):\n\t\t\tword_n = int(topi[k])\n\t\t\tword_input = word_n % decoder.output_size\n\t\t\ttemp_input = torch.LongTensor([word_input])\n\t\t\tindices = int(word_n / decoder.output_size)\n\n\t\t\ttemp_hidden = all_hidden[indices]\n\t\t\ttemp_output = all_outputs[indices]+[word_input]\n\t\t\ttemp_list.append(Beam(float(topv[k]), temp_input, temp_hidden, temp_output))\n\n\t\ttemp_list = sorted(temp_list, key=lambda x: x.score, reverse=True)\n\n\t\tif len(temp_list) < beam_size:\n\t\t\tbeam_list = temp_list\n\t\telse:\n\t\t\tbeam_list = temp_list[:beam_size]\n\treturn beam_list[0].all_output\n\ndef copy_list(l):\n\tr = []\n\tif len(l) == 0:\n\t\treturn r\n\tfor i in l:\n\t\tif type(i) is list:\n\t\t\tr.append(copy_list(i))\n\t\telse:\n\t\t\tr.append(i)\n\treturn r\n\nclass TreeBeam:  # the class save the beam node\n\tdef __init__(self, score, node_stack, embedding_stack, left_childs, out):\n\t\tself.score = score\n\t\tself.embedding_stack = copy_list(embedding_stack)\n\t\tself.node_stack = copy_list(node_stack)\n\t\tself.left_childs = copy_list(left_childs)\n\t\tself.out = copy.deepcopy(out)\n\n\nclass TreeEmbedding:  # the class save the tree\n\tdef __init__(self, embedding, terminal=False):\n\t\tself.embedding = embedding\n\t\tself.terminal = terminal\n\n# def train_tree(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, generate_nums,\n# \t\t\t   encoder, predict, generate, merge, encoder_optimizer, predict_optimizer, generate_optimizer,\n# \t\t\t   merge_optimizer, output_lang, num_pos, english=False):\ndef train_tree(config, input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, generate_nums,\n\t\t\t   embedding, encoder, predict, generate, merge, embedding_optimizer, encoder_optimizer, predict_optimizer, generate_optimizer,\n\t\t\t   merge_optimizer, input_lang, output_lang, num_pos, english=False):\n\n\tnum_mask = []\n\tmax_num_size = max(num_size_batch) + len(generate_nums)\n\tfor i in num_size_batch:\n\t\td = i + len(generate_nums)\n\t\tnum_mask.append([0] * d + [1] * (max_num_size - d))\n\tnum_mask = torch.BoolTensor(num_mask)\n\n\tunk = output_lang.word2index[\"UNK\"]\n\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).transpose(0, 1)\n\n\ttarget = torch.LongTensor(target_batch).transpose(0, 1)\n\n\tpadding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n\tbatch_size = len(input_length)\n\n\tembedding.train()\n\tencoder.train()\n\tpredict.train()\n\tgenerate.train()\n\tmerge.train()\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\t# seq_mask = seq_mask.cuda()\n\t\tpadding_hidden = padding_hidden.cuda()\n\t\tnum_mask = num_mask.cuda()\n\n\t# Zero gradients of both optimizers\n\tembedding_optimizer.zero_grad()\n\tencoder_optimizer.zero_grad()\n\tpredict_optimizer.zero_grad()\n\tgenerate_optimizer.zero_grad()\n\tmerge_optimizer.zero_grad()\n\t# Run words through encoder\n\n\t# pdb.set_trace()\n\tembedded = None\n\torig_idx = None\n\tif config.embedding == 'bert' or config.embedding == 'roberta':\n\t\tcontextual_input = index_batch_to_words(input_batch, input_length, input_lang)\n\t\tinput_seq1, input_len1 = embedding(contextual_input)\n\t\tinput_seq1 = input_seq1.transpose(0,1)\n\t\tembedded, input_length, orig_idx = sort_by_len(input_seq1, input_len1, gpu_init_pytorch(config.gpu))\n\telse:\n\t\tembedded = embedding(input_var)\n\n\tencoder_outputs, problem_output = encoder(embedded, input_length, orig_idx)\n\n\t# encoder_outputs, problem_output = encoder(input_var, input_length)\n\n\t# sequence mask for attention\n\tseq_mask = []\n\tmax_len = max(input_length)\n\tfor i in input_length:\n\t\tseq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n\tseq_mask = torch.BoolTensor(seq_mask)\n\n\tif USE_CUDA:\n\t\tseq_mask = seq_mask.cuda()\n\n\t# Prepare input and output variables\n\tnode_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]\n\n\tmax_target_length = max(target_length)\n\n\tall_node_outputs = []\n\t# all_leafs = []\n\n\tcopy_num_len = [len(_) for _ in num_pos]\n\tnum_size = max(copy_num_len)\n\tall_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  encoder.hidden_size)\n\n\tnum_start = output_lang.num_start\n\tembeddings_stacks = [[] for _ in range(batch_size)]\n\tleft_childs = [None for _ in range(batch_size)]\n\tfor t in range(max_target_length):\n\t\tnum_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n\t\t\tnode_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask)\n\n\t\t# all_leafs.append(p_leaf)\n\t\toutputs = torch.cat((op, num_score), 1)\n\t\tall_node_outputs.append(outputs)\n\n\t\ttarget_t, generate_input = generate_tree_input(target[t].tolist(), outputs, nums_stack_batch, num_start, unk)\n\t\ttarget[t] = target_t\n\t\tif USE_CUDA:\n\t\t\tgenerate_input = generate_input.cuda()\n\t\tleft_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)\n\t\tleft_childs = []\n\t\tfor idx, l, r, node_stack, i, o in zip(range(batch_size), left_child.split(1), right_child.split(1),\n\t\t\t\t\t\t\t\t\t\t\t   node_stacks, target[t].tolist(), embeddings_stacks):\n\t\t\tif len(node_stack) != 0:\n\t\t\t\tnode = node_stack.pop()\n\t\t\telse:\n\t\t\t\tleft_childs.append(None)\n\t\t\t\tcontinue\n\n\t\t\tif i < num_start:\n\t\t\t\tnode_stack.append(TreeNode(r))\n\t\t\t\tnode_stack.append(TreeNode(l, left_flag=True))\n\t\t\t\to.append(TreeEmbedding(node_label[idx].unsqueeze(0), False))\n\t\t\telse:\n\t\t\t\tcurrent_num = current_nums_embeddings[idx, i - num_start].unsqueeze(0)\n\t\t\t\twhile len(o) > 0 and o[-1].terminal:\n\t\t\t\t\tsub_stree = o.pop()\n\t\t\t\t\top = o.pop()\n\t\t\t\t\tcurrent_num = merge(op.embedding, sub_stree.embedding, current_num)\n\t\t\t\to.append(TreeEmbedding(current_num, True))\n\t\t\tif len(o) > 0 and o[-1].terminal:\n\t\t\t\tleft_childs.append(o[-1].embedding)\n\t\t\telse:\n\t\t\t\tleft_childs.append(None)\n\n\t# all_leafs = torch.stack(all_leafs, dim=1)  # B x S x 2\n\tall_node_outputs = torch.stack(all_node_outputs, dim=1)  # B x S x N\n\n\ttarget = target.transpose(0, 1).contiguous()\n\tif USE_CUDA:\n\t\t# all_leafs = all_leafs.cuda()\n\t\tall_node_outputs = all_node_outputs.cuda()\n\t\ttarget = target.cuda()\n\n\t# op_target = target < num_start\n\t# loss_0 = masked_cross_entropy_without_logit(all_leafs, op_target.long(), target_length)\n\tloss = masked_cross_entropy(all_node_outputs, target, target_length)\n\t# loss = loss_0 + loss_1\n\tloss.backward()\n\t# clip the grad\n\t# torch.nn.utils.clip_grad_norm_(encoder.parameters(), 5)\n\t# torch.nn.utils.clip_grad_norm_(predict.parameters(), 5)\n\t# torch.nn.utils.clip_grad_norm_(generate.parameters(), 5)\n\n\t# Update parameters with optimizers\n\tembedding_optimizer.step()\n\tencoder_optimizer.step()\n\tpredict_optimizer.step()\n\tgenerate_optimizer.step()\n\tmerge_optimizer.step()\n\treturn loss.item()  # , loss_0.item(), loss_1.item()\n\n# def evaluate_tree(input_batch, input_length, generate_nums, encoder, predict, generate, merge, output_lang, num_pos,\n# \t\t\t\t  beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):\ndef evaluate_tree(config, input_batch, input_length, generate_nums, embedding, encoder, predict, generate, merge, input_lang, output_lang, num_pos,\n\t\t\t\t  beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):\n\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).unsqueeze(1)\n\n\tnum_mask = torch.BoolTensor(1, len(num_pos) + len(generate_nums)).fill_(0)\n\n\t# Set to not-training mode to disable dropout\n\tembedding.eval()\n\tencoder.eval()\n\tpredict.eval()\n\tgenerate.eval()\n\tmerge.eval()\n\n\tpadding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n\n\tbatch_size = 1\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\t# seq_mask = seq_mask.cuda()\n\t\tpadding_hidden = padding_hidden.cuda()\n\t\tnum_mask = num_mask.cuda()\n\t# Run words through encoder\n\n\t# pdb.set_trace()\n\n\tembedded = None\n\torig_idx = None\n\tif config.embedding == 'bert' or config.embedding == 'roberta':\n\t\tcontextual_input = index_batch_to_words([input_batch], [input_length], input_lang)\n\t\tinput_seq1, input_len1 = embedding(contextual_input)\n\t\tinput_seq1 = input_seq1.transpose(0,1)\n\t\tembedded, input_length, orig_idx = sort_by_len(input_seq1, input_len1, gpu_init_pytorch(config.gpu))\n\t\tinput_length = input_length[0]\n\telse:\n\t\tembedded = embedding(input_var)\n\n\tencoder_outputs, problem_output = encoder(embedded, [input_length], orig_idx)\n\t# encoder_outputs, problem_output = encoder(input_var, [input_length])\n\n\tseq_mask = torch.BoolTensor(1, input_length).fill_(0)\n\n\tif USE_CUDA:\n\t\tseq_mask = seq_mask.cuda()\n\n\t# Prepare input and output variables\n\tnode_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]\n\n\tnum_size = len(num_pos)\n\tall_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  encoder.hidden_size)\n\tnum_start = output_lang.num_start\n\t# B x P x N\n\tembeddings_stacks = [[] for _ in range(batch_size)]\n\tleft_childs = [None for _ in range(batch_size)]\n\n\tbeams = [TreeBeam(0.0, node_stacks, embeddings_stacks, left_childs, [])]\n\n\tfor t in range(max_length):\n\t\tcurrent_beams = []\n\t\twhile len(beams) > 0:\n\t\t\tb = beams.pop()\n\t\t\tif len(b.node_stack[0]) == 0:\n\t\t\t\tcurrent_beams.append(b)\n\t\t\t\tcontinue\n\t\t\t# left_childs = torch.stack(b.left_childs)\n\t\t\tleft_childs = b.left_childs\n\n\t\t\tnum_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n\t\t\t\tb.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden,\n\t\t\t\tseq_mask, num_mask)\n\n\t\t\t# leaf = p_leaf[:, 0].unsqueeze(1)\n\t\t\t# repeat_dims = [1] * leaf.dim()\n\t\t\t# repeat_dims[1] = op.size(1)\n\t\t\t# leaf = leaf.repeat(*repeat_dims)\n\t\t\t#\n\t\t\t# non_leaf = p_leaf[:, 1].unsqueeze(1)\n\t\t\t# repeat_dims = [1] * non_leaf.dim()\n\t\t\t# repeat_dims[1] = num_score.size(1)\n\t\t\t# non_leaf = non_leaf.repeat(*repeat_dims)\n\t\t\t#\n\t\t\t# p_leaf = torch.cat((leaf, non_leaf), dim=1)\n\t\t\tout_score = nn.functional.log_softmax(torch.cat((op, num_score), dim=1), dim=1)\n\n\t\t\t# out_score = p_leaf * out_score\n\n\t\t\ttopv, topi = out_score.topk(min(beam_size, out_score.size()[1]))\n\n\t\t\t# is_leaf = int(topi[0])\n\t\t\t# if is_leaf:\n\t\t\t#     topv, topi = op.topk(1)\n\t\t\t#     out_token = int(topi[0])\n\t\t\t# else:\n\t\t\t#     topv, topi = num_score.topk(1)\n\t\t\t#     out_token = int(topi[0]) + num_start\n\n\t\t\tfor tv, ti in zip(topv.split(1, dim=1), topi.split(1, dim=1)):\n\t\t\t\tcurrent_node_stack = copy_list(b.node_stack)\n\t\t\t\tcurrent_left_childs = []\n\t\t\t\tcurrent_embeddings_stacks = copy_list(b.embedding_stack)\n\t\t\t\tcurrent_out = copy.deepcopy(b.out)\n\n\t\t\t\tout_token = int(ti)\n\t\t\t\tcurrent_out.append(out_token)\n\n\t\t\t\tnode = current_node_stack[0].pop()\n\n\t\t\t\tif out_token < num_start:\n\t\t\t\t\tgenerate_input = torch.LongTensor([out_token])\n\t\t\t\t\tif USE_CUDA:\n\t\t\t\t\t\tgenerate_input = generate_input.cuda()\n\t\t\t\t\tleft_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)\n\n\t\t\t\t\tcurrent_node_stack[0].append(TreeNode(right_child))\n\t\t\t\t\tcurrent_node_stack[0].append(TreeNode(left_child, left_flag=True))\n\n\t\t\t\t\tcurrent_embeddings_stacks[0].append(TreeEmbedding(node_label[0].unsqueeze(0), False))\n\t\t\t\telse:\n\t\t\t\t\tcurrent_num = current_nums_embeddings[0, out_token - num_start].unsqueeze(0)\n\n\t\t\t\t\twhile len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:\n\t\t\t\t\t\tsub_stree = current_embeddings_stacks[0].pop()\n\t\t\t\t\t\top = current_embeddings_stacks[0].pop()\n\t\t\t\t\t\tcurrent_num = merge(op.embedding, sub_stree.embedding, current_num)\n\t\t\t\t\tcurrent_embeddings_stacks[0].append(TreeEmbedding(current_num, True))\n\t\t\t\tif len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:\n\t\t\t\t\tcurrent_left_childs.append(current_embeddings_stacks[0][-1].embedding)\n\t\t\t\telse:\n\t\t\t\t\tcurrent_left_childs.append(None)\n\t\t\t\tcurrent_beams.append(TreeBeam(b.score+float(tv), current_node_stack, current_embeddings_stacks,\n\t\t\t\t\t\t\t\t\t\t\t  current_left_childs, current_out))\n\t\tbeams = sorted(current_beams, key=lambda x: x.score, reverse=True)\n\t\tbeams = beams[:beam_size]\n\t\tflag = True\n\t\tfor b in beams:\n\t\t\tif len(b.node_stack[0]) != 0:\n\t\t\t\tflag = False\n\t\tif flag:\n\t\t\tbreak\n\n\treturn beams[0].out\n\ndef topdown_train_tree(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch,\n\t\t\t\t\t   generate_nums, encoder, predict, generate, encoder_optimizer, predict_optimizer,\n\t\t\t\t\t   generate_optimizer, output_lang, num_pos, english=False):\n\t# sequence mask for attention\n\tseq_mask = []\n\tmax_len = max(input_length)\n\tfor i in input_length:\n\t\tseq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n\tseq_mask = torch.BoolTensor(seq_mask)\n\n\tnum_mask = []\n\tmax_num_size = max(num_size_batch) + len(generate_nums)\n\tfor i in num_size_batch:\n\t\td = i + len(generate_nums)\n\t\tnum_mask.append([0] * d + [1] * (max_num_size - d))\n\tnum_mask = torch.BoolTensor(num_mask)\n\n\tunk = output_lang.word2index[\"UNK\"]\n\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).transpose(0, 1)\n\n\ttarget = torch.LongTensor(target_batch).transpose(0, 1)\n\n\tpadding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n\tbatch_size = len(input_length)\n\n\tencoder.train()\n\tpredict.train()\n\tgenerate.train()\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\tseq_mask = seq_mask.cuda()\n\t\tpadding_hidden = padding_hidden.cuda()\n\t\tnum_mask = num_mask.cuda()\n\n\t# Zero gradients of both optimizers\n\tencoder_optimizer.zero_grad()\n\tpredict_optimizer.zero_grad()\n\tgenerate_optimizer.zero_grad()\n\t# Run words through encoder\n\n\tencoder_outputs, problem_output = encoder(input_var, input_length)\n\t# Prepare input and output variables\n\tnode_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]\n\n\tmax_target_length = max(target_length)\n\n\tall_node_outputs = []\n\t# all_leafs = []\n\n\tcopy_num_len = [len(_) for _ in num_pos]\n\tnum_size = max(copy_num_len)\n\tall_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  encoder.hidden_size)\n\n\tnum_start = output_lang.num_start\n\tleft_childs = [None for _ in range(batch_size)]\n\tfor t in range(max_target_length):\n\t\tnum_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n\t\t\tnode_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask)\n\n\t\t# all_leafs.append(p_leaf)\n\t\toutputs = torch.cat((op, num_score), 1)\n\t\tall_node_outputs.append(outputs)\n\n\t\ttarget_t, generate_input = generate_tree_input(target[t].tolist(), outputs, nums_stack_batch, num_start, unk)\n\t\ttarget[t] = target_t\n\t\tif USE_CUDA:\n\t\t\tgenerate_input = generate_input.cuda()\n\t\tleft_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)\n\t\tfor idx, l, r, node_stack, i in zip(range(batch_size), left_child.split(1), right_child.split(1),\n\t\t\t\t\t\t\t\t\t\t\tnode_stacks, target[t].tolist()):\n\t\t\tif len(node_stack) != 0:\n\t\t\t\tnode = node_stack.pop()\n\t\t\telse:\n\t\t\t\tcontinue\n\n\t\t\tif i < num_start:\n\t\t\t\tnode_stack.append(TreeNode(r))\n\t\t\t\tnode_stack.append(TreeNode(l, left_flag=True))\n\n\t# all_leafs = torch.stack(all_leafs, dim=1)  # B x S x 2\n\tall_node_outputs = torch.stack(all_node_outputs, dim=1)  # B x S x N\n\n\ttarget = target.transpose(0, 1).contiguous()\n\tif USE_CUDA:\n\t\t# all_leafs = all_leafs.cuda()\n\t\tall_node_outputs = all_node_outputs.cuda()\n\t\ttarget = target.cuda()\n\n\t# op_target = target < num_start\n\t# loss_0 = masked_cross_entropy_without_logit(all_leafs, op_target.long(), target_length)\n\tloss = masked_cross_entropy(all_node_outputs, target, target_length)\n\t# loss = loss_0 + loss_1\n\tloss.backward()\n\t# clip the grad\n\t# torch.nn.utils.clip_grad_norm_(encoder.parameters(), 5)\n\t# torch.nn.utils.clip_grad_norm_(predict.parameters(), 5)\n\t# torch.nn.utils.clip_grad_norm_(generate.parameters(), 5)\n\n\t# Update parameters with optimizers\n\tencoder_optimizer.step()\n\tpredict_optimizer.step()\n\tgenerate_optimizer.step()\n\treturn loss.item()  # , loss_0.item(), loss_1.item()\n\ndef topdown_evaluate_tree(input_batch, input_length, generate_nums, encoder, predict, generate, output_lang, num_pos,\n\t\t\t\t\t\t  beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):\n\n\tseq_mask = torch.BoolTensor(1, input_length).fill_(0)\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).unsqueeze(1)\n\n\tnum_mask = torch.BoolTensor(1, len(num_pos) + len(generate_nums)).fill_(0)\n\n\t# Set to not-training mode to disable dropout\n\tencoder.eval()\n\tpredict.eval()\n\tgenerate.eval()\n\n\tpadding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n\n\tbatch_size = 1\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\tseq_mask = seq_mask.cuda()\n\t\tpadding_hidden = padding_hidden.cuda()\n\t\tnum_mask = num_mask.cuda()\n\t# Run words through encoder\n\n\tencoder_outputs, problem_output = encoder(input_var, [input_length])\n\n\t# Prepare input and output variables\n\tnode_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]\n\n\tnum_size = len(num_pos)\n\tall_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  encoder.hidden_size)\n\tnum_start = output_lang.num_start\n\t# B x P x N\n\tembeddings_stacks = [[] for _ in range(batch_size)]\n\tleft_childs = [None for _ in range(batch_size)]\n\n\tbeams = [TreeBeam(0.0, node_stacks, embeddings_stacks, left_childs, [])]\n\n\tfor t in range(max_length):\n\t\tcurrent_beams = []\n\t\twhile len(beams) > 0:\n\t\t\tb = beams.pop()\n\t\t\tif len(b.node_stack[0]) == 0:\n\t\t\t\tcurrent_beams.append(b)\n\t\t\t\tcontinue\n\t\t\t# left_childs = torch.stack(b.left_childs)\n\n\t\t\tnum_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n\t\t\t\tb.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden,\n\t\t\t\tseq_mask, num_mask)\n\n\t\t\t# leaf = p_leaf[:, 0].unsqueeze(1)\n\t\t\t# repeat_dims = [1] * leaf.dim()\n\t\t\t# repeat_dims[1] = op.size(1)\n\t\t\t# leaf = leaf.repeat(*repeat_dims)\n\t\t\t#\n\t\t\t# non_leaf = p_leaf[:, 1].unsqueeze(1)\n\t\t\t# repeat_dims = [1] * non_leaf.dim()\n\t\t\t# repeat_dims[1] = num_score.size(1)\n\t\t\t# non_leaf = non_leaf.repeat(*repeat_dims)\n\t\t\t#\n\t\t\t# p_leaf = torch.cat((leaf, non_leaf), dim=1)\n\t\t\tout_score = nn.functional.log_softmax(torch.cat((op, num_score), dim=1), dim=1)\n\n\t\t\t# out_score = p_leaf * out_score\n\n\t\t\ttopv, topi = out_score.topk(beam_size)\n\n\t\t\t# is_leaf = int(topi[0])\n\t\t\t# if is_leaf:\n\t\t\t#     topv, topi = op.topk(1)\n\t\t\t#     out_token = int(topi[0])\n\t\t\t# else:\n\t\t\t#     topv, topi = num_score.topk(1)\n\t\t\t#     out_token = int(topi[0]) + num_start\n\n\t\t\tfor tv, ti in zip(topv.split(1, dim=1), topi.split(1, dim=1)):\n\t\t\t\tcurrent_node_stack = copy_list(b.node_stack)\n\t\t\t\tcurrent_out = copy.deepcopy(b.out)\n\n\t\t\t\tout_token = int(ti)\n\t\t\t\tcurrent_out.append(out_token)\n\n\t\t\t\tnode = current_node_stack[0].pop()\n\n\t\t\t\tif out_token < num_start:\n\t\t\t\t\tgenerate_input = torch.LongTensor([out_token])\n\t\t\t\t\tif USE_CUDA:\n\t\t\t\t\t\tgenerate_input = generate_input.cuda()\n\t\t\t\t\tleft_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)\n\n\t\t\t\t\tcurrent_node_stack[0].append(TreeNode(right_child))\n\t\t\t\t\tcurrent_node_stack[0].append(TreeNode(left_child, left_flag=True))\n\n\t\t\t\tcurrent_beams.append(TreeBeam(b.score+float(tv), current_node_stack, embeddings_stacks, left_childs,\n\t\t\t\t\t\t\t\t\t\t\t  current_out))\n\t\tbeams = sorted(current_beams, key=lambda x: x.score, reverse=True)\n\t\tbeams = beams[:beam_size]\n\t\tflag = True\n\t\tfor b in beams:\n\t\t\tif len(b.node_stack[0]) != 0:\n\t\t\t\tflag = False\n\t\tif flag:\n\t\t\tbreak\n\n\treturn beams[0].out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:40:04.957254Z","iopub.execute_input":"2024-11-15T11:40:04.957571Z","iopub.status.idle":"2024-11-15T11:40:05.142409Z","shell.execute_reply.started":"2024-11-15T11:40:04.957539Z","shell.execute_reply":"2024-11-15T11:40:05.141600Z"}},"outputs":[],"execution_count":116},{"cell_type":"markdown","source":"## main.py","metadata":{}},{"cell_type":"code","source":"global log_folder\nglobal model_folder\nglobal result_folder\nglobal data_path\nglobal board_path\n\nlog_folder = 'logs'\nmodel_folder = 'models'\noutputs_folder = 'outputs'\nresult_folder = './out/'\ndata_path = '/kaggle/input/svamp-data/data/'\nboard_path = './runs/'\n\n'''read arguments'''\n\nkaggle_args = {\n    'debug': False,\n    'mode': 'train',\n    'gpu': 0,\n    'dropout': 0.1,\n    'heads': 4,\n    'encoder_layers': 1,\n    'decoder_layers': 1,\n    'd_model': 768,\n    'd_ff': 256,\n    'lr': 0.0001,\n    'emb_lr': 1e-5,\n    'batch_size': 16,\n    'epochs': 10,\n    'embedding': 'roberta',\n    'emb_name': 'roberta-base',\n    'mawps_vocab': True,\n    'dataset': 'mawps-asdiv-a_svamp',\n    'run_name': 'mawps_try1',\n    'logging': 0\n}\n\n\n\n\nconfig =  parse_arguments(kaggle_args)\n\nmode = config.mode\n\nif mode == 'train':\n    is_train = True\nelse:\n    is_train = False\n\n''' Set seed for reproducibility'''\nnp.random.seed(config.seed)\ntorch.manual_seed(config.seed)\nrandom.seed(config.seed)\n\n'''GPU initialization'''\ndevice = gpu_init_pytorch(config.gpu)\n\nif config.full_cv:\n    global data_path\n    data_name = config.dataset\n    data_path = data_path + data_name + '/'\n    config.val_result_path = os.path.join(result_folder, 'CV_results_{}.json'.format(data_name))\n    fold_acc_score = 0.0\n    folds_scores = []\n    best_acc = []\n    for z in range(5):\n        run_name = config.run_name + '_fold' + str(z)\n        config.dataset = 'fold' + str(z)\n        config.log_path = os.path.join(log_folder, run_name)\n        config.model_path = os.path.join(model_folder, run_name)\n        config.board_path = os.path.join(board_path, run_name)\n        config.outputs_path = os.path.join(outputs_folder, run_name)\n\n        vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n        vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n        config_file = os.path.join(config.model_path, 'config.p')\n        log_file = os.path.join(config.log_path, 'log.txt')\n\n        if config.results:\n            config.result_path = os.path.join(result_folder, 'val_results_{}.json'.format(config.dataset))\n\n        create_save_directories(config.log_path)\n        create_save_directories(config.model_path)\n        create_save_directories(config.outputs_path)\n\n        logger = get_logger(run_name, log_file, logging.DEBUG)\n\n        logger.info('Experiment Name: {}'.format(config.run_name))\n        logger.debug('Created Relevant Directories')\n\n        logger.info('Loading Data...')\n\n        train_ls, dev_ls = load_raw_data(data_path, config.dataset)\n        pairs_trained, pairs_tested, generate_nums, copy_nums = transfer_num(train_ls, dev_ls, config.challenge_disp)\n\n        logger.debug('Data Loaded...')\n        logger.debug('Number of Training Examples: {}'.format(len(pairs_trained)))\n        logger.debug('Number of Testing Examples: {}'.format(len(pairs_tested)))\n        logger.debug('Extra Numbers: {}'.format(generate_nums))\n        logger.debug('Maximum Number of Numbers: {}'.format(copy_nums))\n\n        # pairs: ([list of words in question], [list of infix Equation tokens incl brackets and N0, N1], [list of numbers], [list of indexes of numbers])\n        # generate_nums: Unmentioned numbers used in eqns in atleast 5 examples ['1', '3.14']\n        # copy_nums: Maximum number of numbers in a single sentence: 15\n\n        # pairs: ([list of words in question], [list of prefix Equation tokens w/ metasymbols as N0, N1], [list of numbers], [list of indexes of numbers])\n\n        logger.info('Creating Vocab...')\n        input_lang = None\n        output_lang = None\n\n        input_lang, output_lang, train_pairs, test_pairs = prepare_data(config, logger, pairs_trained, pairs_tested, config.trim_threshold, generate_nums, copy_nums, input_lang, output_lang, tree=True)\n\n        checkpoint = get_latest_checkpoint(config.model_path, logger)\n\n        with open(vocab1_path, 'wb') as f:\n            pickle.dump(input_lang, f, protocol=pickle.HIGHEST_PROTOCOL)\n        with open(vocab2_path, 'wb') as f:\n            pickle.dump(output_lang, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Vocab saved at {}'.format(vocab1_path))\n\n        config.len_generate_nums = len(generate_nums)\n        config.copy_nums = copy_nums\n\n        with open(config_file, 'wb') as f:\n            pickle.dump(vars(config), f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Config File Saved')\n\n        # train_pairs: ([list of token ids of question], len(ques), [list of token ids of equation], len(equation), [list of numbers], [list of indexes of numbers], [number stack])\n\n        logger.info('Initializing Models...')\n\n        # Initialize models\n        embedding = None\n        if config.embedding == 'bert':\n            embedding = BertEncoder(config.emb_name, device, config.freeze_emb)\n        elif config.embedding == 'roberta':\n            embedding = RobertaEncoder(config.emb_name, device, config.freeze_emb)\n        else:\n            embedding = Embedding(config, input_lang, input_size=input_lang.n_words, embedding_size=config.embedding_size, dropout=config.dropout)\n\n        # encoder = EncoderSeq(input_size=input_lang.n_words, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        encoder = EncoderSeq(cell_type=config.cell_type, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        predict = Prediction(hidden_size=config.hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums), input_size=len(generate_nums), dropout=config.dropout)\n        generate = GenerateNode(hidden_size=config.hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums), embedding_size=config.embedding_size, dropout=config.dropout)\n        merge = Merge(hidden_size=config.hidden_size, embedding_size=config.embedding_size, dropout=config.dropout)\n        # the embedding layer is only for generated number embeddings, operators, and paddings\n\n        logger.debug('Models Initialized')\n        logger.info('Initializing Optimizers...')\n\n        embedding_optimizer = torch.optim.Adam(embedding.parameters(), lr=config.emb_lr, weight_decay=config.weight_decay)\n        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        predict_optimizer = torch.optim.Adam(predict.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        generate_optimizer = torch.optim.Adam(generate.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        merge_optimizer = torch.optim.Adam(merge.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n\n        logger.debug('Optimizers Initialized')\n        logger.info('Initializing Schedulers...')\n\n        embedding_scheduler = torch.optim.lr_scheduler.StepLR(embedding_optimizer, step_size=20, gamma=0.5)\n        encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n        predict_scheduler = torch.optim.lr_scheduler.StepLR(predict_optimizer, step_size=20, gamma=0.5)\n        generate_scheduler = torch.optim.lr_scheduler.StepLR(generate_optimizer, step_size=20, gamma=0.5)\n        merge_scheduler = torch.optim.lr_scheduler.StepLR(merge_optimizer, step_size=20, gamma=0.5)\n\n        logger.debug('Schedulers Initialized')\n\n        logger.info('Loading Models on GPU {}...'.format(config.gpu))\n\n        # Move models to GPU\n        if USE_CUDA:\n            embedding.to(device)\n            encoder.to(device)\n            predict.to(device)\n            generate.to(device)\n            merge.to(device)\n\n        logger.debug('Models loaded on GPU {}'.format(config.gpu))\n\n        generate_num_ids = []\n        for num in generate_nums:\n            generate_num_ids.append(output_lang.word2index[num])\n\n        max_value_corr = 0\n        len_total_eval = 0\n        max_val_acc = 0.0\n        max_train_acc = 0.0\n        eq_acc = 0.0\n        best_epoch = -1\n        min_train_loss = float('inf')\n\n        logger.info('Starting Training Procedure')\n\n        for epoch in range(config.epochs):\n            loss_total = 0\n            input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches = prepare_train_batch(train_pairs, config.batch_size)\n\n            od = OrderedDict()\n            od['Epoch'] = epoch + 1\n            print_log(logger, od)\n\n            start = time.time()\n            for idx in range(len(input_lengths)):\n                # loss = train_tree(\n                # \tinput_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n                # \tnum_stack_batches[idx], num_size_batches[idx], generate_num_ids, encoder, predict, generate, merge,\n                # \tencoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx])\n                loss = train_tree(\n                    config, input_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n                    num_stack_batches[idx], num_size_batches[idx], generate_num_ids, embedding, encoder, predict, generate, merge,\n                    embedding_optimizer, encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, input_lang, output_lang, \n                    num_pos_batches[idx])\n                loss_total += loss\n                print(\"Completed {} / {}...\".format(idx, len(input_lengths)), end = '\\r', flush = True)\n\n            embedding_scheduler.step()\n            encoder_scheduler.step()\n            predict_scheduler.step()\n            generate_scheduler.step()\n            merge_scheduler.step()\n\n            logger.debug('Training for epoch {} completed...\\nTime Taken: {}'.format(epoch, time_since(time.time() - start)))\n\n            if loss_total / len(input_lengths) < min_train_loss:\n                min_train_loss = loss_total / len(input_lengths)\n\n            train_value_ac = 0\n            train_equation_ac = 0\n            train_eval_total = 1\n            if config.show_train_acc:\n                train_eval_total = 0\n                logger.info('Computing Train Accuracy')\n                start = time.time()\n                with torch.no_grad():\n                    for train_batch in train_pairs:\n                        # train_res = evaluate_tree(train_batch[0], train_batch[1], generate_num_ids, encoder, predict, generate,\n                        # \t\t\t\t\t\t merge, output_lang, train_batch[5], beam_size=config.beam_size)\n                        train_res = evaluate_tree(config, train_batch[0], train_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                                 merge, input_lang, output_lang, train_batch[5], beam_size=config.beam_size)\n                        train_val_ac, train_equ_ac, _, _ = compute_prefix_tree_result(train_res, train_batch[2], output_lang, train_batch[4], train_batch[6])\n\n                        if train_val_ac:\n                            train_value_ac += 1\n                        if train_equ_ac:\n                            train_equation_ac += 1\n                        train_eval_total += 1\n\n                logger.debug('Train Accuracy Computed...\\nTime Taken: {}'.format(time_since(time.time() - start)))\n\n            logger.info('Starting Validation')\n\n            value_ac = 0\n            equation_ac = 0\n            eval_total = 0\n            start = time.time()\n\n            with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                f_out.write('---------------------------------------\\n')\n                f_out.write('Epoch: ' + str(epoch) + '\\n')\n                f_out.write('---------------------------------------\\n')\n                f_out.close()\n\n            ex_num = 0\n            for test_batch in test_pairs:\n                # test_res = evaluate_tree(test_batch[0], test_batch[1], generate_num_ids, encoder, predict, generate,\n                # \t\t\t\t\t\t merge, output_lang, test_batch[5], beam_size=config.beam_size)\n                test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                         merge, input_lang, output_lang, test_batch[5], beam_size=config.beam_size)\n                val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n                cur_result = 0\n                if val_ac:\n                    value_ac += 1\n                    cur_result = 1\n                if equ_ac:\n                    equation_ac += 1\n                eval_total += 1\n\n                with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                    f_out.write('Example: ' + str(ex_num) + '\\n')\n                    f_out.write('Source: ' + stack_to_string(sentence_from_indexes(input_lang, test_batch[0])) + '\\n')\n                    f_out.write('Target: ' + stack_to_string(sentence_from_indexes(output_lang, test_batch[2])) + '\\n')\n                    f_out.write('Generated: ' + stack_to_string(sentence_from_indexes(output_lang, test_res)) + '\\n')\n                    if config.nums_disp:\n                        src_nums = len(test_batch[4])\n                        tgt_nums = 0\n                        pred_nums = 0\n                        for k_tgt in sentence_from_indexes(output_lang, test_batch[2]):\n                            if k_tgt not in ['+', '-', '*', '/']:\n                                tgt_nums += 1\n                        for k_pred in sentence_from_indexes(output_lang, test_res):\n                            if k_pred not in ['+', '-', '*', '/']:\n                                pred_nums += 1\n                        f_out.write('Numbers in question: ' + str(src_nums) + '\\n')\n                        f_out.write('Numbers in Target Equation: ' + str(tgt_nums) + '\\n')\n                        f_out.write('Numbers in Predicted Equation: ' + str(pred_nums) + '\\n')\n                    f_out.write('Result: ' + str(cur_result) + '\\n' + '\\n')\n                    f_out.close()\n\n                ex_num+=1\n\n            if float(train_value_ac) / train_eval_total > max_train_acc:\n                max_train_acc = float(train_value_ac) / train_eval_total\n\n            if float(value_ac) / eval_total > max_val_acc:\n                max_value_corr = value_ac\n                len_total_eval = eval_total\n                max_val_acc = float(value_ac) / eval_total\n                eq_acc = float(equation_ac) / eval_total\n                best_epoch = epoch+1\n\n                state = {\n                        'epoch' : epoch,\n                        'best_epoch': best_epoch-1,\n                        'embedding_state_dict': embedding.state_dict(),\n                        'encoder_state_dict': encoder.state_dict(),\n                        'predict_state_dict': predict.state_dict(),\n                        'generate_state_dict': generate.state_dict(),\n                        'merge_state_dict': merge.state_dict(),\n                        'embedding_optimizer_state_dict': embedding_optimizer.state_dict(),\n                        'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n                        'predict_optimizer_state_dict': predict_optimizer.state_dict(),\n                        'generate_optimizer_state_dict': generate_optimizer.state_dict(),\n                        'merge_optimizer_state_dict': merge_optimizer.state_dict(),\n                        'embedding_scheduler_state_dict': embedding_scheduler.state_dict(),\n                        'encoder_scheduler_state_dict': encoder_scheduler.state_dict(),\n                        'predict_scheduler_state_dict': predict_scheduler.state_dict(),\n                        'generate_scheduler_state_dict': generate_scheduler.state_dict(),\n                        'merge_scheduler_state_dict': merge_scheduler.state_dict(),\n                        'voc1': input_lang,\n                        'voc2': output_lang,\n                        'train_loss_epoch' : loss_total / len(input_lengths),\n                        'min_train_loss' : min_train_loss,\n                        'val_acc_epoch' : float(value_ac) / eval_total,\n                        'max_val_acc' : max_val_acc,\n                        'equation_acc' : eq_acc,\n                        'max_train_acc' : max_train_acc,\n                        'generate_nums' : generate_nums\n                    }\n\n                if config.save_model:\n                    save_checkpoint(state, epoch, logger, config.model_path, config.ckpt)\n\n            od = OrderedDict()\n            od['Epoch'] = epoch + 1\n            od['best_epoch'] = best_epoch\n            od['train_loss_epoch'] = loss_total / len(input_lengths)\n            od['min_train_loss'] = min_train_loss\n            od['train_acc_epoch'] = float(train_value_ac) / train_eval_total\n            od['max_train_acc'] = max_train_acc\n            od['val_acc_epoch'] = float(value_ac) / eval_total\n            od['equation_acc_epoch'] = float(equation_ac) / eval_total\n            od['max_val_acc'] = max_val_acc\n            od['equation_acc'] = eq_acc\n            print_log(logger, od)\n\n            logger.debug('Validation Completed...\\nTime Taken: {}'.format(time_since(time.time() - start)))\n\n        if config.results:\n            store_results(config, max_train_acc, max_val_acc, eq_acc, min_train_loss, best_epoch)\n            logger.info('Scores saved at {}'.format(config.result_path))\n\n        best_acc.append((max_value_corr, len_total_eval))\n\n    total_value_corr = 0\n    total_len = 0\n    for w in range(len(best_acc)):\n        folds_scores.append(float(best_acc[w][0])/best_acc[w][1])\n        total_value_corr += best_acc[w][0]\n        total_len += best_acc[w][1]\n    fold_acc_score = float(total_value_corr)/total_len\n\n    store_val_results(config, fold_acc_score, folds_scores)\n    logger.info('Final Val score: {}'.format(fold_acc_score))\n\n\nelse:\n    run_name = config.run_name\n    config.log_path = os.path.join(log_folder, run_name)\n    config.model_path = os.path.join(model_folder, run_name)\n    config.board_path = os.path.join(board_path, run_name)\n    config.outputs_path = os.path.join(outputs_folder, run_name)\n\n    vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n    vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n    config_file = os.path.join(config.model_path, 'config.p')\n    log_file = os.path.join(config.log_path, 'log.txt')\n\n    if config.results:\n        config.result_path = os.path.join(result_folder, 'val_results_{}.json'.format(config.dataset))\n\n    if is_train:\n        create_save_directories(config.log_path)\n        create_save_directories(config.model_path)\n        create_save_directories(config.outputs_path)\n    else:\n        create_save_directories(config.log_path)\n        create_save_directories(config.result_path)\n\n    logger = get_logger(run_name, log_file, logging.DEBUG)\n\n    logger.info('Experiment Name: {}'.format(config.run_name))\n    logger.debug('Created Relevant Directories')\n\n    logger.info('Loading Data...')\n\n    train_ls, dev_ls = load_raw_data(data_path, config.dataset, is_train)\n    pairs_trained, pairs_tested, generate_nums, copy_nums = transfer_num(train_ls, dev_ls, config.challenge_disp)\n\n    logger.debug('Data Loaded...')\n    if is_train:\n        logger.debug('Number of Training Examples: {}'.format(len(pairs_trained)))\n    logger.debug('Number of Testing Examples: {}'.format(len(pairs_tested)))\n    logger.debug('Extra Numbers: {}'.format(generate_nums))\n    logger.debug('Maximum Number of Numbers: {}'.format(copy_nums))\n\n    # pairs: ([list of words in question], [list of infix Equation tokens incl brackets and N0, N1], [list of numbers], [list of indexes of numbers])\n    # generate_nums: Unmentioned numbers used in eqns in atleast 5 examples ['1', '3.14']\n    # copy_nums: Maximum number of numbers in a single sentence: 15\n\n    # pairs: ([list of words in question], [list of prefix Equation tokens w/ metasymbols as N0, N1], [list of numbers], [list of indexes of numbers])\n\n    if is_train:\n        logger.info('Creating Vocab...')\n        input_lang = None\n        output_lang = None\n    else:\n        logger.info('Loading Vocab File...')\n\n        with open(vocab1_path, 'rb') as f:\n            input_lang = pickle.load(f)\n        with open(vocab2_path, 'rb') as f:\n            output_lang = pickle.load(f)\n\n        logger.info('Vocab Files loaded from {}\\nNumber of Words: {}'.format(vocab1_path, input_lang.n_words))\n\n    input_lang, output_lang, train_pairs, test_pairs = prepare_data(config, logger, pairs_trained, pairs_tested, config.trim_threshold, generate_nums, copy_nums, input_lang, output_lang, tree=True)\n\n    checkpoint = get_latest_checkpoint(config.model_path, logger)\n\n    if is_train:\n        with open(vocab1_path, 'wb') as f:\n            pickle.dump(input_lang, f, protocol=pickle.HIGHEST_PROTOCOL)\n        with open(vocab2_path, 'wb') as f:\n            pickle.dump(output_lang, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Vocab saved at {}'.format(vocab1_path))\n\n        generate_num_ids = []\n        for num in generate_nums:\n            generate_num_ids.append(output_lang.word2index[num])\n\n        config.len_generate_nums = len(generate_nums)\n        config.copy_nums = copy_nums\n\n        with open(config_file, 'wb') as f:\n            pickle.dump(vars(config), f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Config File Saved')\n\n        # train_pairs: ([list of token ids of question], len(ques), [list of token ids of equation], len(equation), [list of numbers], [list of indexes of numbers], [number stack])\n\n        logger.info('Initializing Models...')\n\n        # Initialize models\n        embedding = None\n        if config.embedding == 'bert':\n            embedding = BertEncoder(config.emb_name, device, config.freeze_emb)\n        elif config.embedding == 'roberta':\n            embedding = RobertaEncoder(config.emb_name, device, config.freeze_emb)\n        else:\n            embedding = Embedding(config, input_lang, input_size=input_lang.n_words, embedding_size=config.embedding_size, dropout=config.dropout)\n\n        # encoder = EncoderSeq(input_size=input_lang.n_words, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        encoder = EncoderSeq(cell_type=config.cell_type, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        predict = Prediction(hidden_size=config.hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums), input_size=len(generate_nums), dropout=config.dropout)\n        generate = GenerateNode(hidden_size=config.hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums), embedding_size=config.embedding_size, dropout=config.dropout)\n        merge = Merge(hidden_size=config.hidden_size, embedding_size=config.embedding_size, dropout=config.dropout)\n        # the embedding layer is only for generated number embeddings, operators, and paddings\n\n        logger.debug('Models Initialized')\n        logger.info('Initializing Optimizers...')\n\n        embedding_optimizer = torch.optim.Adam(embedding.parameters(), lr=config.emb_lr, weight_decay=config.weight_decay)\n        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        predict_optimizer = torch.optim.Adam(predict.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        generate_optimizer = torch.optim.Adam(generate.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        merge_optimizer = torch.optim.Adam(merge.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n\n        logger.debug('Optimizers Initialized')\n        logger.info('Initializing Schedulers...')\n\n        embedding_scheduler = torch.optim.lr_scheduler.StepLR(embedding_optimizer, step_size=20, gamma=0.5)\n        encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n        predict_scheduler = torch.optim.lr_scheduler.StepLR(predict_optimizer, step_size=20, gamma=0.5)\n        generate_scheduler = torch.optim.lr_scheduler.StepLR(generate_optimizer, step_size=20, gamma=0.5)\n        merge_scheduler = torch.optim.lr_scheduler.StepLR(merge_optimizer, step_size=20, gamma=0.5)\n\n        logger.debug('Schedulers Initialized')\n\n        logger.info('Loading Models on GPU {}...'.format(config.gpu))\n\n        # Move models to GPU\n        if USE_CUDA:\n            embedding.to(device)\n            encoder.to(device)\n            predict.to(device)\n            generate.to(device)\n            merge.to(device)\n\n        logger.debug('Models loaded on GPU {}'.format(config.gpu))\n\n        # generate_num_ids = []\n        # for num in generate_nums:\n        # \tgenerate_num_ids.append(output_lang.word2index[num])\n\n        max_val_acc = 0.0\n        max_train_acc = 0.0\n        eq_acc = 0.0\n        best_epoch = -1\n        min_train_loss = float('inf')\n\n        logger.info('Starting Training Procedure')\n\n        for epoch in range(config.epochs):\n            loss_total = 0\n            input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches = prepare_train_batch(train_pairs, config.batch_size)\n\n            od = OrderedDict()\n            od['Epoch'] = epoch + 1\n            print_log(logger, od)\n\n            start = time.time()\n            for idx in range(len(input_lengths)):\n                # loss = train_tree(\n                # \tinput_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n                # \tnum_stack_batches[idx], num_size_batches[idx], generate_num_ids, encoder, predict, generate, merge,\n                # \tencoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx])\n                loss = train_tree(\n                    config, input_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n                    num_stack_batches[idx], num_size_batches[idx], generate_num_ids, embedding, encoder, predict, generate, merge,\n                    embedding_optimizer, encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, input_lang, output_lang, \n                    num_pos_batches[idx])\n                loss_total += loss\n                print(\"Completed {} / {}...\".format(idx, len(input_lengths)), end = '\\r', flush = True)\n\n            embedding_scheduler.step()\n            encoder_scheduler.step()\n            predict_scheduler.step()\n            generate_scheduler.step()\n            merge_scheduler.step()\n\n            logger.debug('Training for epoch {} completed...\\nTime Taken: {}'.format(epoch, time_since(time.time() - start)))\n\n            if loss_total / len(input_lengths) < min_train_loss:\n                min_train_loss = loss_total / len(input_lengths)\n\n            train_value_ac = 0\n            train_equation_ac = 0\n            train_eval_total = 1\n            if config.show_train_acc:\n                train_eval_total = 0\n                logger.info('Computing Train Accuracy')\n                start = time.time()\n                with torch.no_grad():\n                    for train_batch in train_pairs:\n                        # train_res = evaluate_tree(train_batch[0], train_batch[1], generate_num_ids, encoder, predict, generate,\n                        # \t\t\t\t\t\t merge, output_lang, train_batch[5], beam_size=config.beam_size)\n                        train_res = evaluate_tree(config, train_batch[0], train_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                                 merge, input_lang, output_lang, train_batch[5], beam_size=config.beam_size)\n                        train_val_ac, train_equ_ac, _, _ = compute_prefix_tree_result(train_res, train_batch[2], output_lang, train_batch[4], train_batch[6])\n\n                        if train_val_ac:\n                            train_value_ac += 1\n                        if train_equ_ac:\n                            train_equation_ac += 1\n                        train_eval_total += 1\n\n                logger.debug('Train Accuracy Computed...\\nTime Taken: {}'.format(time_since(time.time() - start)))\n\n            logger.info('Starting Validation')\n\n            value_ac = 0\n            equation_ac = 0\n            eval_total = 0\n            start = time.time()\n\n            with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                f_out.write('---------------------------------------\\n')\n                f_out.write('Epoch: ' + str(epoch) + '\\n')\n                f_out.write('---------------------------------------\\n')\n                f_out.close()\n\n            ex_num = 0\n            for test_batch in test_pairs:\n                # test_res = evaluate_tree(test_batch[0], test_batch[1], generate_num_ids, encoder, predict, generate,\n                # \t\t\t\t\t\t merge, output_lang, test_batch[5], beam_size=config.beam_size)\n                test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                         merge, input_lang, output_lang, test_batch[5], beam_size=config.beam_size)\n                val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n                cur_result = 0\n                if val_ac:\n                    value_ac += 1\n                    cur_result = 1\n                if equ_ac:\n                    equation_ac += 1\n                eval_total += 1\n\n                with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                    f_out.write('Example: ' + str(ex_num) + '\\n')\n                    f_out.write('Source: ' + stack_to_string(sentence_from_indexes(input_lang, test_batch[0])) + '\\n')\n                    f_out.write('Target: ' + stack_to_string(sentence_from_indexes(output_lang, test_batch[2])) + '\\n')\n                    f_out.write('Generated: ' + stack_to_string(sentence_from_indexes(output_lang, test_res)) + '\\n')\n                    if config.challenge_disp:\n                        f_out.write('Type: ' + test_batch[7] + '\\n')\n                        f_out.write('Variation Type: ' + test_batch[8] + '\\n')\n                        f_out.write('Annotator: ' + test_batch[9] + '\\n')\n                        f_out.write('Alternate: ' + str(test_batch[10]) + '\\n')\n                    if config.nums_disp:\n                        src_nums = len(test_batch[4])\n                        tgt_nums = 0\n                        pred_nums = 0\n                        for k_tgt in sentence_from_indexes(output_lang, test_batch[2]):\n                            if k_tgt not in ['+', '-', '*', '/']:\n                                tgt_nums += 1\n                        for k_pred in sentence_from_indexes(output_lang, test_res):\n                            if k_pred not in ['+', '-', '*', '/']:\n                                pred_nums += 1\n                        f_out.write('Numbers in question: ' + str(src_nums) + '\\n')\n                        f_out.write('Numbers in Target Equation: ' + str(tgt_nums) + '\\n')\n                        f_out.write('Numbers in Predicted Equation: ' + str(pred_nums) + '\\n')\n                    f_out.write('Result: ' + str(cur_result) + '\\n' + '\\n')\n                    f_out.close()\n\n                ex_num+=1\n\n            if float(train_value_ac) / train_eval_total > max_train_acc:\n                max_train_acc = float(train_value_ac) / train_eval_total\n\n            if float(value_ac) / eval_total > max_val_acc:\n                max_val_acc = float(value_ac) / eval_total\n                eq_acc = float(equation_ac) / eval_total\n                best_epoch = epoch+1\n\n                state = {\n                        'epoch' : epoch,\n                        'best_epoch': best_epoch-1,\n                        'embedding_state_dict': embedding.state_dict(),\n                        'encoder_state_dict': encoder.state_dict(),\n                        'predict_state_dict': predict.state_dict(),\n                        'generate_state_dict': generate.state_dict(),\n                        'merge_state_dict': merge.state_dict(),\n                        'embedding_optimizer_state_dict': embedding_optimizer.state_dict(),\n                        'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n                        'predict_optimizer_state_dict': predict_optimizer.state_dict(),\n                        'generate_optimizer_state_dict': generate_optimizer.state_dict(),\n                        'merge_optimizer_state_dict': merge_optimizer.state_dict(),\n                        'embedding_scheduler_state_dict': embedding_scheduler.state_dict(),\n                        'encoder_scheduler_state_dict': encoder_scheduler.state_dict(),\n                        'predict_scheduler_state_dict': predict_scheduler.state_dict(),\n                        'generate_scheduler_state_dict': generate_scheduler.state_dict(),\n                        'merge_scheduler_state_dict': merge_scheduler.state_dict(),\n                        'voc1': input_lang,\n                        'voc2': output_lang,\n                        'train_loss_epoch' : loss_total / len(input_lengths),\n                        'min_train_loss' : min_train_loss,\n                        'val_acc_epoch' : float(value_ac) / eval_total,\n                        'max_val_acc' : max_val_acc,\n                        'equation_acc' : eq_acc,\n                        'max_train_acc' : max_train_acc,\n                        'generate_nums' : generate_nums\n                    }\n\n                if config.save_model:\n                    save_checkpoint(state, epoch, logger, config.model_path, config.ckpt)\n\n            od = OrderedDict()\n            od['Epoch'] = epoch + 1\n            od['best_epoch'] = best_epoch\n            od['train_loss_epoch'] = loss_total / len(input_lengths)\n            od['min_train_loss'] = min_train_loss\n            od['train_acc_epoch'] = float(train_value_ac) / train_eval_total\n            od['max_train_acc'] = max_train_acc\n            od['val_acc_epoch'] = float(value_ac) / eval_total\n            od['equation_acc_epoch'] = float(equation_ac) / eval_total\n            od['max_val_acc'] = max_val_acc\n            od['equation_acc'] = eq_acc\n            print_log(logger, od)\n\n            logger.debug('Validation Completed...\\nTime Taken: {}'.format(time_since(time.time() - start)))\n\n        if config.results:\n            store_results(config, max_train_acc, max_val_acc, eq_acc, min_train_loss, best_epoch)\n            logger.info('Scores saved at {}'.format(config.result_path))\n\n    else:\n        gpu = config.gpu\n        mode = config.mode\n        dataset = config.dataset\n        batch_size = config.batch_size\n        old_run_name = config.run_name\n        with open(config_file, 'rb') as f:\n            config = AttrDict(pickle.load(f))\n            config.gpu = gpu\n            config.mode = mode\n            config.dataset = dataset\n            config.batch_size = batch_size\n\n        logger.info('Initializing Models...')\n\n        # Initialize models\n        embedding = None\n        if config.embedding == 'bert':\n            embedding = BertEncoder(config.emb_name, device, config.freeze_emb)\n        elif config.embedding == 'roberta':\n            embedding = RobertaEncoder(config.emb_name, device, config.freeze_emb)\n        else:\n            embedding = Embedding(config, input_lang, input_size=input_lang.n_words, embedding_size=config.embedding_size, dropout=config.dropout)\n\n        # encoder = EncoderSeq(input_size=input_lang.n_words, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        encoder = EncoderSeq(cell_type=config.cell_type, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        predict = Prediction(hidden_size=config.hidden_size, op_nums=output_lang.n_words - config.copy_nums - 1 - config.len_generate_nums, input_size=config.len_generate_nums, dropout=config.dropout)\n        generate = GenerateNode(hidden_size=config.hidden_size, op_nums=output_lang.n_words - config.copy_nums - 1 - config.len_generate_nums, embedding_size=config.embedding_size, dropout=config.dropout)\n        merge = Merge(hidden_size=config.hidden_size, embedding_size=config.embedding_size, dropout=config.dropout)\n        # the embedding layer is only for generated number embeddings, operators, and paddings\n\n        logger.debug('Models Initialized')\n\n        epoch_offset, min_train_loss, max_train_acc, max_val_acc, equation_acc, best_epoch, generate_nums = load_checkpoint(config, embedding, encoder, predict, generate, merge, config.mode, checkpoint, logger, device)\n\n        logger.info('Prediction from')\n        od = OrderedDict()\n        od['epoch'] = epoch_offset\n        od['min_train_loss'] = min_train_loss\n        od['max_train_acc'] = max_train_acc\n        od['max_val_acc'] = max_val_acc\n        od['equation_acc'] = equation_acc\n        od['best_epoch'] = best_epoch\n        print_log(logger, od)\n\n        generate_num_ids = []\n        for num in generate_nums:\n            generate_num_ids.append(output_lang.word2index[num])\n\n        value_ac = 0\n        equation_ac = 0\n        eval_total = 0\n        start = time.time()\n\n        with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n            f_out.write('---------------------------------------\\n')\n            f_out.write('Test Name: ' + old_run_name + '\\n')\n            f_out.write('---------------------------------------\\n')\n            f_out.close()\n\n        test_res_ques, test_res_act, test_res_gen, test_res_scores = [], [], [], []\n\n        ex_num = 0\n        for test_batch in test_pairs:\n            test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                     merge, input_lang, output_lang, test_batch[5], beam_size=config.beam_size)\n            val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n            cur_result = 0\n            if val_ac:\n                value_ac += 1\n                cur_result = 1\n            if equ_ac:\n                equation_ac += 1\n            eval_total += 1\n\n            test_res_ques.append(stack_to_string(sentence_from_indexes(input_lang, test_batch[0])))\n            test_res_act.append(stack_to_string(sentence_from_indexes(output_lang, test_batch[2])))\n            test_res_gen.append(stack_to_string(sentence_from_indexes(output_lang, test_res)))\n            test_res_scores.append(cur_result)\n\n            with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                f_out.write('Example: ' + str(ex_num) + '\\n')\n                f_out.write('Source: ' + stack_to_string(sentence_from_indexes(input_lang, test_batch[0])) + '\\n')\n                f_out.write('Target: ' + stack_to_string(sentence_from_indexes(output_lang, test_batch[2])) + '\\n')\n                f_out.write('Generated: ' + stack_to_string(sentence_from_indexes(output_lang, test_res)) + '\\n')\n                if config.nums_disp:\n                    src_nums = len(test_batch[4])\n                    tgt_nums = 0\n                    pred_nums = 0\n                    for k_tgt in sentence_from_indexes(output_lang, test_batch[2]):\n                        if k_tgt not in ['+', '-', '*', '/']:\n                            tgt_nums += 1\n                    for k_pred in sentence_from_indexes(output_lang, test_res):\n                        if k_pred not in ['+', '-', '*', '/']:\n                            pred_nums += 1\n                    f_out.write('Numbers in question: ' + str(src_nums) + '\\n')\n                    f_out.write('Numbers in Target Equation: ' + str(tgt_nums) + '\\n')\n                    f_out.write('Numbers in Predicted Equation: ' + str(pred_nums) + '\\n')\n                f_out.write('Result: ' + str(cur_result) + '\\n' + '\\n')\n                f_out.close()\n\n            ex_num+=1\n\n        results_df = pd.DataFrame([test_res_ques, test_res_act, test_res_gen, test_res_scores]).transpose()\n        results_df.columns = ['Question', 'Actual Equation', 'Generated Equation', 'Score']\n        csv_file_path = os.path.join(config.outputs_path, config.dataset+'.csv')\n        results_df.to_csv(csv_file_path, index = False)\n        logger.info('Accuracy: {}'.format(sum(test_res_scores)/len(test_res_scores)))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:40:05.144726Z","iopub.execute_input":"2024-11-15T11:40:05.145098Z","iopub.status.idle":"2024-11-15T12:23:27.278627Z","shell.execute_reply.started":"2024-11-15T11:40:05.145056Z","shell.execute_reply":"2024-11-15T12:23:27.276851Z"}},"outputs":[{"name":"stderr","text":"2024-11-15 11:40:05,282 | INFO | 1121885345.py: 405 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-15 11:40:05,282 | INFO | 1121885345.py: 405 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-15 11:40:05,282 | INFO | 1121885345.py: 405 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-15 11:40:05,282 | INFO | 1121885345.py: 405 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-15 11:40:05,282 | INFO | 1121885345.py: 405 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-15 11:40:05,287 | DEBUG | 1121885345.py: 406 : <module>() ::\t Created Relevant Directories\n2024-11-15 11:40:05,287 | DEBUG | 1121885345.py: 406 : <module>() ::\t Created Relevant Directories\n2024-11-15 11:40:05,287 | DEBUG | 1121885345.py: 406 : <module>() ::\t Created Relevant Directories\n2024-11-15 11:40:05,287 | DEBUG | 1121885345.py: 406 : <module>() ::\t Created Relevant Directories\n2024-11-15 11:40:05,287 | DEBUG | 1121885345.py: 406 : <module>() ::\t Created Relevant Directories\n2024-11-15 11:40:05,292 | INFO | 1121885345.py: 408 : <module>() ::\t Loading Data...\n2024-11-15 11:40:05,292 | INFO | 1121885345.py: 408 : <module>() ::\t Loading Data...\n2024-11-15 11:40:05,292 | INFO | 1121885345.py: 408 : <module>() ::\t Loading Data...\n2024-11-15 11:40:05,292 | INFO | 1121885345.py: 408 : <module>() ::\t Loading Data...\n2024-11-15 11:40:05,292 | INFO | 1121885345.py: 408 : <module>() ::\t Loading Data...\n","output_type":"stream"},{"name":"stdout","text":"Transfer numbers...\n","output_type":"stream"},{"name":"stderr","text":"2024-11-15 11:40:07,230 | DEBUG | 1121885345.py: 413 : <module>() ::\t Data Loaded...\n2024-11-15 11:40:07,230 | DEBUG | 1121885345.py: 413 : <module>() ::\t Data Loaded...\n2024-11-15 11:40:07,230 | DEBUG | 1121885345.py: 413 : <module>() ::\t Data Loaded...\n2024-11-15 11:40:07,230 | DEBUG | 1121885345.py: 413 : <module>() ::\t Data Loaded...\n2024-11-15 11:40:07,230 | DEBUG | 1121885345.py: 413 : <module>() ::\t Data Loaded...\n2024-11-15 11:40:07,235 | DEBUG | 1121885345.py: 415 : <module>() ::\t Number of Training Examples: 3138\n2024-11-15 11:40:07,235 | DEBUG | 1121885345.py: 415 : <module>() ::\t Number of Training Examples: 3138\n2024-11-15 11:40:07,235 | DEBUG | 1121885345.py: 415 : <module>() ::\t Number of Training Examples: 3138\n2024-11-15 11:40:07,235 | DEBUG | 1121885345.py: 415 : <module>() ::\t Number of Training Examples: 3138\n2024-11-15 11:40:07,235 | DEBUG | 1121885345.py: 415 : <module>() ::\t Number of Training Examples: 3138\n2024-11-15 11:40:07,239 | DEBUG | 1121885345.py: 416 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-15 11:40:07,239 | DEBUG | 1121885345.py: 416 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-15 11:40:07,239 | DEBUG | 1121885345.py: 416 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-15 11:40:07,239 | DEBUG | 1121885345.py: 416 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-15 11:40:07,239 | DEBUG | 1121885345.py: 416 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-15 11:40:07,244 | DEBUG | 1121885345.py: 417 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-15 11:40:07,244 | DEBUG | 1121885345.py: 417 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-15 11:40:07,244 | DEBUG | 1121885345.py: 417 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-15 11:40:07,244 | DEBUG | 1121885345.py: 417 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-15 11:40:07,244 | DEBUG | 1121885345.py: 417 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-15 11:40:07,248 | DEBUG | 1121885345.py: 418 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-15 11:40:07,248 | DEBUG | 1121885345.py: 418 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-15 11:40:07,248 | DEBUG | 1121885345.py: 418 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-15 11:40:07,248 | DEBUG | 1121885345.py: 418 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-15 11:40:07,248 | DEBUG | 1121885345.py: 418 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-15 11:40:07,254 | INFO | 1121885345.py: 427 : <module>() ::\t Creating Vocab...\n2024-11-15 11:40:07,254 | INFO | 1121885345.py: 427 : <module>() ::\t Creating Vocab...\n2024-11-15 11:40:07,254 | INFO | 1121885345.py: 427 : <module>() ::\t Creating Vocab...\n2024-11-15 11:40:07,254 | INFO | 1121885345.py: 427 : <module>() ::\t Creating Vocab...\n2024-11-15 11:40:07,254 | INFO | 1121885345.py: 427 : <module>() ::\t Creating Vocab...\n2024-11-15 11:40:08,449 | DEBUG | 505000428.py: 519 : trim() ::\t keep_words 4069 / 4069 = 1.0\n2024-11-15 11:40:08,449 | DEBUG | 505000428.py: 519 : trim() ::\t keep_words 4069 / 4069 = 1.0\n2024-11-15 11:40:08,449 | DEBUG | 505000428.py: 519 : trim() ::\t keep_words 4069 / 4069 = 1.0\n2024-11-15 11:40:08,449 | DEBUG | 505000428.py: 519 : trim() ::\t keep_words 4069 / 4069 = 1.0\n2024-11-15 11:40:08,449 | DEBUG | 505000428.py: 519 : trim() ::\t keep_words 4069 / 4069 = 1.0\n2024-11-15 11:40:08,689 | DEBUG | 505000428.py: 756 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-15 11:40:08,689 | DEBUG | 505000428.py: 756 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-15 11:40:08,689 | DEBUG | 505000428.py: 756 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-15 11:40:08,689 | DEBUG | 505000428.py: 756 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-15 11:40:08,689 | DEBUG | 505000428.py: 756 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-15 11:40:08,775 | WARNING | 505000428.py: 364 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-15 11:40:08,775 | WARNING | 505000428.py: 364 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-15 11:40:08,775 | WARNING | 505000428.py: 364 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-15 11:40:08,775 | WARNING | 505000428.py: 364 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-15 11:40:08,775 | WARNING | 505000428.py: 364 : get_latest_checkpoint() ::\t No Checkpoints Found\n2024-11-15 11:40:08,783 | DEBUG | 1121885345.py: 450 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-15 11:40:08,783 | DEBUG | 1121885345.py: 450 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-15 11:40:08,783 | DEBUG | 1121885345.py: 450 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-15 11:40:08,783 | DEBUG | 1121885345.py: 450 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-15 11:40:08,783 | DEBUG | 1121885345.py: 450 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-15 11:40:08,788 | DEBUG | 1121885345.py: 462 : <module>() ::\t Config File Saved\n2024-11-15 11:40:08,788 | DEBUG | 1121885345.py: 462 : <module>() ::\t Config File Saved\n2024-11-15 11:40:08,788 | DEBUG | 1121885345.py: 462 : <module>() ::\t Config File Saved\n2024-11-15 11:40:08,788 | DEBUG | 1121885345.py: 462 : <module>() ::\t Config File Saved\n2024-11-15 11:40:08,788 | DEBUG | 1121885345.py: 462 : <module>() ::\t Config File Saved\n2024-11-15 11:40:08,793 | INFO | 1121885345.py: 466 : <module>() ::\t Initializing Models...\n2024-11-15 11:40:08,793 | INFO | 1121885345.py: 466 : <module>() ::\t Initializing Models...\n2024-11-15 11:40:08,793 | INFO | 1121885345.py: 466 : <module>() ::\t Initializing Models...\n2024-11-15 11:40:08,793 | INFO | 1121885345.py: 466 : <module>() ::\t Initializing Models...\n2024-11-15 11:40:08,793 | INFO | 1121885345.py: 466 : <module>() ::\t Initializing Models...\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2024-11-15 11:40:09,466 | DEBUG | 1121885345.py: 484 : <module>() ::\t Models Initialized\n2024-11-15 11:40:09,466 | DEBUG | 1121885345.py: 484 : <module>() ::\t Models Initialized\n2024-11-15 11:40:09,466 | DEBUG | 1121885345.py: 484 : <module>() ::\t Models Initialized\n2024-11-15 11:40:09,466 | DEBUG | 1121885345.py: 484 : <module>() ::\t Models Initialized\n2024-11-15 11:40:09,466 | DEBUG | 1121885345.py: 484 : <module>() ::\t Models Initialized\n2024-11-15 11:40:09,470 | INFO | 1121885345.py: 485 : <module>() ::\t Initializing Optimizers...\n2024-11-15 11:40:09,470 | INFO | 1121885345.py: 485 : <module>() ::\t Initializing Optimizers...\n2024-11-15 11:40:09,470 | INFO | 1121885345.py: 485 : <module>() ::\t Initializing Optimizers...\n2024-11-15 11:40:09,470 | INFO | 1121885345.py: 485 : <module>() ::\t Initializing Optimizers...\n2024-11-15 11:40:09,470 | INFO | 1121885345.py: 485 : <module>() ::\t Initializing Optimizers...\n2024-11-15 11:40:09,475 | DEBUG | 1121885345.py: 493 : <module>() ::\t Optimizers Initialized\n2024-11-15 11:40:09,475 | DEBUG | 1121885345.py: 493 : <module>() ::\t Optimizers Initialized\n2024-11-15 11:40:09,475 | DEBUG | 1121885345.py: 493 : <module>() ::\t Optimizers Initialized\n2024-11-15 11:40:09,475 | DEBUG | 1121885345.py: 493 : <module>() ::\t Optimizers Initialized\n2024-11-15 11:40:09,475 | DEBUG | 1121885345.py: 493 : <module>() ::\t Optimizers Initialized\n2024-11-15 11:40:09,480 | INFO | 1121885345.py: 494 : <module>() ::\t Initializing Schedulers...\n2024-11-15 11:40:09,480 | INFO | 1121885345.py: 494 : <module>() ::\t Initializing Schedulers...\n2024-11-15 11:40:09,480 | INFO | 1121885345.py: 494 : <module>() ::\t Initializing Schedulers...\n2024-11-15 11:40:09,480 | INFO | 1121885345.py: 494 : <module>() ::\t Initializing Schedulers...\n2024-11-15 11:40:09,480 | INFO | 1121885345.py: 494 : <module>() ::\t Initializing Schedulers...\n2024-11-15 11:40:09,483 | DEBUG | 1121885345.py: 502 : <module>() ::\t Schedulers Initialized\n2024-11-15 11:40:09,483 | DEBUG | 1121885345.py: 502 : <module>() ::\t Schedulers Initialized\n2024-11-15 11:40:09,483 | DEBUG | 1121885345.py: 502 : <module>() ::\t Schedulers Initialized\n2024-11-15 11:40:09,483 | DEBUG | 1121885345.py: 502 : <module>() ::\t Schedulers Initialized\n2024-11-15 11:40:09,483 | DEBUG | 1121885345.py: 502 : <module>() ::\t Schedulers Initialized\n2024-11-15 11:40:09,486 | INFO | 1121885345.py: 504 : <module>() ::\t Loading Models on GPU 0...\n2024-11-15 11:40:09,486 | INFO | 1121885345.py: 504 : <module>() ::\t Loading Models on GPU 0...\n2024-11-15 11:40:09,486 | INFO | 1121885345.py: 504 : <module>() ::\t Loading Models on GPU 0...\n2024-11-15 11:40:09,486 | INFO | 1121885345.py: 504 : <module>() ::\t Loading Models on GPU 0...\n2024-11-15 11:40:09,486 | INFO | 1121885345.py: 504 : <module>() ::\t Loading Models on GPU 0...\n2024-11-15 11:40:09,676 | DEBUG | 1121885345.py: 514 : <module>() ::\t Models loaded on GPU 0\n2024-11-15 11:40:09,676 | DEBUG | 1121885345.py: 514 : <module>() ::\t Models loaded on GPU 0\n2024-11-15 11:40:09,676 | DEBUG | 1121885345.py: 514 : <module>() ::\t Models loaded on GPU 0\n2024-11-15 11:40:09,676 | DEBUG | 1121885345.py: 514 : <module>() ::\t Models loaded on GPU 0\n2024-11-15 11:40:09,676 | DEBUG | 1121885345.py: 514 : <module>() ::\t Models loaded on GPU 0\n2024-11-15 11:40:09,682 | INFO | 1121885345.py: 526 : <module>() ::\t Starting Training Procedure\n2024-11-15 11:40:09,682 | INFO | 1121885345.py: 526 : <module>() ::\t Starting Training Procedure\n2024-11-15 11:40:09,682 | INFO | 1121885345.py: 526 : <module>() ::\t Starting Training Procedure\n2024-11-15 11:40:09,682 | INFO | 1121885345.py: 526 : <module>() ::\t Starting Training Procedure\n2024-11-15 11:40:09,682 | INFO | 1121885345.py: 526 : <module>() ::\t Starting Training Procedure\n2024-11-15 11:40:09,889 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n2024-11-15 11:40:09,889 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n2024-11-15 11:40:09,889 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n2024-11-15 11:40:09,889 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n2024-11-15 11:40:09,889 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 11:40:55,476 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:40:55,476 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:40:55,476 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:40:55,476 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:40:55,476 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:40:55,481 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:40:55,481 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:40:55,481 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:40:55,481 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:40:55,481 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:43:32,485 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 11:43:32,485 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 11:43:32,485 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 11:43:32,485 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 11:43:32,485 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 11:43:32,489 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:43:32,489 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:43:32,489 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:43:32,489 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:43:32,489 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:44:28,525 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.2242009582253277\t\n min train loss: 1.2242009582253277\t\n train acc epoch: 0.04461440407903123\t\n max train acc: 0.04461440407903123\t\n val acc epoch: 0.034\t\n equation acc epoch: 0.021\t\n max val acc: 0.034\t\n equation acc: 0.021\t\n2024-11-15 11:44:28,525 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.2242009582253277\t\n min train loss: 1.2242009582253277\t\n train acc epoch: 0.04461440407903123\t\n max train acc: 0.04461440407903123\t\n val acc epoch: 0.034\t\n equation acc epoch: 0.021\t\n max val acc: 0.034\t\n equation acc: 0.021\t\n2024-11-15 11:44:28,525 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.2242009582253277\t\n min train loss: 1.2242009582253277\t\n train acc epoch: 0.04461440407903123\t\n max train acc: 0.04461440407903123\t\n val acc epoch: 0.034\t\n equation acc epoch: 0.021\t\n max val acc: 0.034\t\n equation acc: 0.021\t\n2024-11-15 11:44:28,525 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.2242009582253277\t\n min train loss: 1.2242009582253277\t\n train acc epoch: 0.04461440407903123\t\n max train acc: 0.04461440407903123\t\n val acc epoch: 0.034\t\n equation acc epoch: 0.021\t\n max val acc: 0.034\t\n equation acc: 0.021\t\n2024-11-15 11:44:28,525 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.2242009582253277\t\n min train loss: 1.2242009582253277\t\n train acc epoch: 0.04461440407903123\t\n max train acc: 0.04461440407903123\t\n val acc epoch: 0.034\t\n equation acc epoch: 0.021\t\n max val acc: 0.034\t\n equation acc: 0.021\t\n2024-11-15 11:44:28,530 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:44:28,530 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:44:28,530 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:44:28,530 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:44:28,530 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:44:28,734 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n2024-11-15 11:44:28,734 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n2024-11-15 11:44:28,734 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n2024-11-15 11:44:28,734 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n2024-11-15 11:44:28,734 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 11:45:14,137 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:45:14,137 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:45:14,137 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:45:14,137 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:45:14,137 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:45:14,142 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:45:14,142 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:45:14,142 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:45:14,142 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:45:14,142 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:47:55,832 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 41s\n2024-11-15 11:47:55,832 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 41s\n2024-11-15 11:47:55,832 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 41s\n2024-11-15 11:47:55,832 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 41s\n2024-11-15 11:47:55,832 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 41s\n2024-11-15 11:47:55,836 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:47:55,836 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:47:55,836 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:47:55,836 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:47:55,836 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:48:54,721 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.7318120508024535\t\n min train loss: 0.7318120508024535\t\n train acc epoch: 0.5089228808158063\t\n max train acc: 0.5089228808158063\t\n val acc epoch: 0.176\t\n equation acc epoch: 0.158\t\n max val acc: 0.176\t\n equation acc: 0.158\t\n2024-11-15 11:48:54,721 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.7318120508024535\t\n min train loss: 0.7318120508024535\t\n train acc epoch: 0.5089228808158063\t\n max train acc: 0.5089228808158063\t\n val acc epoch: 0.176\t\n equation acc epoch: 0.158\t\n max val acc: 0.176\t\n equation acc: 0.158\t\n2024-11-15 11:48:54,721 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.7318120508024535\t\n min train loss: 0.7318120508024535\t\n train acc epoch: 0.5089228808158063\t\n max train acc: 0.5089228808158063\t\n val acc epoch: 0.176\t\n equation acc epoch: 0.158\t\n max val acc: 0.176\t\n equation acc: 0.158\t\n2024-11-15 11:48:54,721 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.7318120508024535\t\n min train loss: 0.7318120508024535\t\n train acc epoch: 0.5089228808158063\t\n max train acc: 0.5089228808158063\t\n val acc epoch: 0.176\t\n equation acc epoch: 0.158\t\n max val acc: 0.176\t\n equation acc: 0.158\t\n2024-11-15 11:48:54,721 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.7318120508024535\t\n min train loss: 0.7318120508024535\t\n train acc epoch: 0.5089228808158063\t\n max train acc: 0.5089228808158063\t\n val acc epoch: 0.176\t\n equation acc epoch: 0.158\t\n max val acc: 0.176\t\n equation acc: 0.158\t\n2024-11-15 11:48:54,725 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 11:48:54,725 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 11:48:54,725 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 11:48:54,725 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 11:48:54,725 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 11:48:54,934 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n2024-11-15 11:48:54,934 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n2024-11-15 11:48:54,934 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n2024-11-15 11:48:54,934 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n2024-11-15 11:48:54,934 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 11:49:40,122 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:49:40,122 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:49:40,122 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:49:40,122 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:49:40,122 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:49:40,128 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:49:40,128 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:49:40,128 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:49:40,128 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:49:40,128 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:52:09,641 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 29s\n2024-11-15 11:52:09,641 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 29s\n2024-11-15 11:52:09,641 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 29s\n2024-11-15 11:52:09,641 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 29s\n2024-11-15 11:52:09,641 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 29s\n2024-11-15 11:52:09,645 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:52:09,645 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:52:09,645 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:52:09,645 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:52:09,645 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:53:06,380 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.4922108223595595\t\n min train loss: 0.4922108223595595\t\n train acc epoch: 0.6905672402804334\t\n max train acc: 0.6905672402804334\t\n val acc epoch: 0.223\t\n equation acc epoch: 0.191\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:53:06,380 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.4922108223595595\t\n min train loss: 0.4922108223595595\t\n train acc epoch: 0.6905672402804334\t\n max train acc: 0.6905672402804334\t\n val acc epoch: 0.223\t\n equation acc epoch: 0.191\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:53:06,380 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.4922108223595595\t\n min train loss: 0.4922108223595595\t\n train acc epoch: 0.6905672402804334\t\n max train acc: 0.6905672402804334\t\n val acc epoch: 0.223\t\n equation acc epoch: 0.191\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:53:06,380 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.4922108223595595\t\n min train loss: 0.4922108223595595\t\n train acc epoch: 0.6905672402804334\t\n max train acc: 0.6905672402804334\t\n val acc epoch: 0.223\t\n equation acc epoch: 0.191\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:53:06,380 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.4922108223595595\t\n min train loss: 0.4922108223595595\t\n train acc epoch: 0.6905672402804334\t\n max train acc: 0.6905672402804334\t\n val acc epoch: 0.223\t\n equation acc epoch: 0.191\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:53:06,384 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:53:06,384 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:53:06,384 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:53:06,384 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:53:06,384 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 11:53:06,592 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n2024-11-15 11:53:06,592 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n2024-11-15 11:53:06,592 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n2024-11-15 11:53:06,592 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n2024-11-15 11:53:06,592 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 11:53:51,845 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:53:51,845 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:53:51,845 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:53:51,845 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:53:51,845 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 45s\n2024-11-15 11:53:51,850 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:53:51,850 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:53:51,850 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:53:51,850 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:53:51,850 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:56:35,725 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 43s\n2024-11-15 11:56:35,725 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 43s\n2024-11-15 11:56:35,725 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 43s\n2024-11-15 11:56:35,725 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 43s\n2024-11-15 11:56:35,725 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 43s\n2024-11-15 11:56:35,730 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:56:35,730 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:56:35,730 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:56:35,730 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:56:35,730 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 11:57:37,106 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n best epoch: 3\t\n train loss epoch: 0.38031426442758687\t\n min train loss: 0.38031426442758687\t\n train acc epoch: 0.7718291905672403\t\n max train acc: 0.7718291905672403\t\n val acc epoch: 0.221\t\n equation acc epoch: 0.19\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:57:37,106 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n best epoch: 3\t\n train loss epoch: 0.38031426442758687\t\n min train loss: 0.38031426442758687\t\n train acc epoch: 0.7718291905672403\t\n max train acc: 0.7718291905672403\t\n val acc epoch: 0.221\t\n equation acc epoch: 0.19\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:57:37,106 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n best epoch: 3\t\n train loss epoch: 0.38031426442758687\t\n min train loss: 0.38031426442758687\t\n train acc epoch: 0.7718291905672403\t\n max train acc: 0.7718291905672403\t\n val acc epoch: 0.221\t\n equation acc epoch: 0.19\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:57:37,106 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n best epoch: 3\t\n train loss epoch: 0.38031426442758687\t\n min train loss: 0.38031426442758687\t\n train acc epoch: 0.7718291905672403\t\n max train acc: 0.7718291905672403\t\n val acc epoch: 0.221\t\n equation acc epoch: 0.19\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:57:37,106 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 4\t\n best epoch: 3\t\n train loss epoch: 0.38031426442758687\t\n min train loss: 0.38031426442758687\t\n train acc epoch: 0.7718291905672403\t\n max train acc: 0.7718291905672403\t\n val acc epoch: 0.221\t\n equation acc epoch: 0.19\t\n max val acc: 0.223\t\n equation acc: 0.191\t\n2024-11-15 11:57:37,110 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 1s\n2024-11-15 11:57:37,110 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 1s\n2024-11-15 11:57:37,110 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 1s\n2024-11-15 11:57:37,110 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 1s\n2024-11-15 11:57:37,110 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 1s\n2024-11-15 11:57:37,314 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n2024-11-15 11:57:37,314 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n2024-11-15 11:57:37,314 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n2024-11-15 11:57:37,314 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n2024-11-15 11:57:37,314 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 11:58:21,692 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 11:58:21,692 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 11:58:21,692 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 11:58:21,692 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 11:58:21,692 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 11:58:21,696 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:58:21,696 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:58:21,696 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:58:21,696 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 11:58:21,696 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:00:44,915 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 23s\n2024-11-15 12:00:44,915 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 23s\n2024-11-15 12:00:44,915 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 23s\n2024-11-15 12:00:44,915 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 23s\n2024-11-15 12:00:44,915 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 23s\n2024-11-15 12:00:44,920 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:00:44,920 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:00:44,920 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:00:44,920 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:00:44,920 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:01:38,461 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n best epoch: 5\t\n train loss epoch: 0.3168967299276802\t\n min train loss: 0.3168967299276802\t\n train acc epoch: 0.8358827278521351\t\n max train acc: 0.8358827278521351\t\n val acc epoch: 0.245\t\n equation acc epoch: 0.221\t\n max val acc: 0.245\t\n equation acc: 0.221\t\n2024-11-15 12:01:38,461 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n best epoch: 5\t\n train loss epoch: 0.3168967299276802\t\n min train loss: 0.3168967299276802\t\n train acc epoch: 0.8358827278521351\t\n max train acc: 0.8358827278521351\t\n val acc epoch: 0.245\t\n equation acc epoch: 0.221\t\n max val acc: 0.245\t\n equation acc: 0.221\t\n2024-11-15 12:01:38,461 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n best epoch: 5\t\n train loss epoch: 0.3168967299276802\t\n min train loss: 0.3168967299276802\t\n train acc epoch: 0.8358827278521351\t\n max train acc: 0.8358827278521351\t\n val acc epoch: 0.245\t\n equation acc epoch: 0.221\t\n max val acc: 0.245\t\n equation acc: 0.221\t\n2024-11-15 12:01:38,461 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n best epoch: 5\t\n train loss epoch: 0.3168967299276802\t\n min train loss: 0.3168967299276802\t\n train acc epoch: 0.8358827278521351\t\n max train acc: 0.8358827278521351\t\n val acc epoch: 0.245\t\n equation acc epoch: 0.221\t\n max val acc: 0.245\t\n equation acc: 0.221\t\n2024-11-15 12:01:38,461 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 5\t\n best epoch: 5\t\n train loss epoch: 0.3168967299276802\t\n min train loss: 0.3168967299276802\t\n train acc epoch: 0.8358827278521351\t\n max train acc: 0.8358827278521351\t\n val acc epoch: 0.245\t\n equation acc epoch: 0.221\t\n max val acc: 0.245\t\n equation acc: 0.221\t\n2024-11-15 12:01:38,465 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 53s\n2024-11-15 12:01:38,465 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 53s\n2024-11-15 12:01:38,465 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 53s\n2024-11-15 12:01:38,465 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 53s\n2024-11-15 12:01:38,465 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 53s\n2024-11-15 12:01:38,669 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n2024-11-15 12:01:38,669 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n2024-11-15 12:01:38,669 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n2024-11-15 12:01:38,669 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n2024-11-15 12:01:38,669 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 12:02:23,075 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:02:23,075 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:02:23,075 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:02:23,075 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:02:23,075 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:02:23,080 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:02:23,080 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:02:23,080 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:02:23,080 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:02:23,080 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:05:03,557 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:05:03,557 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:05:03,557 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:05:03,557 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:05:03,557 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:05:03,562 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:05:03,562 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:05:03,562 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:05:03,562 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:05:03,562 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:06:03,346 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n best epoch: 6\t\n train loss epoch: 0.25186822915137724\t\n min train loss: 0.25186822915137724\t\n train acc epoch: 0.8836838750796686\t\n max train acc: 0.8836838750796686\t\n val acc epoch: 0.248\t\n equation acc epoch: 0.218\t\n max val acc: 0.248\t\n equation acc: 0.218\t\n2024-11-15 12:06:03,346 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n best epoch: 6\t\n train loss epoch: 0.25186822915137724\t\n min train loss: 0.25186822915137724\t\n train acc epoch: 0.8836838750796686\t\n max train acc: 0.8836838750796686\t\n val acc epoch: 0.248\t\n equation acc epoch: 0.218\t\n max val acc: 0.248\t\n equation acc: 0.218\t\n2024-11-15 12:06:03,346 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n best epoch: 6\t\n train loss epoch: 0.25186822915137724\t\n min train loss: 0.25186822915137724\t\n train acc epoch: 0.8836838750796686\t\n max train acc: 0.8836838750796686\t\n val acc epoch: 0.248\t\n equation acc epoch: 0.218\t\n max val acc: 0.248\t\n equation acc: 0.218\t\n2024-11-15 12:06:03,346 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n best epoch: 6\t\n train loss epoch: 0.25186822915137724\t\n min train loss: 0.25186822915137724\t\n train acc epoch: 0.8836838750796686\t\n max train acc: 0.8836838750796686\t\n val acc epoch: 0.248\t\n equation acc epoch: 0.218\t\n max val acc: 0.248\t\n equation acc: 0.218\t\n2024-11-15 12:06:03,346 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 6\t\n best epoch: 6\t\n train loss epoch: 0.25186822915137724\t\n min train loss: 0.25186822915137724\t\n train acc epoch: 0.8836838750796686\t\n max train acc: 0.8836838750796686\t\n val acc epoch: 0.248\t\n equation acc epoch: 0.218\t\n max val acc: 0.248\t\n equation acc: 0.218\t\n2024-11-15 12:06:03,351 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 59s\n2024-11-15 12:06:03,351 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 59s\n2024-11-15 12:06:03,351 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 59s\n2024-11-15 12:06:03,351 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 59s\n2024-11-15 12:06:03,351 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 59s\n2024-11-15 12:06:03,555 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n2024-11-15 12:06:03,555 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n2024-11-15 12:06:03,555 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n2024-11-15 12:06:03,555 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n2024-11-15 12:06:03,555 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 12:06:47,899 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:06:47,899 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:06:47,899 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:06:47,899 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:06:47,899 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:06:47,903 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:06:47,903 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:06:47,903 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:06:47,903 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:06:47,903 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:09:24,465 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 12:09:24,465 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 12:09:24,465 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 12:09:24,465 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 12:09:24,465 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 36s\n2024-11-15 12:09:24,469 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:09:24,469 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:09:24,469 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:09:24,469 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:09:24,469 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:10:22,886 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.20845108292996883\t\n min train loss: 0.20845108292996883\t\n train acc epoch: 0.911408540471638\t\n max train acc: 0.911408540471638\t\n val acc epoch: 0.25\t\n equation acc epoch: 0.219\t\n max val acc: 0.25\t\n equation acc: 0.219\t\n2024-11-15 12:10:22,886 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.20845108292996883\t\n min train loss: 0.20845108292996883\t\n train acc epoch: 0.911408540471638\t\n max train acc: 0.911408540471638\t\n val acc epoch: 0.25\t\n equation acc epoch: 0.219\t\n max val acc: 0.25\t\n equation acc: 0.219\t\n2024-11-15 12:10:22,886 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.20845108292996883\t\n min train loss: 0.20845108292996883\t\n train acc epoch: 0.911408540471638\t\n max train acc: 0.911408540471638\t\n val acc epoch: 0.25\t\n equation acc epoch: 0.219\t\n max val acc: 0.25\t\n equation acc: 0.219\t\n2024-11-15 12:10:22,886 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.20845108292996883\t\n min train loss: 0.20845108292996883\t\n train acc epoch: 0.911408540471638\t\n max train acc: 0.911408540471638\t\n val acc epoch: 0.25\t\n equation acc epoch: 0.219\t\n max val acc: 0.25\t\n equation acc: 0.219\t\n2024-11-15 12:10:22,886 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 7\t\n best epoch: 7\t\n train loss epoch: 0.20845108292996883\t\n min train loss: 0.20845108292996883\t\n train acc epoch: 0.911408540471638\t\n max train acc: 0.911408540471638\t\n val acc epoch: 0.25\t\n equation acc epoch: 0.219\t\n max val acc: 0.25\t\n equation acc: 0.219\t\n2024-11-15 12:10:22,890 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 12:10:22,890 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 12:10:22,890 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 12:10:22,890 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 12:10:22,890 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 58s\n2024-11-15 12:10:23,092 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n2024-11-15 12:10:23,092 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n2024-11-15 12:10:23,092 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n2024-11-15 12:10:23,092 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n2024-11-15 12:10:23,092 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 12:11:07,587 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:11:07,587 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:11:07,587 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:11:07,587 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:11:07,587 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:11:07,590 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:11:07,590 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:11:07,590 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:11:07,590 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:11:07,590 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:13:40,643 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 33s\n2024-11-15 12:13:40,643 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 33s\n2024-11-15 12:13:40,643 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 33s\n2024-11-15 12:13:40,643 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 33s\n2024-11-15 12:13:40,643 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 33s\n2024-11-15 12:13:40,648 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:13:40,648 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:13:40,648 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:13:40,648 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:13:40,648 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:14:38,464 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.18477131119754336\t\n min train loss: 0.18477131119754336\t\n train acc epoch: 0.9289356277884002\t\n max train acc: 0.9289356277884002\t\n val acc epoch: 0.266\t\n equation acc epoch: 0.23\t\n max val acc: 0.266\t\n equation acc: 0.23\t\n2024-11-15 12:14:38,464 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.18477131119754336\t\n min train loss: 0.18477131119754336\t\n train acc epoch: 0.9289356277884002\t\n max train acc: 0.9289356277884002\t\n val acc epoch: 0.266\t\n equation acc epoch: 0.23\t\n max val acc: 0.266\t\n equation acc: 0.23\t\n2024-11-15 12:14:38,464 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.18477131119754336\t\n min train loss: 0.18477131119754336\t\n train acc epoch: 0.9289356277884002\t\n max train acc: 0.9289356277884002\t\n val acc epoch: 0.266\t\n equation acc epoch: 0.23\t\n max val acc: 0.266\t\n equation acc: 0.23\t\n2024-11-15 12:14:38,464 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.18477131119754336\t\n min train loss: 0.18477131119754336\t\n train acc epoch: 0.9289356277884002\t\n max train acc: 0.9289356277884002\t\n val acc epoch: 0.266\t\n equation acc epoch: 0.23\t\n max val acc: 0.266\t\n equation acc: 0.23\t\n2024-11-15 12:14:38,464 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.18477131119754336\t\n min train loss: 0.18477131119754336\t\n train acc epoch: 0.9289356277884002\t\n max train acc: 0.9289356277884002\t\n val acc epoch: 0.266\t\n equation acc epoch: 0.23\t\n max val acc: 0.266\t\n equation acc: 0.23\t\n2024-11-15 12:14:38,469 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 57s\n2024-11-15 12:14:38,469 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 57s\n2024-11-15 12:14:38,469 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 57s\n2024-11-15 12:14:38,469 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 57s\n2024-11-15 12:14:38,469 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 57s\n2024-11-15 12:14:38,672 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n2024-11-15 12:14:38,672 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n2024-11-15 12:14:38,672 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n2024-11-15 12:14:38,672 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n2024-11-15 12:14:38,672 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 12:15:23,037 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:15:23,037 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:15:23,037 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:15:23,037 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:15:23,037 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:15:23,041 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:15:23,041 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:15:23,041 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:15:23,041 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:15:23,041 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:18:03,808 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:18:03,808 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:18:03,808 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:18:03,808 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:18:03,808 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 40s\n2024-11-15 12:18:03,812 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:18:03,812 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:18:03,812 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:18:03,812 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:18:03,812 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:19:00,623 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.16139830062685884\t\n min train loss: 0.16139830062685884\t\n train acc epoch: 0.9308476736775015\t\n max train acc: 0.9308476736775015\t\n val acc epoch: 0.277\t\n equation acc epoch: 0.249\t\n max val acc: 0.277\t\n equation acc: 0.249\t\n2024-11-15 12:19:00,623 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.16139830062685884\t\n min train loss: 0.16139830062685884\t\n train acc epoch: 0.9308476736775015\t\n max train acc: 0.9308476736775015\t\n val acc epoch: 0.277\t\n equation acc epoch: 0.249\t\n max val acc: 0.277\t\n equation acc: 0.249\t\n2024-11-15 12:19:00,623 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.16139830062685884\t\n min train loss: 0.16139830062685884\t\n train acc epoch: 0.9308476736775015\t\n max train acc: 0.9308476736775015\t\n val acc epoch: 0.277\t\n equation acc epoch: 0.249\t\n max val acc: 0.277\t\n equation acc: 0.249\t\n2024-11-15 12:19:00,623 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.16139830062685884\t\n min train loss: 0.16139830062685884\t\n train acc epoch: 0.9308476736775015\t\n max train acc: 0.9308476736775015\t\n val acc epoch: 0.277\t\n equation acc epoch: 0.249\t\n max val acc: 0.277\t\n equation acc: 0.249\t\n2024-11-15 12:19:00,623 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.16139830062685884\t\n min train loss: 0.16139830062685884\t\n train acc epoch: 0.9308476736775015\t\n max train acc: 0.9308476736775015\t\n val acc epoch: 0.277\t\n equation acc epoch: 0.249\t\n max val acc: 0.277\t\n equation acc: 0.249\t\n2024-11-15 12:19:00,626 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 12:19:00,626 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 12:19:00,626 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 12:19:00,626 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 12:19:00,626 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 56s\n2024-11-15 12:19:01,143 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n2024-11-15 12:19:01,143 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n2024-11-15 12:19:01,143 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n2024-11-15 12:19:01,143 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n2024-11-15 12:19:01,143 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 196 / 197...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-15 12:19:45,552 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:19:45,552 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:19:45,552 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:19:45,552 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:19:45,552 | DEBUG | 1121885345.py: 556 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 44s\n2024-11-15 12:19:45,557 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:19:45,557 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:19:45,557 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:19:45,557 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:19:45,557 | INFO | 1121885345.py: 566 : <module>() ::\t Computing Train Accuracy\n2024-11-15 12:22:17,265 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 31s\n2024-11-15 12:22:17,265 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 31s\n2024-11-15 12:22:17,265 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 31s\n2024-11-15 12:22:17,265 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 31s\n2024-11-15 12:22:17,265 | DEBUG | 1121885345.py: 582 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 31s\n2024-11-15 12:22:17,269 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:22:17,269 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:22:17,269 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:22:17,269 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:22:17,269 | INFO | 1121885345.py: 584 : <module>() ::\t Starting Validation\n2024-11-15 12:23:11,275 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.14598076895995038\t\n min train loss: 0.14598076895995038\t\n train acc epoch: 0.9388145315487572\t\n max train acc: 0.9388145315487572\t\n val acc epoch: 0.296\t\n equation acc epoch: 0.267\t\n max val acc: 0.296\t\n equation acc: 0.267\t\n2024-11-15 12:23:11,275 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.14598076895995038\t\n min train loss: 0.14598076895995038\t\n train acc epoch: 0.9388145315487572\t\n max train acc: 0.9388145315487572\t\n val acc epoch: 0.296\t\n equation acc epoch: 0.267\t\n max val acc: 0.296\t\n equation acc: 0.267\t\n2024-11-15 12:23:11,275 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.14598076895995038\t\n min train loss: 0.14598076895995038\t\n train acc epoch: 0.9388145315487572\t\n max train acc: 0.9388145315487572\t\n val acc epoch: 0.296\t\n equation acc epoch: 0.267\t\n max val acc: 0.296\t\n equation acc: 0.267\t\n2024-11-15 12:23:11,275 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.14598076895995038\t\n min train loss: 0.14598076895995038\t\n train acc epoch: 0.9388145315487572\t\n max train acc: 0.9388145315487572\t\n val acc epoch: 0.296\t\n equation acc epoch: 0.267\t\n max val acc: 0.296\t\n equation acc: 0.267\t\n2024-11-15 12:23:11,275 | INFO | 505000428.py: 405 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.14598076895995038\t\n min train loss: 0.14598076895995038\t\n train acc epoch: 0.9388145315487572\t\n max train acc: 0.9388145315487572\t\n val acc epoch: 0.296\t\n equation acc epoch: 0.267\t\n max val acc: 0.296\t\n equation acc: 0.267\t\n2024-11-15 12:23:11,280 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 54s\n2024-11-15 12:23:11,280 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 54s\n2024-11-15 12:23:11,280 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 54s\n2024-11-15 12:23:11,280 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 54s\n2024-11-15 12:23:11,280 | DEBUG | 1121885345.py: 694 : <module>() ::\t Validation Completed...\nTime Taken: 0h 0m 54s\n","output_type":"stream"},{"name":"stdout","text":"--Return--\nNone\n> \u001b[0;32m/tmp/ipykernel_30/505000428.py\u001b[0m(446)\u001b[0;36mstore_results\u001b[0;34m()\u001b[0m\n\u001b[0;32m    444 \u001b[0;31m                        \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 446 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mstore_val_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ipdb>  exit\n"}],"execution_count":117},{"cell_type":"code","source":"# import torch\n# torch.save(model, '/kaggle/working/entire_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T12:23:27.279359Z","iopub.status.idle":"2024-11-15T12:23:27.279768Z","shell.execute_reply.started":"2024-11-15T12:23:27.279571Z","shell.execute_reply":"2024-11-15T12:23:27.279590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_full_question(question, numbers):\n    for number in numbers:\n        if \"NUM\" in question:  # Check if 'NUM' exists in the question\n            question = question.replace(\"NUM\", str(number), 1)\n        else:\n            break  # Stop if there are no more 'NUM' placeholders\n    return question\n\n\ndef convert_eqn(equation, numbers):\n    for i, num in enumerate(numbers):\n        placeholder = f\"N{i}\"\n        equation = equation.replace(placeholder, str(num))\n    return equation\n\n\n# Function to write evaluation results into a file\ndef write_to_file(filename, data):\n    with open(filename, 'w') as f:\n        for line in data:\n            f.write(line + '\\n')\n\n# Loop over the validation data and collect output for file\noutput_lines = []\nct = 0\nfor test_batch in test_pairs:\n    test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                         merge, input_lang, output_lang, test_batch[5], beam_size=config.beam_size)\n    val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n    numbers = test_batch[4]\n    ques = generate_full_question(stack_to_string(sentence_from_indexes(input_lang, test_batch[0])), numbers)\n    output_lines.append(f\"Question: {ques}\")\n    true_eqn = convert_eqn(stack_to_string(sentence_from_indexes(output_lang, test_batch[2])), numbers)\n    output_lines.append(f\"True Answer: {true_eqn}\")\n    decode_eqn = convert_eqn(stack_to_string(sentence_from_indexes(output_lang, test_res)), numbers)\n    output_lines.append(f\"Decoded Answer: {decode_eqn}\")\n    \n    result_comparison = \"Correct\" if true_eqn == decode_eqn else \"Incorrect\"\n    output_lines.append(f\"Predicted Result: {result_comparison}\")\n    output_lines.append(\"-\" * 80)\n    ct += 1\n\n# Write all collected lines to eval.txt\nwrite_to_file(\"gts_eval_robert.txt\", output_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T12:23:33.103749Z","iopub.execute_input":"2024-11-15T12:23:33.104679Z","iopub.status.idle":"2024-11-15T12:24:26.988287Z","shell.execute_reply.started":"2024-11-15T12:23:33.104638Z","shell.execute_reply":"2024-11-15T12:24:26.987246Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # val_loss, decoder_output, decoder_attn = model.greedy_decode(\n    #     ques, sent1_var, sent2_var, input_len1, input_len2, validation=True\n    # )\n    \n    # # Iterate over each entry in the batch and collect the required information\n    # for i in range(len(ques)):\n    #     # Retrieve question, expected equation, numbers, and decoder output\n    #     question = ques[i]\n    #     expected_eqn = data['eqn'][i]\n    #     decoded_eqn = ' '.join(decoder_output[i])  # Convert list to string format\n    #     numbers = list(map(int, nums[i].split()))\n    #     true_answer = ans[i].item()\n\n    #     # Convert the equation tokens and evaluate the decoded answer\n    #     op = stack_to_string(decoder_output[i])\n    #     num = [float(nu) for nu in nums[i].split()]\n    #     pred = ans_evaluator(op, num)\n        \n    #     # Generate the converted question and equations\n    #     converted_question = generate_full_question(question, numbers)\n    #     converted_expected_eqn = convert_eqn(expected_eqn, numbers)\n    #     converted_decoded_eqn = convert_eqn(decoded_eqn, numbers)\n\n    #     # Compare decoded answer with true answer\n    #     result_comparison = \"Correct\" if abs(pred - true_answer) <= 0.1 else \"Incorrect\"\n\n    #     # Prepare output for file\n    #     output_lines.append(f\"Converted Question {i+1}: {converted_question}\")\n    #     output_lines.append(f\"True Answer: {true_answer}\")\n    #     output_lines.append(f\"Decoded Answer: {pred}\")\n    #     output_lines.append(f\"Predicted Result: {result_comparison}\")\n    #     output_lines.append(\"-\" * 80)\n    #     break\n        \nprint(output_lines)\n# Write all collected lines to eval.txt\n# write_to_file(\"gts_eval_roberta.txt\", output_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T12:23:27.282376Z","iopub.status.idle":"2024-11-15T12:23:27.282857Z","shell.execute_reply.started":"2024-11-15T12:23:27.282592Z","shell.execute_reply":"2024-11-15T12:23:27.282619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(test_pairs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T12:23:27.284369Z","iopub.status.idle":"2024-11-15T12:23:27.284866Z","shell.execute_reply.started":"2024-11-15T12:23:27.284590Z","shell.execute_reply":"2024-11-15T12:23:27.284615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for test_batch in test_pairs:\n                # test_res = evaluate_tree(test_batch[0], test_batch[1], generate_num_ids, encoder, predict, generate,\n                # \t\t\t\t\t\t merge, output_lang, test_batch[5], beam_size=config.beam_size)\n                test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                         merge, input_lang, output_lang, test_batch[5], beam_size=config.beam_size)\n                val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n                cur_result = 0\n                if val_ac:\n                    value_ac += 1\n                    cur_result = 1\n                if equ_ac:","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T12:23:27.286515Z","iopub.status.idle":"2024-11-15T12:23:27.286877Z","shell.execute_reply.started":"2024-11-15T12:23:27.286691Z","shell.execute_reply":"2024-11-15T12:23:27.286720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"                    equation_ac += 1\n                eval_total += 1\n\n                with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                    f_out.write('Example: ' + str(ex_num) + '\\n')\n                    f_out.write('Source: ' + stack_to_string(sentence_from_indexes(input_lang, test_batch[0])) + '\\n')\n                    f_out.write('Target: ' + stack_to_string(sentence_from_indexes(output_lang, test_batch[2])) + '\\n')\n                    f_out.write('Generated: ' + stack_to_string(sentence_from_indexes(output_lang, test_res)) + '\\n')\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T12:23:27.287907Z","iopub.status.idle":"2024-11-15T12:23:27.288236Z","shell.execute_reply.started":"2024-11-15T12:23:27.288068Z","shell.execute_reply":"2024-11-15T12:23:27.288084Z"}},"outputs":[],"execution_count":null}]}