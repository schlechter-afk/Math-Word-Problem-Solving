{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9917101,"sourceType":"datasetVersion","datasetId":6094413}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nfrom transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\nimport pdb\nfrom torch.nn import functional\n\nfrom gensim import models\nimport torch.nn.functional as F\nimport copy\nimport math\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.module import Module\n\nimport logging\nfrom glob import glob\nfrom torch.autograd import Variable\nimport numpy as np\nimport os\nimport sys\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom copy import deepcopy\nimport re\n\nimport pandas as pd\nimport json\n\nimport random\nimport copy\nimport nltk\nimport argparse\n\nimport time\nimport torch.optim\nfrom collections import OrderedDict\ntry:\n\timport cPickle as pickle\nexcept ImportError:\n\timport pickle\n    \nimport math\nimport torch.nn.functional as f","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-16T10:53:04.278690Z","iopub.execute_input":"2024-11-16T10:53:04.279174Z","iopub.status.idle":"2024-11-16T10:53:04.289868Z","shell.execute_reply.started":"2024-11-16T10:53:04.279133Z","shell.execute_reply":"2024-11-16T10:53:04.288714Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Components","metadata":{}},{"cell_type":"code","source":"#########################################\n# contextual_embeddings.py\n#########################################\n\nclass BertEncoder(nn.Module):\n\tdef __init__(self, bert_model = 'bert-base-uncased',device = 'cuda:0 ', freeze_bert = False):\n\t\tsuper(BertEncoder, self).__init__()\n\t\tself.bert_layer = BertModel.from_pretrained(bert_model)\n\t\tself.bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n\t\tself.device = device\n\t\t\n\t\tif freeze_bert:\n\t\t\tfor p in self.bert_layer.parameters():\n\t\t\t\tp.requires_grad = False\n\t\t\n\tdef bertify_input(self, sentences):\n\t\t'''\n\t\tPreprocess the input sentences using bert tokenizer and converts them to a torch tensor containing token ids\n\n\t\t'''\n\t\t#Tokenize the input sentences for feeding into BERT\n\t\t# pdb.set_trace()\n\t\tall_tokens  = [['[CLS]'] + self.bert_tokenizer.tokenize(sentence) + ['[SEP]'] for sentence in sentences]\n\n\t\tindex_retrieve = []\n\t\tfor sent in all_tokens:\n\t\t\tcur_ls = []\n\t\t\tfor j in range(1, len(sent)):\n\t\t\t\tif sent[j][0] == '#':\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\tcur_ls.append(j)\n\t\t\tindex_retrieve.append(cur_ls)\n\t\t\n\t\t#Pad all the sentences to a maximum length\n\t\tinput_lengths = [len(tokens) for tokens in all_tokens]\n\t\tmax_length    = max(input_lengths)\n\t\tpadded_tokens = [tokens + ['[PAD]' for _ in range(max_length - len(tokens))] for tokens in all_tokens]\n\n\t\t#Convert tokens to token ids\n\t\ttoken_ids = torch.tensor([self.bert_tokenizer.convert_tokens_to_ids(tokens) for tokens in padded_tokens]).to(self.device)\n\n\t\t#Obtain attention masks\n\t\tpad_token = self.bert_tokenizer.convert_tokens_to_ids('[PAD]')\n\t\tattn_masks = (token_ids != pad_token).long()\n\n\t\treturn token_ids, attn_masks, input_lengths, index_retrieve\n\n\tdef forward(self, sentences):\n\t\t'''\n\t\tFeed the batch of sentences to a BERT encoder to obtain contextualized representations of each token\n\t\t'''\n\t\t#Preprocess sentences\n\t\ttoken_ids, attn_masks, input_lengths, index_retrieve = self.bertify_input(sentences)\n\n\t\t#Feed through bert\n\t\tcont_reps, _ = self.bert_layer(token_ids, attention_mask = attn_masks)\n\n\t\treturn cont_reps, input_lengths, token_ids, index_retrieve\n\nclass RobertaEncoder(nn.Module):\n\tdef __init__(self, roberta_model = 'roberta-base', device = 'cuda:0 ', freeze_roberta = False):\n\t\tsuper(RobertaEncoder, self).__init__()\n\t\tself.roberta_layer = RobertaModel.from_pretrained(roberta_model)\n\t\tself.roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model)\n\t\tself.device = device\n\t\t\n\t\tif freeze_roberta:\n\t\t\tfor p in self.roberta_layer.parameters():\n\t\t\t\tp.requires_grad = False\n\t\t\n\tdef robertify_input(self, sentences):\n\t\t'''\n\t\tPreprocess the input sentences using roberta tokenizer and converts them to a torch tensor containing token ids\n\n\t\t'''\n\t\t# Tokenize the input sentences for feeding into RoBERTa\n\t\tall_tokens  = [['<s>'] + self.roberta_tokenizer.tokenize(sentence) + ['</s>'] for sentence in sentences]\n\t\t\n\t\tindex_retrieve = []\n\t\tfor sent in all_tokens:\n\t\t\tcur_ls = [1]\n\t\t\tfor j in range(2, len(sent)):\n\t\t\t\tif sent[j][0] == '\\u0120':\n\t\t\t\t\tcur_ls.append(j)\n\t\t\tindex_retrieve.append(cur_ls)\t\t\t\t\n\t\t\n\t\t# Pad all the sentences to a maximum length\n\t\tinput_lengths = [len(tokens) for tokens in all_tokens]\n\t\tmax_length    = max(input_lengths)\n\t\tpadded_tokens = [tokens + ['<pad>' for _ in range(max_length - len(tokens))] for tokens in all_tokens]\n\n\t\t# Convert tokens to token ids\n\t\ttoken_ids = torch.tensor([self.roberta_tokenizer.convert_tokens_to_ids(tokens) for tokens in padded_tokens]).to(self.device)\n\n\t\t# Obtain attention masks\n\t\tpad_token = self.roberta_tokenizer.convert_tokens_to_ids('<pad>')\n\t\tattn_masks = (token_ids != pad_token).long()\n\n\t\treturn token_ids, attn_masks, input_lengths, index_retrieve\n\n\tdef forward(self, sentences):\n\t\t'''\n\t\tFeed the batch of sentences to a RoBERTa encoder to obtain contextualized representations of each token\n\t\t'''\n\t\t# Preprocess sentences\n\t\ttoken_ids, attn_masks, input_lengths, index_retrieve = self.robertify_input(sentences)\n\n\t\t# Feed through RoBERTa\n\t\tcont_reps = self.roberta_layer(token_ids, attention_mask = attn_masks)\n\t\tcont_reps = cont_reps.last_hidden_state\n\t\treturn cont_reps, input_lengths, token_ids, index_retrieve\n    \n#########################################\n# masked_cross_entropy.py\n#########################################\n\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.data.max()\n    batch_size = sequence_length.size(0)\n    seq_range = torch.arange(0, max_len).long()\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    if sequence_length.is_cuda:\n        seq_range_expand = seq_range_expand.cuda()\n    seq_length_expand = (sequence_length.unsqueeze(1).expand_as(seq_range_expand))\n    return seq_range_expand < seq_length_expand\n\n\ndef masked_cross_entropy(logits, target, length):\n    if torch.cuda.is_available():\n        length = torch.LongTensor(length).cuda()\n    else:\n        length = torch.LongTensor(length)\n    \"\"\"\n    Args:\n        logits: A Variable containing a FloatTensor of size\n            (batch, max_len, num_classes) which contains the\n            unnormalized probability for each class.\n        target: A Variable containing a LongTensor of size\n            (batch, max_len) which contains the index of the true\n            class for each corresponding step.\n        length: A Variable containing a LongTensor of size (batch,)\n            which contains the length of each data in a batch.\n    Returns:\n        loss: An average loss value masked by the length.\n    \"\"\"\n\n    # logits_flat: (batch * max_len, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # log_probs_flat: (batch * max_len, num_classes)\n    log_probs_flat = functional.log_softmax(logits_flat, dim=1)\n    # target_flat: (batch * max_len, 1)\n    target_flat = target.view(-1, 1)\n    # losses_flat: (batch * max_len, 1)\n    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n\n    # losses: (batch, max_len)\n    losses = losses_flat.view(*target.size())\n    # mask: (batch, max_len)\n    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n    losses = losses * mask.float()\n    loss = losses.sum() / length.float().sum()\n    # if loss.item() > 10:\n    #     print(losses, target)\n    return loss\n\n\ndef masked_cross_entropy_without_logit(logits, target, length):\n    if torch.cuda.is_available():\n        length = torch.LongTensor(length).cuda()\n    else:\n        length = torch.LongTensor(length)\n    \"\"\"\n    Args:\n        logits: A Variable containing a FloatTensor of size\n            (batch, max_len, num_classes) which contains the\n            unnormalized probability for each class.\n        target: A Variable containing a LongTensor of size\n            (batch, max_len) which contains the index of the true\n            class for each corresponding step.\n        length: A Variable containing a LongTensor of size (batch,)\n            which contains the length of each data in a batch.\n    Returns:\n        loss: An average loss value masked by the length.\n    \"\"\"\n\n    # logits_flat: (batch * max_len, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n\n    # log_probs_flat: (batch * max_len, num_classes)\n    log_probs_flat = torch.log(logits_flat + 1e-12)\n\n    # target_flat: (batch * max_len, 1)\n    target_flat = target.view(-1, 1)\n    # losses_flat: (batch * max_len, 1)\n    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n\n    # losses: (batch, max_len)\n    losses = losses_flat.view(*target.size())\n\n    # mask: (batch, max_len)\n    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n    losses = losses * mask.float()\n    loss = losses.sum() / length.float().sum()\n    # if loss.item() > 10:\n    #     print(losses, target)\n    return loss\n\n\n#########################################\n# models.py\n#########################################\n\nclass Embedding(nn.Module):\n\tdef __init__(self, config, input_lang, input_size, embedding_size, dropout=0.5):\n\t\tsuper(Embedding, self).__init__()\n\n\t\tself.config = config\n\t\tself.input_lang = input_lang\n\t\tself.input_size = input_size\n\t\tself.embedding_size = embedding_size\n\n\t\tif self.config.embedding == 'word2vec':\n\t\t\tself.config.embedding_size = 300\n\t\t\tself.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self._form_embeddings(self.config.word2vec_bin)), freeze = self.config.freeze_emb)\n\t\telse:\n\t\t\tself.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n\t\tself.em_dropout = nn.Dropout(dropout)\n\n\tdef _form_embeddings(self, file_path):\n\t\tweights_all = models.KeyedVectors.load_word2vec_format(file_path, limit=200000, binary=True)\n\t\tweight_req  = torch.randn(self.input_size, self.config.embedding_size)\n\t\tfor temp_ind in range(len(self.input_lang.index2word)):\n\t\t\tvalue = self.input_lang.index2word[temp_ind]\n\t\t\tif value in weights_all:\n\t\t\t\tweight_req[temp_ind] = torch.FloatTensor(weights_all[value])\n\t\t# for key, value in self.voc1.id2w.items():\n\t\t# \tif value in weights_all:\n\t\t# \t\tweight_req[key] = torch.FloatTensor(weights_all[value])\n\n\t\treturn weight_req\n\n\tdef forward(self, input_seqs):\n\t\tembedded = self.embedding(input_seqs)  # S x B x E\n\t\tembedded = self.em_dropout(embedded)\n\t\treturn embedded\n\nclass EncoderRNN(nn.Module):\n\tdef __init__(self, input_size, embedding_size, hidden_size, n_layers=2, dropout=0.5):\n\t\tsuper(EncoderRNN, self).__init__()\n\n\t\tself.input_size = input_size\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\t\tself.n_layers = n_layers\n\t\tself.dropout = dropout\n\n\t\tself.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n\t\tself.em_dropout = nn.Dropout(dropout)\n\t\tself.gru = nn.GRU(embedding_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n\n\tdef forward(self, input_seqs, input_lengths, hidden=None):\n\t\t# Note: we run this all at once (over multiple batches of multiple sequences)\n\t\tembedded = self.embedding(input_seqs)  # S x B x E\n\t\tembedded = self.em_dropout(embedded)\n\t\tpacked = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n\t\toutputs, hidden = self.gru(packed, hidden)\n\t\toutputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n\t\toutputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n\t\t# S x B x H\n\t\treturn outputs, hidden\n\n\nclass Attn(nn.Module):\n\tdef __init__(self, hidden_size):\n\t\tsuper(Attn, self).__init__()\n\t\tself.hidden_size = hidden_size\n\t\tself.attn = nn.Linear(hidden_size * 2, hidden_size)\n\t\tself.score = nn.Linear(hidden_size, 1, bias=False)\n\t\tself.softmax = nn.Softmax(dim=1)\n\n\tdef forward(self, hidden, encoder_outputs, seq_mask=None):\n\t\tmax_len = encoder_outputs.size(0)\n\t\trepeat_dims = [1] * hidden.dim()\n\t\trepeat_dims[0] = max_len\n\t\thidden = hidden.repeat(*repeat_dims)  # S x B x H\n\t\t# For each position of encoder outputs\n\t\tthis_batch_size = encoder_outputs.size(1)\n\t\tenergy_in = torch.cat((hidden, encoder_outputs), 2).view(-1, 2 * self.hidden_size)\n\t\tattn_energies = self.score(torch.tanh(self.attn(energy_in)))  # (S x B) x 1\n\t\tattn_energies = attn_energies.squeeze(1)\n\t\tattn_energies = attn_energies.view(max_len, this_batch_size).transpose(0, 1)  # B x S\n\t\tif seq_mask is not None:\n\t\t\tattn_energies = attn_energies.masked_fill_(seq_mask, -1e12)\n\t\tattn_energies = self.softmax(attn_energies)\n\t\t# Normalize energies to weights in range 0 to 1, resize to B x 1 x S\n\t\treturn attn_energies.unsqueeze(1)\n\n\nclass AttnDecoderRNN(nn.Module):\n\tdef __init__(\n\t\t\tself, hidden_size, embedding_size, input_size, output_size, n_layers=2, dropout=0.5):\n\t\tsuper(AttnDecoderRNN, self).__init__()\n\n\t\t# Keep for reference\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\t\tself.input_size = input_size\n\t\tself.output_size = output_size\n\t\tself.n_layers = n_layers\n\t\tself.dropout = dropout\n\n\t\t# Define layers\n\t\tself.em_dropout = nn.Dropout(dropout)\n\t\tself.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n\t\tself.gru = nn.GRU(hidden_size + embedding_size, hidden_size, n_layers, dropout=dropout)\n\t\tself.concat = nn.Linear(hidden_size * 2, hidden_size)\n\t\tself.out = nn.Linear(hidden_size, output_size)\n\t\t# Choose attention model\n\t\tself.attn = Attn(hidden_size)\n\n\tdef forward(self, input_seq, last_hidden, encoder_outputs, seq_mask):\n\t\t# Get the embedding of the current input word (last output word)\n\t\tbatch_size = input_seq.size(0)\n\t\tembedded = self.embedding(input_seq)\n\t\tembedded = self.em_dropout(embedded)\n\t\tembedded = embedded.view(1, batch_size, self.embedding_size)  # S=1 x B x N\n\n\t\t# Calculate attention from current RNN state and all encoder outputs;\n\t\t# apply to encoder outputs to get weighted average\n\t\tattn_weights = self.attn(last_hidden[-1].unsqueeze(0), encoder_outputs, seq_mask)\n\t\tcontext = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B x S=1 x N\n\n\t\t# Get current hidden state from input word and last hidden state\n\t\trnn_output, hidden = self.gru(torch.cat((embedded, context.transpose(0, 1)), 2), last_hidden)\n\n\t\t# Attentional vector using the RNN hidden state and context vector\n\t\t# concatenated together (Luong eq. 5)\n\t\toutput = self.out(torch.tanh(self.concat(torch.cat((rnn_output.squeeze(0), context.squeeze(1)), 1))))\n\n\t\t# Return final output, hidden state\n\t\treturn output, hidden\n\n\nclass TreeNode:  # the class save the tree node\n\tdef __init__(self, embedding, left_flag=False):\n\t\tself.embedding = embedding\n\t\tself.left_flag = left_flag\n\n\nclass Score(nn.Module):\n\tdef __init__(self, input_size, hidden_size):\n\t\tsuper(Score, self).__init__()\n\t\tself.input_size = input_size\n\t\tself.hidden_size = hidden_size\n\t\tself.attn = nn.Linear(hidden_size + input_size, hidden_size)\n\t\tself.score = nn.Linear(hidden_size, 1, bias=False)\n\n\tdef forward(self, hidden, num_embeddings, num_mask=None):\n\t\tmax_len = num_embeddings.size(1)\n\t\trepeat_dims = [1] * hidden.dim()\n\t\trepeat_dims[1] = max_len\n\t\thidden = hidden.repeat(*repeat_dims)  # B x O x H\n\t\t# For each position of encoder outputs\n\t\tthis_batch_size = num_embeddings.size(0)\n\t\tenergy_in = torch.cat((hidden, num_embeddings), 2).view(-1, self.input_size + self.hidden_size)\n\t\tscore = self.score(torch.tanh(self.attn(energy_in)))  # (B x O) x 1\n\t\tscore = score.squeeze(1)\n\t\tscore = score.view(this_batch_size, -1)  # B x O\n\t\tif num_mask is not None:\n\t\t\tscore = score.masked_fill_(num_mask, -1e12)\n\t\treturn score\n\n\nclass TreeAttn(nn.Module):\n\tdef __init__(self, input_size, hidden_size):\n\t\tsuper(TreeAttn, self).__init__()\n\t\tself.input_size = input_size\n\t\tself.hidden_size = hidden_size\n\t\tself.attn = nn.Linear(hidden_size + input_size, hidden_size)\n\t\tself.score = nn.Linear(hidden_size, 1)\n\n\tdef forward(self, hidden, encoder_outputs, seq_mask=None):\n\t\tmax_len = encoder_outputs.size(0)\n\n\t\trepeat_dims = [1] * hidden.dim()\n\t\trepeat_dims[0] = max_len\n\t\thidden = hidden.repeat(*repeat_dims)  # S x B x H\n\t\tthis_batch_size = encoder_outputs.size(1)\n\n\t\tenergy_in = torch.cat((hidden, encoder_outputs), 2).view(-1, self.input_size + self.hidden_size)\n\n\t\tscore_feature = torch.tanh(self.attn(energy_in))\n\t\tattn_energies = self.score(score_feature)  # (S x B) x 1\n\t\tattn_energies = attn_energies.squeeze(1)\n\t\tattn_energies = attn_energies.view(max_len, this_batch_size).transpose(0, 1)  # B x S\n\t\tif seq_mask is not None:\n\t\t\tattn_energies = attn_energies.masked_fill_(seq_mask, -1e12)\n\t\tattn_energies = nn.functional.softmax(attn_energies, dim=1)  # B x S\n\n\t\treturn attn_energies.unsqueeze(1)\n\n\nclass EncoderSeq(nn.Module):\n\t# def __init__(self, input_size, embedding_size, hidden_size, n_layers=2, dropout=0.5):\n\tdef __init__(self, cell_type, embedding_size, hidden_size, n_layers=2, dropout=0.5):\n\t\tsuper(EncoderSeq, self).__init__()\n\n\t\t# self.input_size = input_size\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\t\tself.n_layers = n_layers\n\t\tself.dropout = dropout\n\n\t\t# self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=0)\n\t\t# self.em_dropout = nn.Dropout(dropout)\n\n\t\tif cell_type == 'lstm':\n\t\t\tself.rnn = nn.LSTM(self.embedding_size, self.hidden_size,\n\t\t\t\t\t\t\t   num_layers=self.n_layers,\n\t\t\t\t\t\t\t   dropout=(0 if self.n_layers == 1 else self.dropout),\n\t\t\t\t\t\t\t   bidirectional=True)\n\t\telif cell_type == 'gru':\n\t\t\tself.rnn = nn.GRU(embedding_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n\t\telse:\n\t\t\tself.rnn = nn.RNN(self.embedding_size, self.hidden_size,\n\t\t\t\t\t\t\t  num_layers=self.n_layers,\n\t\t\t\t\t\t\t  nonlinearity='tanh',\t\t\t\t\t\t\t# ['relu', 'tanh']\n\t\t\t\t\t\t\t  dropout=(0 if self.n_layers == 1 else self.dropout),\n\t\t\t\t\t\t\t  bidirectional=True)\n\n\t\tself.gcn = Graph_Module(hidden_size, hidden_size, hidden_size)\n\n\tdef forward(self, embedded, input_lengths, orig_idx, batch_graph, hidden=None):\n\t\t# Note: we run this all at once (over multiple batches of multiple sequences)\n\t\t# embedded = self.embedding(input_seqs)  # S x B x E\n\t\t# embedded = self.em_dropout(embedded)\n\t\tpacked = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n\t\tpade_hidden = hidden\n\t\t# pade_outputs, pade_hidden = self.gru_pade(packed, pade_hidden)\n\t\tpade_outputs, pade_hidden = self.rnn(packed, pade_hidden)\n\t\tpade_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(pade_outputs)\n\n\t\tif orig_idx is not None:\n\t\t\tpade_outputs = pade_outputs.index_select(1, orig_idx)\n\n\t\tproblem_output = pade_outputs[-1, :, :self.hidden_size] + pade_outputs[0, :, self.hidden_size:]\n\t\tpade_outputs = pade_outputs[:, :, :self.hidden_size] + pade_outputs[:, :, self.hidden_size:]  # S x B x H\n\t\t# pdb.set_trace()\n\t\t_, pade_outputs = self.gcn(pade_outputs, batch_graph)\n\t\tpade_outputs = pade_outputs.transpose(0, 1)\n\t\treturn pade_outputs, problem_output\n\n\nclass Prediction(nn.Module):\n\t# a seq2tree decoder with Problem aware dynamic encoding\n\n\tdef __init__(self, hidden_size, op_nums, input_size, dropout=0.5):\n\t\tsuper(Prediction, self).__init__()\n\n\t\t# Keep for reference\n\t\tself.hidden_size = hidden_size\n\t\tself.input_size = input_size\n\t\tself.op_nums = op_nums\n\n\t\t# Define layers\n\t\tself.dropout = nn.Dropout(dropout)\n\n\t\tself.embedding_weight = nn.Parameter(torch.randn(1, input_size, hidden_size))\n\n\t\t# for Computational symbols and Generated numbers\n\t\tself.concat_l = nn.Linear(hidden_size, hidden_size)\n\t\tself.concat_r = nn.Linear(hidden_size * 2, hidden_size)\n\t\tself.concat_lg = nn.Linear(hidden_size, hidden_size)\n\t\tself.concat_rg = nn.Linear(hidden_size * 2, hidden_size)\n\n\t\tself.ops = nn.Linear(hidden_size * 2, op_nums)\n\n\t\tself.attn = TreeAttn(hidden_size, hidden_size)\n\t\tself.score = Score(hidden_size * 2, hidden_size)\n\n\tdef forward(self, node_stacks, left_childs, encoder_outputs, num_pades, padding_hidden, seq_mask, mask_nums):\n\t\tcurrent_embeddings = []\n\n\t\tfor st in node_stacks:\n\t\t\tif len(st) == 0:\n\t\t\t\tcurrent_embeddings.append(padding_hidden)\n\t\t\telse:\n\t\t\t\tcurrent_node = st[-1]\n\t\t\t\tcurrent_embeddings.append(current_node.embedding)\n\n\t\tcurrent_node_temp = []\n\t\tfor l, c in zip(left_childs, current_embeddings):\n\t\t\tif l is None:\n\t\t\t\tc = self.dropout(c)\n\t\t\t\tg = torch.tanh(self.concat_l(c))\n\t\t\t\tt = torch.sigmoid(self.concat_lg(c))\n\t\t\t\tcurrent_node_temp.append(g * t)\n\t\t\telse:\n\t\t\t\tld = self.dropout(l)\n\t\t\t\tc = self.dropout(c)\n\t\t\t\tg = torch.tanh(self.concat_r(torch.cat((ld, c), 1)))\n\t\t\t\tt = torch.sigmoid(self.concat_rg(torch.cat((ld, c), 1)))\n\t\t\t\tcurrent_node_temp.append(g * t)\n\n\t\tcurrent_node = torch.stack(current_node_temp)\n\n\t\tcurrent_embeddings = self.dropout(current_node)\n\n\t\tcurrent_attn = self.attn(current_embeddings.transpose(0, 1), encoder_outputs, seq_mask)\n\t\tcurrent_context = current_attn.bmm(encoder_outputs.transpose(0, 1))  # B x 1 x N\n\n\t\t# the information to get the current quantity\n\t\tbatch_size = current_embeddings.size(0)\n\t\t# predict the output (this node corresponding to output(number or operator)) with PADE\n\n\t\trepeat_dims = [1] * self.embedding_weight.dim()\n\t\trepeat_dims[0] = batch_size\n\t\tembedding_weight = self.embedding_weight.repeat(*repeat_dims)  # B x input_size x N\n\t\tembedding_weight = torch.cat((embedding_weight, num_pades), dim=1)  # B x O x N\n\n\t\tleaf_input = torch.cat((current_node, current_context), 2)\n\t\tleaf_input = leaf_input.squeeze(1)\n\t\tleaf_input = self.dropout(leaf_input)\n\n\t\t# p_leaf = nn.functional.softmax(self.is_leaf(leaf_input), 1)\n\t\t# max pooling the embedding_weight\n\t\tembedding_weight_ = self.dropout(embedding_weight)\n\t\tnum_score = self.score(leaf_input.unsqueeze(1), embedding_weight_, mask_nums)\n\n\t\t# num_score = nn.functional.softmax(num_score, 1)\n\n\t\top = self.ops(leaf_input)\n\n\t\t# return p_leaf, num_score, op, current_embeddings, current_attn\n\n\t\treturn num_score, op, current_node, current_context, embedding_weight\n\n\nclass GenerateNode(nn.Module):\n\tdef __init__(self, hidden_size, op_nums, embedding_size, dropout=0.5):\n\t\tsuper(GenerateNode, self).__init__()\n\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\n\t\tself.embeddings = nn.Embedding(op_nums, embedding_size)\n\t\tself.em_dropout = nn.Dropout(dropout)\n\t\tself.generate_l = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\t\tself.generate_r = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\t\tself.generate_lg = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\t\tself.generate_rg = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\n\tdef forward(self, node_embedding, node_label, current_context):\n\t\tnode_label_ = self.embeddings(node_label)\n\t\tnode_label = self.em_dropout(node_label_)\n\t\tnode_embedding = node_embedding.squeeze(1)\n\t\tcurrent_context = current_context.squeeze(1)\n\t\tnode_embedding = self.em_dropout(node_embedding)\n\t\tcurrent_context = self.em_dropout(current_context)\n\n\t\tl_child = torch.tanh(self.generate_l(torch.cat((node_embedding, current_context, node_label), 1)))\n\t\tl_child_g = torch.sigmoid(self.generate_lg(torch.cat((node_embedding, current_context, node_label), 1)))\n\t\tr_child = torch.tanh(self.generate_r(torch.cat((node_embedding, current_context, node_label), 1)))\n\t\tr_child_g = torch.sigmoid(self.generate_rg(torch.cat((node_embedding, current_context, node_label), 1)))\n\t\tl_child = l_child * l_child_g\n\t\tr_child = r_child * r_child_g\n\t\treturn l_child, r_child, node_label_\n\n\nclass Merge(nn.Module):\n\tdef __init__(self, hidden_size, embedding_size, dropout=0.5):\n\t\tsuper(Merge, self).__init__()\n\n\t\tself.embedding_size = embedding_size\n\t\tself.hidden_size = hidden_size\n\n\t\tself.em_dropout = nn.Dropout(dropout)\n\t\tself.merge = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\t\tself.merge_g = nn.Linear(hidden_size * 2 + embedding_size, hidden_size)\n\n\tdef forward(self, node_embedding, sub_tree_1, sub_tree_2):\n\t\tsub_tree_1 = self.em_dropout(sub_tree_1)\n\t\tsub_tree_2 = self.em_dropout(sub_tree_2)\n\t\tnode_embedding = self.em_dropout(node_embedding)\n\n\t\tsub_tree = torch.tanh(self.merge(torch.cat((node_embedding, sub_tree_1, sub_tree_2), 1)))\n\t\tsub_tree_g = torch.sigmoid(self.merge_g(torch.cat((node_embedding, sub_tree_1, sub_tree_2), 1)))\n\t\tsub_tree = sub_tree * sub_tree_g\n\t\treturn sub_tree\n\t\n\t\n\t\n# Graph Module\ndef clones(module, N):\n\t\"Produce N identical layers.\"\n\treturn nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\nclass LayerNorm(nn.Module):\n\t\"Construct a layernorm module (See citation for details).\"\n\tdef __init__(self, features, eps=1e-6):\n\t\tsuper(LayerNorm, self).__init__()\n\t\tself.a_2 = nn.Parameter(torch.ones(features))\n\t\tself.b_2 = nn.Parameter(torch.zeros(features))\n\t\tself.eps = eps\n\n\tdef forward(self, x):\n\t\tmean = x.mean(-1, keepdim=True)\n\t\tstd = x.std(-1, keepdim=True)\n\t\treturn self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\nclass PositionwiseFeedForward(nn.Module):\n\t\"Implements FFN equation.\"\n\tdef __init__(self, d_model, d_ff,d_out, dropout=0.1):\n\t\tsuper(PositionwiseFeedForward, self).__init__()\n\t\tself.w_1 = nn.Linear(d_model, d_ff)\n\t\tself.w_2 = nn.Linear(d_ff, d_out)\n\t\tself.dropout = nn.Dropout(dropout)\n\n\tdef forward(self, x):\n\t\treturn self.w_2(self.dropout(F.relu(self.w_1(x))))\n\nclass Graph_Module(nn.Module):\n\tdef __init__(self, indim, hiddim, outdim, dropout=0.3):\n\t\tsuper(Graph_Module, self).__init__()\n\t\t'''\n\t\t## Variables:\n\t\t- indim: dimensionality of input node features\n\t\t- hiddim: dimensionality of the joint hidden embedding\n\t\t- outdim: dimensionality of the output node features\n\t\t- combined_feature_dim: dimensionality of the joint hidden embedding for graph\n\t\t- K: number of graph nodes/objects on the image\n\t\t'''\n\t\tself.in_dim = indim\n\t\t#self.combined_dim = outdim\n\t\t\n\t\t#self.edge_layer_1 = nn.Linear(indim, outdim)\n\t\t#self.edge_layer_2 = nn.Linear(outdim, outdim)\n\t\t\n\t\t#self.dropout = nn.Dropout(p=dropout)\n\t\t#self.edge_layer_1 = nn.utils.weight_norm(self.edge_layer_1)\n\t\t#self.edge_layer_2 = nn.utils.weight_norm(self.edge_layer_2)\n\t\tself.h = 4\n\t\tself.d_k = outdim//self.h\n\t\t\n\t\t#layer = GCN(indim, hiddim, self.d_k, dropout)\n\t\tself.graph = clones(GCN(indim, hiddim, self.d_k, dropout), 4)\n\t\t\n\t\t#self.Graph_0 = GCN(indim, hiddim, outdim//4, dropout)\n\t\t#self.Graph_1 = GCN(indim, hiddim, outdim//4, dropout)\n\t\t#self.Graph_2 = GCN(indim, hiddim, outdim//4, dropout)\n\t\t#self.Graph_3 = GCN(indim, hiddim, outdim//4, dropout)\n\t\t\n\t\tself.feed_foward = PositionwiseFeedForward(indim, hiddim, outdim, dropout)\n\t\tself.norm = LayerNorm(outdim)\n\n\tdef get_adj(self, graph_nodes):\n\t\t'''\n\t\t## Inputs:\n\t\t- graph_nodes (batch_size, K, in_feat_dim): input features\n\t\t## Returns:\n\t\t- adjacency matrix (batch_size, K, K)\n\t\t'''\n\t\tself.K = graph_nodes.size(1)\n\t\tgraph_nodes = graph_nodes.contiguous().view(-1, self.in_dim)\n\t\t\n\t\t# layer 1\n\t\th = self.edge_layer_1(graph_nodes)\n\t\th = F.relu(h)\n\t\t\n\t\t# layer 2\n\t\th = self.edge_layer_2(h)\n\t\th = F.relu(h)\n\n\t\t# outer product\n\t\th = h.view(-1, self.K, self.combined_dim)\n\t\tadjacency_matrix = torch.matmul(h, h.transpose(1, 2))\n\t\t\n\t\tadjacency_matrix = self.b_normal(adjacency_matrix)\n\n\t\treturn adjacency_matrix\n\t\n\tdef normalize(self, A, symmetric=True):\n\t\t'''\n\t\t## Inputs:\n\t\t- adjacency matrix (K, K) : A\n\t\t## Returns:\n\t\t- adjacency matrix (K, K) \n\t\t'''\n\t\tA = A + torch.eye(A.size(0)).cuda().float()\n\t\td = A.sum(1)\n\t\tif symmetric:\n\t\t\t# D = D^{-1/2}\n\t\t\tD = torch.diag(torch.pow(d, -0.5))\n\t\t\treturn D.mm(A).mm(D)\n\t\telse :\n\t\t\tD = torch.diag(torch.pow(d,-1))\n\t\t\treturn D.mm(A)\n\t   \n\tdef b_normal(self, adj):\n\t\tbatch = adj.size(0)\n\t\tfor i in range(batch):\n\t\t\tadj[i] = self.normalize(adj[i])\n\t\treturn adj\n\n\tdef forward(self, graph_nodes, graph):\n\t\t'''\n\t\t## Inputs:\n\t\t- graph_nodes (batch_size, K, in_feat_dim): input features\n\t\t## Returns:\n\t\t- graph_encode_features (batch_size, K, out_feat_dim)\n\t\t'''\n\t\tnbatches = graph_nodes.size(0)\n\t\tmbatches = graph.size(0)\n\t\tif nbatches != mbatches:\n\t\t\tgraph_nodes = graph_nodes.transpose(0, 1)\n\t\t# adj (batch_size, K, K): adjacency matrix\n\t\tif not bool(graph.numel()):\n\t\t\tadj = self.get_adj(graph_nodes)\n\t\t\t#adj = adj.unsqueeze(1)\n\t\t\t#adj = torch.cat((adj,adj,adj),1)\n\t\t\tadj_list = [adj,adj,adj,adj]\n\t\telse:\n\t\t\tadj = graph.float()\n\t\t\tadj_list = [adj[:,1,:],adj[:,1,:],adj[:,4,:],adj[:,4,:]]\n\t\t#print(adj)\n\t\t\n\t\tg_feature = \\\n\t\t\ttuple([l(graph_nodes,x) for l, x in zip(self.graph,adj_list)])\n\t\t#g_feature_0 = self.Graph_0(graph_nodes,adj[0])\n\t\t#g_feature_1 = self.Graph_1(graph_nodes,adj[1])\n\t\t#g_feature_2 = self.Graph_2(graph_nodes,adj[2])\n\t\t#g_feature_3 = self.Graph_3(graph_nodes,adj[3])\n\t\t#print('g_feature')\n\t\t#print(type(g_feature))\n\t\t\n\t\t\n\t\tg_feature = self.norm(torch.cat(g_feature,2)) + graph_nodes\n\t\t#print('g_feature')\n\t\t#print(g_feature.shape)\n\t\t\n\t\tgraph_encode_features = self.feed_foward(g_feature) + g_feature\n\t\t\n\t\treturn adj, graph_encode_features\n\n# GCN\nclass GCN(nn.Module):\n\tdef __init__(self, in_feat_dim, nhid, out_feat_dim, dropout):\n\t\tsuper(GCN, self).__init__()\n\t\t'''\n\t\t## Inputs:\n\t\t- graph_nodes (batch_size, K, in_feat_dim): input features\n\t\t- adjacency matrix (batch_size, K, K)\n\t\t## Returns:\n\t\t- gcn_enhance_feature (batch_size, K, out_feat_dim)\n\t\t'''\n\t\tself.gc1 = GraphConvolution(in_feat_dim, nhid)\n\t\tself.gc2 = GraphConvolution(nhid, out_feat_dim)\n\t\tself.dropout = dropout\n\n\tdef forward(self, x, adj):\n\t\tx = F.relu(self.gc1(x, adj))\n\t\tx = F.dropout(x, self.dropout, training=self.training)\n\t\tx = self.gc2(x, adj)\n\t\treturn x\n\t\n# Graph_Conv\nclass GraphConvolution(Module):\n\t\"\"\"\n\tSimple GCN layer, similar to https://arxiv.org/abs/1609.02907\n\t\"\"\"\n\n\tdef __init__(self, in_features, out_features, bias=True):\n\t\tsuper(GraphConvolution, self).__init__()\n\t\tself.in_features = in_features\n\t\tself.out_features = out_features\n\t\tself.weight = Parameter(torch.FloatTensor(in_features, out_features))\n\t\tif bias:\n\t\t\tself.bias = Parameter(torch.FloatTensor(out_features))\n\t\telse:\n\t\t\tself.register_parameter('bias', None)\n\t\tself.reset_parameters()\n\n\tdef reset_parameters(self):\n\t\tstdv = 1. / math.sqrt(self.weight.size(1))\n\t\tself.weight.data.uniform_(-stdv, stdv)\n\t\tif self.bias is not None:\n\t\t\tself.bias.data.uniform_(-stdv, stdv)\n\n\tdef forward(self, input, adj):\n\t\t#print(input.shape)\n\t\t#print(self.weight.shape)\n\t\tsupport = torch.matmul(input, self.weight)\n\t\t#print(adj.shape)\n\t\t#print(support.shape)\n\t\toutput = torch.matmul(adj, support)\n\t\t\n\t\tif self.bias is not None:\n\t\t\treturn output + self.bias\n\t\telse:\n\t\t\treturn output\n\n\tdef __repr__(self):\n\t\treturn self.__class__.__name__ + ' (' \\\n\t\t\t   + str(self.in_features) + ' -> ' \\\n\t\t\t   + str(self.out_features) + ')'\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T10:53:07.002843Z","iopub.execute_input":"2024-11-16T10:53:07.003241Z","iopub.status.idle":"2024-11-16T10:53:07.148561Z","shell.execute_reply.started":"2024-11-16T10:53:07.003201Z","shell.execute_reply":"2024-11-16T10:53:07.147486Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## utils","metadata":{}},{"cell_type":"code","source":"#########################################\n# expression_transfer.py\n#########################################\n\n# An expression tree node\nclass Et:\n    # Constructor to create a node\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n\n# Returns root of constructed tree for given postfix expression\ndef construct_exp_tree(postfix):\n    stack = []\n\n    # Traverse through every character of input expression\n    for char in postfix:\n\n        # if operand, simply push into stack\n        if char not in [\"+\", \"-\", \"*\", \"/\", \"^\"]:\n            t = Et(char)\n            stack.append(t)\n        # Operator\n        else:\n            # Pop two top nodes\n            t = Et(char)\n            t1 = stack.pop()\n            t2 = stack.pop()\n\n            # make them children\n            t.right = t1\n            t.left = t2\n\n            # Add this subexpression to stack\n            stack.append(t)\n    # Only element  will be the root of expression tree\n    t = stack.pop()\n    return t\n\n\ndef from_infix_to_postfix(expression):\n    st = list()\n    res = list()\n    priority = {\"+\": 0, \"-\": 0, \"*\": 1, \"/\": 1, \"^\": 2}\n    for e in expression:\n        if e in [\"(\", \"[\"]:\n            st.append(e)\n        elif e == \")\":\n            c = st.pop()\n            while c != \"(\":\n                res.append(c)\n                c = st.pop()\n        elif e == \"]\":\n            c = st.pop()\n            while c != \"[\":\n                res.append(c)\n                c = st.pop()\n        elif e in priority:\n            while len(st) > 0 and st[-1] not in [\"(\", \"[\"] and priority[e] <= priority[st[-1]]:\n                res.append(st.pop())\n            st.append(e)\n        else:\n            res.append(e)\n    while len(st) > 0:\n        res.append(st.pop())\n    return res\n\n\ndef from_infix_to_prefix(expression):\n    st = list()\n    res = list()\n    priority = {\"+\": 0, \"-\": 0, \"*\": 1, \"/\": 1, \"^\": 2}\n    expression = deepcopy(expression)\n    expression.reverse()\n    for e in expression:\n        if e in [\")\", \"]\"]:\n            st.append(e)\n        elif e == \"(\":\n            c = st.pop()\n            while c != \")\":\n                res.append(c)\n                c = st.pop()\n        elif e == \"[\":\n            c = st.pop()\n            while c != \"]\":\n                res.append(c)\n                c = st.pop()\n        elif e in priority:\n            while len(st) > 0 and st[-1] not in [\")\", \"]\"] and priority[e] < priority[st[-1]]:\n                res.append(st.pop())\n            st.append(e)\n        else:\n            res.append(e)\n    while len(st) > 0:\n        res.append(st.pop())\n    res.reverse()\n    return res\n\n\ndef out_expression_list(test, output_lang, num_list, num_stack=None):\n    max_index = output_lang.n_words\n    res = []\n    for i in test:\n        # if i == 0:\n        #     return res\n        if i < max_index - 1:\n            idx = output_lang.index2word[i]\n            if idx[0] == \"N\":\n                if int(idx[1:]) >= len(num_list):\n                    return None\n                res.append(num_list[int(idx[1:])])\n            else:\n                res.append(idx)\n        else:\n            pos_list = num_stack.pop()\n            c = num_list[pos_list[0]]\n            res.append(c)\n    return res\n\n\ndef compute_postfix_expression(post_fix):\n    st = list()\n    operators = [\"+\", \"-\", \"^\", \"*\", \"/\"]\n    for p in post_fix:\n        if p not in operators:\n            pos = re.search(\"\\d+\\(\", p)\n            if pos:\n                st.append(eval(p[pos.start(): pos.end() - 1] + \"+\" + p[pos.end() - 1:]))\n            elif p[-1] == \"%\":\n                    st.append(float(p[:-1]) / 100)\n            else:\n                st.append(eval(p))\n        elif p == \"+\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a + b)\n        elif p == \"*\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a * b)\n        elif p == \"*\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a * b)\n        elif p == \"/\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            if a == 0:\n                return None\n            st.append(b / a)\n        elif p == \"-\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(b - a)\n        elif p == \"^\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a ** b)\n        else:\n            return None\n    if len(st) == 1:\n        return st.pop()\n    return None\n\n\ndef compute_prefix_expression(pre_fix):\n    st = list()\n    operators = [\"+\", \"-\", \"^\", \"*\", \"/\"]\n    pre_fix = deepcopy(pre_fix)\n    pre_fix.reverse()\n    for p in pre_fix:\n        if p not in operators:\n            pos = re.search(\"\\d+\\(\", p)\n            if pos:\n                st.append(eval(p[pos.start(): pos.end() - 1] + \"+\" + p[pos.end() - 1:]))\n            elif p[-1] == \"%\":\n                st.append(float(p[:-1]) / 100)\n            else:\n                st.append(eval(p))\n        elif p == \"+\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a + b)\n        elif p == \"*\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a * b)\n        elif p == \"*\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a * b)\n        elif p == \"/\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            if b == 0:\n                return None\n            st.append(a / b)\n        elif p == \"-\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            st.append(a - b)\n        elif p == \"^\" and len(st) > 1:\n            a = st.pop()\n            b = st.pop()\n            if float(eval(b)) != 2.0 or float(eval(b)) != 3.0:\n                return None\n            st.append(a ** b)\n        else:\n            return None\n    if len(st) == 1:\n        return st.pop()\n    return None\n\n\n#########################################\n# helper.py\n#########################################\n\ndef gpu_init_pytorch(gpu_num):\n\t'''\n\t\tInitialize GPU\n\t'''\n\ttorch.cuda.set_device(int(gpu_num))\n\tdevice = torch.device(\"cuda:{}\".format(\n\t\tgpu_num) if torch.cuda.is_available() else \"cpu\")\n\treturn device\n\ndef create_save_directories(path):\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\ndef stack_to_string(stack):\n\top = \"\"\n\tfor i in stack:\n\t\tif op == \"\":\n\t\t\top = op + i\n\t\telse:\n\t\t\top = op + ' ' + i\n\treturn op\n\ndef index_batch_to_words(input_batch, input_length, lang):\n\t'''\n\t\tArgs:\n\t\t\tinput_batch: List of BS x Max_len\n\t\t\tinput_length: List of BS\n\t\tReturn:\n\t\t\tcontextual_input: List of BS\n\t'''\n\tcontextual_input = []\n\tfor i in range(len(input_batch)):\n\t\tcontextual_input.append(stack_to_string(sentence_from_indexes(lang, input_batch[i][:input_length[i]])))\n\n\treturn contextual_input\n\ndef sort_by_len(seqs, input_len, device=None, dim=1):\n\torig_idx = list(range(seqs.size(dim)))\n\t# pdb.set_trace()\n\n\t# Index by which sorting needs to be done\n\tsorted_idx = sorted(orig_idx, key=lambda k: input_len[k], reverse=True)\n\tsorted_idx= torch.LongTensor(sorted_idx)\n\tif device:\n\t\tsorted_idx = sorted_idx.to(device)\n\n\tsorted_seqs = seqs.index_select(1, sorted_idx)\n\tsorted_lens=  [input_len[i] for i in sorted_idx]\n\n\t# For restoring original order\n\torig_idx = sorted(orig_idx, key=lambda k: sorted_idx[k])\n\torig_idx = torch.LongTensor(orig_idx)\n\tif device:\n\t\torig_idx = orig_idx.to(device)\n\t\t# sorted_lens = torch.LongTensor(sorted_lens).to(device)\n\treturn sorted_seqs, sorted_lens, orig_idx\n\ndef save_checkpoint(state, epoch, logger, model_path, ckpt):\n\t'''\n\t\tSaves the model state along with epoch number. The name format is important for \n\t\tthe load functions. Don't mess with it.\n\n\t\tArgs:\n\t\t\tmodel state\n\t\t\tepoch number\n\t\t\tlogger variable\n\t\t\tdirectory to save models\n\t\t\tcheckpoint name\n\t'''\n\tckpt_path = os.path.join(model_path, '{}.pt'.format(ckpt))\n\tlogger.info('Saving Checkpoint at : {}'.format(ckpt_path))\n\ttorch.save(state, ckpt_path)\n\ndef load_checkpoint(config, embedding, encoder, predict, generate, merge, mode, ckpt_path, logger, device,\n\t\t\t\t\tembedding_optimizer = None, encoder_optimizer = None, predict_optimizer = None, generate_optimizer = None, merge_optimizer = None,\n\t\t\t\t\tembedding_scheduler = None, encoder_scheduler = None, predict_scheduler = None, generate_scheduler = None, merge_scheduler = None\n\t\t\t\t\t):\n\tcheckpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n\n\tembedding.load_state_dict(checkpoint['embedding_state_dict'])\n\tencoder.load_state_dict(checkpoint['encoder_state_dict'])\n\tpredict.load_state_dict(checkpoint['predict_state_dict'])\n\tgenerate.load_state_dict(checkpoint['generate_state_dict'])\n\tmerge.load_state_dict(checkpoint['merge_state_dict'])\n\n\tif mode == 'train':\n\t\tembedding_optimizer.load_state_dict(checkpoint['embedding_optimizer_state_dict'])\n\t\tencoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n\t\tpredict_optimizer.load_state_dict(checkpoint['predict_optimizer_state_dict'])\n\t\tgenerate_optimizer.load_state_dict(checkpoint['generate_optimizer_state_dict'])\n\t\tmerge_optimizer.load_state_dict(checkpoint['merge_optimizer_state_dict'])\n\n\t\tembedding_scheduler.load_state_dict(checkpoint['embedding_scheduler_state_dict'])\n\t\tencoder_scheduler.load_state_dict(checkpoint['encoder_scheduler_state_dict'])\n\t\tpredict_scheduler.load_state_dict(checkpoint['predict_scheduler_state_dict'])\n\t\tgenerate_scheduler.load_state_dict(checkpoint['generate_scheduler_state_dict'])\n\t\tmerge_scheduler.load_state_dict(checkpoint['merge_scheduler_state_dict'])\n\n\tstart_epoch = checkpoint['epoch']\n\tmin_train_loss  = checkpoint['min_train_loss']\n\tmax_train_acc = checkpoint['max_train_acc']\n\tmax_val_acc = checkpoint['max_val_acc']\n\tequation_acc = checkpoint['equation_acc']\n\tbest_epoch = checkpoint['best_epoch']\n\tgenerate_nums = checkpoint['generate_nums']\n\n\tembedding.to(device)\n\tencoder.to(device)\n\tpredict.to(device)\n\tgenerate.to(device)\n\tmerge.to(device)\n\n\tlogger.info('Successfully Loaded Checkpoint from {}, with epoch number: {} for {}'.format(ckpt_path, start_epoch, mode))\n\n\tif mode == 'train':\n\t\tembedding.train()\n\t\tencoder.train()\n\t\tpredict.train()\n\t\tgenerate.train()\n\t\tmerge.train()\n\telse:\n\t\tembedding.eval()\n\t\tencoder.eval()\n\t\tpredict.eval()\n\t\tgenerate.eval()\n\t\tmerge.eval()\t\t\n\n\treturn start_epoch, min_train_loss, max_train_acc, max_val_acc, equation_acc, best_epoch, generate_nums\n\ndef get_latest_checkpoint(model_path, logger):\n\t'''\n\t\tLooks for the checkpoint with highest epoch number in the directory \"model_path\" \n\n\t\tArgs:\n\t\t\tmodel_path: including the run_name\n\t\t\tlogger variable: to log messages\n\t\tReturns:\n\t\t\tcheckpoint: path to the latest checkpoint \n\t'''\n\n\tckpts = glob('{}/*.pt'.format(model_path))\n\tckpts = sorted(ckpts)\n\n\tif len(ckpts) == 0:\n\t\tlogger.warning('No Checkpoints Found')\n\n\t\treturn None\n\telse:\n\t\t#pdb.set_trace()\n\t\t#latest_epoch = max([int(x.split('_')[-1].split('.')[0]) for x in ckpts])\n\t\t#ckpts = sorted(ckpts, key= lambda x: int(x.split('_')[-1].split('.')[0]) , reverse=True )\n\t\tckpt_path = ckpts[0]\n\t\t#logger.info('Checkpoint found with epoch number : {}'.format(latest_epoch))\n\t\tlogger.debug('Checkpoint found at : {}'.format(ckpt_path))\n\n\t\treturn ckpt_path\n    \n#########################################\n# logger.py\n#########################################\n\ndef get_logger(name, log_file_path='./logs/temp.log', logging_level=logging.INFO, log_format='%(asctime)s | %(levelname)s | %(filename)s: %(lineno)s : %(funcName)s() ::\\t %(message)s'):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging_level)\n\tformatter = logging.Formatter(log_format)\n\n\tfile_handler = logging.FileHandler(log_file_path, mode='w')\n\tfile_handler.setLevel(logging_level)\n\tfile_handler.setFormatter(formatter)\n\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging_level)\n\tstream_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(stream_handler)\n\n\treturn logger\n\ndef print_log(logger, dict):\n\tstring = ''\n\tfor key, value in dict.items():\n\t\tstring += '\\n {}: {}\\t'.format(key.replace('_', ' '), value)\n\tlogger.info(string)\n\ndef store_results(config, max_train_acc, max_val_acc, eq_acc, min_train_loss, best_epoch):\n\ttry:\n\t\twith open(config.result_path) as f:\n\t\t\tres_data =json.load(f)\n\texcept:\n\t\tres_data = {}\n\ttry:\n\t\tmin_train_loss = min_train_loss.item()\n\texcept:\n\t\tpass\n\t# try:\n\t# \tmin_val_loss = min_val_loss.item()\n\t# except:\n\t# \tpass\n\t# try:\n\tdata= {'run name' : str(config.run_name)\n\t, 'max val acc': str(max_val_acc)\n\t, 'equation acc': str(eq_acc)\n\t, 'max train acc': str(max_train_acc)\n\t, 'min train loss': str(min_train_loss)\n\t, 'best epoch': str(best_epoch)\n\t, 'epochs' : config.epochs\n\t, 'dataset' : config.dataset\n\t, 'embedding': config.embedding\n\t, 'embedding_size' : config.embedding_size\n\t, 'embedding_lr': config.emb_lr\n\t, 'freeze_emb': config.freeze_emb\n\t, 'cell_type' : config.cell_type\n\t, 'hidden_size' : config.hidden_size\n\t, 'depth' : config.depth\n\t, 'lr' : config.lr\n\t, 'batch_size' : config.batch_size\n\t, 'dropout' : config.dropout\n\t}\n\tres_data[str(config.run_name)] = data\n\n\t# with open(config.result_path, 'w', encoding='utf-8') as f:\n\t# \tjson.dump(res_data, f, ensure_ascii= False, indent= 4)\n\t# except:\n\t# \tpdb.set_trace()\n\ndef store_val_results(config, acc_score, folds_scores):\n\ttry:\n\t\twith open(config.val_result_path) as f:\n\t\t\tres_data = json.load(f)\n\texcept:\n\t\tres_data = {}\n\n\ttry:\n\t\tdata= {'run_name' : str(config.run_name)\n\t\t, '5-fold avg acc score' : str(acc_score)\n\t\t, 'Fold0 acc' : folds_scores[0]\n\t\t, 'Fold1 acc' : folds_scores[1]\n\t\t, 'Fold2 acc' : folds_scores[2]\n\t\t, 'Fold3 acc' : folds_scores[3]\n\t\t, 'Fold4 acc' : folds_scores[4]\n\t\t, 'epochs' : config.epochs\n\t\t, 'embedding': config.embedding\n\t\t, 'embedding_size' : config.embedding_size\n\t\t, 'embedding_lr': config.emb_lr\n\t\t, 'freeze_emb': config.freeze_emb\n\t\t, 'cell_type' : config.cell_type\n\t\t, 'hidden_size' : config.hidden_size\n\t\t, 'depth' : config.depth\n\t\t, 'lr' : config.lr\n\t\t, 'batch_size' : config.batch_size\n\t\t, 'dropout' : config.dropout\n\t\t}\n\t\tres_data[str(config.run_name)] = data\n\n\t\t# with open(config.val_result_path, 'w', encoding='utf-8') as f:\n\t\t# \tjson.dump(res_data, f, ensure_ascii= False, indent= 4)\n\texcept:\n\t\tpdb.set_trace()\n        \n        \n#########################################\n# pre_data.py\n#########################################\n\nPAD_token = 0\n\n\nclass Lang:\n\t\"\"\"\n\tclass to save the vocab and two dict: the word->index and index->word\n\t\"\"\"\n\tdef __init__(self):\n\t\tself.word2index = {}\n\t\tself.word2count = {}\n\t\tself.index2word = []\n\t\tself.n_words = 0  # Count word tokens\n\t\tself.num_start = 0\n\n\tdef add_sen_to_vocab(self, sentence):  # add words of sentence to vocab\n\t\tfor word in sentence:\n\t\t\tif re.search(\"N\\d+|NUM|\\d+\", word):\n\t\t\t\tcontinue\n\t\t\tif word not in self.index2word:\n\t\t\t\tself.word2index[word] = self.n_words\n\t\t\t\tself.word2count[word] = 1\n\t\t\t\tself.index2word.append(word)\n\t\t\t\tself.n_words += 1\n\t\t\telse:\n\t\t\t\tself.word2count[word] += 1\n\n\tdef trim(self, min_count):  # trim words below a certain count threshold\n\t\tkeep_words = []\n\n\t\tfor k, v in self.word2count.items():\n\t\t\tif v >= min_count:\n\t\t\t\tkeep_words.append(k)\n\n\t\tprint('keep_words %s / %s = %.4f' % (\n\t\t\tlen(keep_words), len(self.index2word), len(keep_words) / len(self.index2word)\n\t\t))\n\n\t\t# Reinitialize dictionaries\n\t\tself.word2index = {}\n\t\t# self.word2count = {}\n\t\tself.index2word = []\n\t\tself.n_words = 0  # Count default tokens\n\n\t\tfor word in keep_words:\n\t\t\tself.word2index[word] = self.n_words\n\t\t\tself.index2word.append(word)\n\t\t\tself.n_words += 1\n\n\tdef build_input_lang(self, logger, trim_min_count):  # build the input lang vocab and dict\n\t\tif trim_min_count > 0:\n\t\t\tself.trim(trim_min_count)\n\t\t\tself.index2word = [\"PAD\", \"NUM\", \"UNK\"] + self.index2word\n\t\telse:\n\t\t\tself.index2word = [\"PAD\", \"NUM\"] + self.index2word\n\t\tself.word2index = {}\n\t\tself.n_words = len(self.index2word)\n\t\tfor i, j in enumerate(self.index2word):\n\t\t\tself.word2index[j] = i\n\n\tdef build_output_lang(self, generate_num, copy_nums):  # build the output lang vocab and dict\n\t\tself.index2word = [\"PAD\", \"EOS\"] + self.index2word + generate_num + [\"N\" + str(i) for i in range(copy_nums)] +\\\n\t\t\t\t\t\t  [\"SOS\", \"UNK\"]\n\t\tself.n_words = len(self.index2word)\n\t\tfor i, j in enumerate(self.index2word):\n\t\t\tself.word2index[j] = i\n\n\tdef build_output_lang_for_tree(self, generate_num, copy_nums):  # build the output lang vocab and dict\n\t\tself.num_start = len(self.index2word)\n\n\t\tself.index2word = self.index2word + generate_num + [\"N\" + str(i) for i in range(copy_nums)] + [\"UNK\"]\n\t\tself.n_words = len(self.index2word)\n\n\t\tfor i, j in enumerate(self.index2word):\n\t\t\tself.word2index[j] = i\n\ndef load_raw_data(data_path, dataset, is_train = True):  # load the data to list(dict())\n\ttrain_ls = None\n\tif is_train:\n\t\ttrain_path = os.path.join(data_path, dataset, 'train.csv')\n\t\ttrain_df = pd.read_csv(train_path, converters={'group_nums': eval})\n\t\ttrain_ls = train_df.to_dict('records')\n\n\tdev_path = os.path.join(data_path, dataset, 'dev.csv')\n\tdev_df = pd.read_csv(dev_path, converters={'group_nums': eval})\n\tdev_ls = dev_df.to_dict('records')\n\n\treturn train_ls, dev_ls\n\n\n# remove the superfluous brackets\ndef remove_brackets(x):\n\ty = x\n\tif x[0] == \"(\" and x[-1] == \")\":\n\t\tx = x[1:-1]\n\t\tflag = True\n\t\tcount = 0\n\t\tfor s in x:\n\t\t\tif s == \")\":\n\t\t\t\tcount -= 1\n\t\t\t\tif count < 0:\n\t\t\t\t\tflag = False\n\t\t\t\t\tbreak\n\t\t\telif s == \"(\":\n\t\t\t\tcount += 1\n\t\tif flag:\n\t\t\treturn x\n\treturn y\n\n\ndef load_mawps_data(filename):  # load the json data to list(dict()) for MAWPS\n\tprint(\"Reading lines...\")\n\tf = open(filename, encoding=\"utf-8\")\n\tdata = json.load(f)\n\tout_data = []\n\tfor d in data:\n\t\tif \"lEquations\" not in d or len(d[\"lEquations\"]) != 1: # Only single equations\n\t\t\tcontinue\n\t\tx = d[\"lEquations\"][0].replace(\" \", \"\")\n\n\t\tif \"lQueryVars\" in d and len(d[\"lQueryVars\"]) == 1: # When Equations are annotated with variables\n\t\t\tv = d[\"lQueryVars\"][0]\n\t\t\tif v + \"=\" == x[:len(v)+1]: # If eqn of the form 'Var=...'  \n\t\t\t\txt = x[len(v)+1:]\n\t\t\t\tif len(set(xt) - set(\"0123456789.+-*/()\")) == 0:\n\t\t\t\t\ttemp = d.copy()\n\t\t\t\t\ttemp[\"lEquations\"] = xt\n\t\t\t\t\tout_data.append(temp)\n\t\t\t\t\tcontinue\n\n\t\t\tif \"=\" + v == x[-len(v)-1:]: # If eqn of the form '...=Var'\n\t\t\t\txt = x[:-len(v)-1]\n\t\t\t\tif len(set(xt) - set(\"0123456789.+-*/()\")) == 0:\n\t\t\t\t\ttemp = d.copy()\n\t\t\t\t\ttemp[\"lEquations\"] = xt\n\t\t\t\t\tout_data.append(temp)\n\t\t\t\t\tcontinue\n\n\t\tif len(set(x) - set(\"0123456789.+-*/()=xX\")) != 0: # If equation has anything not in the set on RHS of -\n\t\t\tcontinue\n\n\t\tif x[:2] == \"x=\" or x[:2] == \"X=\":\n\t\t\tif len(set(x[2:]) - set(\"0123456789.+-*/()\")) == 0:\n\t\t\t\ttemp = d.copy()\n\t\t\t\ttemp[\"lEquations\"] = x[2:]\n\t\t\t\tout_data.append(temp)\n\t\t\t\tcontinue\n\t\tif x[-2:] == \"=x\" or x[-2:] == \"=X\":\n\t\t\tif len(set(x[:-2]) - set(\"0123456789.+-*/()\")) == 0:\n\t\t\t\ttemp = d.copy()\n\t\t\t\ttemp[\"lEquations\"] = x[:-2]\n\t\t\t\tout_data.append(temp)\n\t\t\t\tcontinue\n\treturn out_data\n\n\ndef load_roth_data(filename):  # load the json data to dict(dict()) for roth data\n\tprint(\"Reading lines...\")\n\tf = open(filename, encoding=\"utf-8\")\n\tdata = json.load(f)\n\tout_data = {}\n\tfor d in data:\n\t\tif \"lEquations\" not in d or len(d[\"lEquations\"]) != 1:\n\t\t\tcontinue\n\t\tx = d[\"lEquations\"][0].replace(\" \", \"\")\n\n\t\tif \"lQueryVars\" in d and len(d[\"lQueryVars\"]) == 1:\n\t\t\tv = d[\"lQueryVars\"][0]\n\t\t\tif v + \"=\" == x[:len(v)+1]:\n\t\t\t\txt = x[len(v)+1:]\n\t\t\t\tif len(set(xt) - set(\"0123456789.+-*/()\")) == 0:\n\t\t\t\t\ttemp = d.copy()\n\t\t\t\t\ttemp[\"lEquations\"] = remove_brackets(xt)\n\t\t\t\t\ty = temp[\"sQuestion\"]\n\t\t\t\t\tseg = y.strip().split(\" \")\n\t\t\t\t\ttemp_y = \"\"\n\t\t\t\t\tfor s in seg:\n\t\t\t\t\t\tif len(s) > 1 and (s[-1] == \",\" or s[-1] == \".\" or s[-1] == \"?\"):\n\t\t\t\t\t\t\ttemp_y += s[:-1] + \" \" + s[-1:] + \" \"\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttemp_y += s + \" \"\n\t\t\t\t\ttemp[\"sQuestion\"] = temp_y[:-1]\n\t\t\t\t\tout_data[temp[\"iIndex\"]] = temp\n\t\t\t\t\tcontinue\n\n\t\t\tif \"=\" + v == x[-len(v)-1:]:\n\t\t\t\txt = x[:-len(v)-1]\n\t\t\t\tif len(set(xt) - set(\"0123456789.+-*/()\")) == 0:\n\t\t\t\t\ttemp = d.copy()\n\t\t\t\t\ttemp[\"lEquations\"] = remove_brackets(xt)\n\t\t\t\t\ty = temp[\"sQuestion\"]\n\t\t\t\t\tseg = y.strip().split(\" \")\n\t\t\t\t\ttemp_y = \"\"\n\t\t\t\t\tfor s in seg:\n\t\t\t\t\t\tif len(s) > 1 and (s[-1] == \",\" or s[-1] == \".\" or s[-1] == \"?\"):\n\t\t\t\t\t\t\ttemp_y += s[:-1] + \" \" + s[-1:] + \" \"\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttemp_y += s + \" \"\n\t\t\t\t\ttemp[\"sQuestion\"] = temp_y[:-1]\n\t\t\t\t\tout_data[temp[\"iIndex\"]] = temp\n\t\t\t\t\tcontinue\n\n\t\tif len(set(x) - set(\"0123456789.+-*/()=xX\")) != 0:\n\t\t\tcontinue\n\n\t\tif x[:2] == \"x=\" or x[:2] == \"X=\":\n\t\t\tif len(set(x[2:]) - set(\"0123456789.+-*/()\")) == 0:\n\t\t\t\ttemp = d.copy()\n\t\t\t\ttemp[\"lEquations\"] = remove_brackets(x[2:])\n\t\t\t\ty = temp[\"sQuestion\"]\n\t\t\t\tseg = y.strip().split(\" \")\n\t\t\t\ttemp_y = \"\"\n\t\t\t\tfor s in seg:\n\t\t\t\t\tif len(s) > 1 and (s[-1] == \",\" or s[-1] == \".\" or s[-1] == \"?\"):\n\t\t\t\t\t\ttemp_y += s[:-1] + \" \" + s[-1:] + \" \"\n\t\t\t\t\telse:\n\t\t\t\t\t\ttemp_y += s + \" \"\n\t\t\t\ttemp[\"sQuestion\"] = temp_y[:-1]\n\t\t\t\tout_data[temp[\"iIndex\"]] = temp\n\t\t\t\tcontinue\n\t\tif x[-2:] == \"=x\" or x[-2:] == \"=X\":\n\t\t\tif len(set(x[:-2]) - set(\"0123456789.+-*/()\")) == 0:\n\t\t\t\ttemp = d.copy()\n\t\t\t\ttemp[\"lEquations\"] = remove_brackets(x[2:])\n\t\t\t\ty = temp[\"sQuestion\"]\n\t\t\t\tseg = y.strip().split(\" \")\n\t\t\t\ttemp_y = \"\"\n\t\t\t\tfor s in seg:\n\t\t\t\t\tif len(s) > 1 and (s[-1] == \",\" or s[-1] == \".\" or s[-1] == \"?\"):\n\t\t\t\t\t\ttemp_y += s[:-1] + \" \" + s[-1:] + \" \"\n\t\t\t\t\telse:\n\t\t\t\t\t\ttemp_y += s + \" \"\n\t\t\t\ttemp[\"sQuestion\"] = temp_y[:-1]\n\t\t\t\tout_data[temp[\"iIndex\"]] = temp\n\t\t\t\tcontinue\n\treturn out_data\n\n\ndef transfer_num(train_ls, dev_ls, chall = False):  # transfer num into \"NUM\"\n\tprint(\"Transfer numbers...\")\n\tdev_pairs = []\n\tgenerate_nums = []\n\tgenerate_nums_dict = {}\n\tcopy_nums = 0\n\n\tif train_ls != None:\n\t\ttrain_pairs = []\n\t\tfor d in train_ls:\n\t\t\t# nums = []\n\t\t\tnums = d['Numbers'].split()\n\t\t\tinput_seq = []\n\t\t\tseg = nltk.word_tokenize(d[\"Question\"].strip())\n\t\t\tequation = d[\"Equation\"].split()\n\n\t\t\tnumz = ['0','1','2','3','4','5','6','7','8','9']\n\t\t\topz = ['+', '-', '*', '/']\n\t\t\tidxs = []\n\t\t\tfor s in range(len(seg)):\n\t\t\t\tif len(seg[s]) >= 7 and seg[s][:6] == \"number\" and seg[s][6] in numz:\n\t\t\t\t\tinput_seq.append(\"NUM\")\n\t\t\t\t\tidxs.append(s)\n\t\t\t\telse:\n\t\t\t\t\tinput_seq.append(seg[s])\n\t\t\tif copy_nums < len(nums):\n\t\t\t\tcopy_nums = len(nums)\n\n\t\t\tout_seq = []\n\t\t\tfor e1 in equation:\n\t\t\t\tif len(e1) >= 7 and e1[:6] == \"number\":\n\t\t\t\t\tout_seq.append('N'+e1[6:])\n\t\t\t\telif e1 not in opz:\n\t\t\t\t\tgenerate_nums.append(e1)\n\t\t\t\t\tif e1 not in generate_nums_dict:\n\t\t\t\t\t\tgenerate_nums_dict[e1] = 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tgenerate_nums_dict[e1] += 1\n\t\t\t\t\tout_seq.append(e1)\n\t\t\t\telse:\n\t\t\t\t\tout_seq.append(e1)\n\n\t\t\ttrain_pairs.append((input_seq, out_seq, nums, idxs, d['group_nums']))\n\telse:\n\t\ttrain_pairs = None\n\n\tfor d in dev_ls:\n\t\t# nums = []\n\t\tnums = d['Numbers'].split()\n\t\tinput_seq = []\n\t\ttry:\n\t\t\tseg = nltk.word_tokenize(d[\"Question\"].strip())\n\t\texcept:\n\t\t\tpdb.set_trace()\n\t\tequation = d[\"Equation\"].split()\n\n\t\tnumz = ['0','1','2','3','4','5','6','7','8','9']\n\t\topz = ['+', '-', '*', '/']\n\t\tidxs = []\n\t\tfor s in range(len(seg)):\n\t\t\tif len(seg[s]) >= 7 and seg[s][:6] == \"number\" and seg[s][6] in numz:\n\t\t\t\tinput_seq.append(\"NUM\")\n\t\t\t\tidxs.append(s)\n\t\t\telse:\n\t\t\t\tinput_seq.append(seg[s])\n\t\tif copy_nums < len(nums):\n\t\t\tcopy_nums = len(nums)\n\n\t\tout_seq = []\n\t\tfor e1 in equation:\n\t\t\tif len(e1) >= 7 and e1[:6] == \"number\":\n\t\t\t\tout_seq.append('N'+e1[6:])\n\t\t\telif e1 not in opz:\n\t\t\t\tgenerate_nums.append(e1)\n\t\t\t\tif e1 not in generate_nums_dict:\n\t\t\t\t\tgenerate_nums_dict[e1] = 1\n\t\t\t\telse:\n\t\t\t\t\tgenerate_nums_dict[e1] += 1\n\t\t\t\tout_seq.append(e1)\n\t\t\telse:\n\t\t\t\tout_seq.append(e1)\n\t\tif chall:\n\t\t\tdev_pairs.append((input_seq, out_seq, nums, idxs, d['group_nums'], d['Type'], d['Variation Type'], d['Annotator'], d['Alternate']))\n\t\telse:\n\t\t\tdev_pairs.append((input_seq, out_seq, nums, idxs, d['group_nums']))\n\n\ttemp_g = []\n\tfor g in generate_nums_dict:\n\t\tif generate_nums_dict[g] >= 5:\n\t\t\ttemp_g.append(g)\n\treturn train_pairs, dev_pairs, temp_g, copy_nums\n\n\ndef transfer_english_num(data):  # transfer num into \"NUM\"\n\tprint(\"Transfer numbers...\")\n\tpattern = re.compile(\"\\d+,\\d+|\\d+\\.\\d+|\\d+\")\n\tpairs = []\n\tgenerate_nums = {} # Unmentioned numbers used in eqns in atleast 5 examples\n\tcopy_nums = 0 # Maximum number of numbers in a single sentence\n\tfor d in data:\n\t\tnums = []\n\t\tinput_seq = []\n\t\tseg = d[\"sQuestion\"].strip().split(\" \")\n\t\tequations = d[\"lEquations\"]\n\n\t\tfor s in seg:\n\t\t\tpos = re.search(pattern, s)\n\t\t\tif pos:\n\t\t\t\tif pos.start() > 0:\n\t\t\t\t\tinput_seq.append(s[:pos.start()])\n\t\t\t\tnum = s[pos.start(): pos.end()]\n\t\t\t\tnums.append(num.replace(\",\", \"\"))\n\t\t\t\tinput_seq.append(\"NUM\")\n\t\t\t\tif pos.end() < len(s):\n\t\t\t\t\tinput_seq.append(s[pos.end():])\n\t\t\telse:\n\t\t\t\tinput_seq.append(s)\n\n\t\tif copy_nums < len(nums):\n\t\t\tcopy_nums = len(nums)\n\t\teq_segs = []\n\t\ttemp_eq = \"\"\n\t\tfor e in equations:\n\t\t\tif e not in \"()+-*/\":\n\t\t\t\ttemp_eq += e\n\t\t\telif temp_eq != \"\":\n\t\t\t\tcount_eq = []\n\t\t\t\tfor n_idx, n in enumerate(nums):\n\t\t\t\t\tif abs(float(n) - float(temp_eq)) < 1e-4:\n\t\t\t\t\t\tcount_eq.append(n_idx)\n\t\t\t\t\t\tif n != temp_eq:\n\t\t\t\t\t\t\tnums[n_idx] = temp_eq\n\t\t\t\tif len(count_eq) == 0:\n\t\t\t\t\tflag = True\n\t\t\t\t\tfor gn in generate_nums:\n\t\t\t\t\t\tif abs(float(gn) - float(temp_eq)) < 1e-4:\n\t\t\t\t\t\t\tgenerate_nums[gn] += 1\n\t\t\t\t\t\t\tif temp_eq != gn:\n\t\t\t\t\t\t\t\ttemp_eq = gn\n\t\t\t\t\t\t\tflag = False\n\t\t\t\t\tif flag:\n\t\t\t\t\t\tgenerate_nums[temp_eq] = 0\n\t\t\t\t\teq_segs.append(temp_eq)\n\t\t\t\telif len(count_eq) == 1:\n\t\t\t\t\teq_segs.append(\"N\"+str(count_eq[0]))\n\t\t\t\telse:\n\t\t\t\t\teq_segs.append(temp_eq)\n\t\t\t\teq_segs.append(e)\n\t\t\t\ttemp_eq = \"\"\n\t\t\telse:\n\t\t\t\teq_segs.append(e)\n\t\tif temp_eq != \"\":\n\t\t\tcount_eq = []\n\t\t\tfor n_idx, n in enumerate(nums):\n\t\t\t\tif abs(float(n) - float(temp_eq)) < 1e-4:\n\t\t\t\t\tcount_eq.append(n_idx)\n\t\t\t\t\tif n != temp_eq:\n\t\t\t\t\t\tnums[n_idx] = temp_eq\n\t\t\tif len(count_eq) == 0:\n\t\t\t\tflag = True\n\t\t\t\tfor gn in generate_nums:\n\t\t\t\t\tif abs(float(gn) - float(temp_eq)) < 1e-4:\n\t\t\t\t\t\tgenerate_nums[gn] += 1\n\t\t\t\t\t\tif temp_eq != gn:\n\t\t\t\t\t\t\ttemp_eq = gn\n\t\t\t\t\t\tflag = False\n\t\t\t\tif flag:\n\t\t\t\t\tgenerate_nums[temp_eq] = 0\n\t\t\t\teq_segs.append(temp_eq)\n\t\t\telif len(count_eq) == 1:\n\t\t\t\teq_segs.append(\"N\" + str(count_eq[0]))\n\t\t\telse:\n\t\t\t\teq_segs.append(temp_eq)\n\n\t\tnum_pos = []\n\t\tfor i, j in enumerate(input_seq):\n\t\t\tif j == \"NUM\":\n\t\t\t\tnum_pos.append(i)\n\t\tif len(nums) != 0:\n\t\t\tpairs.append((input_seq, eq_segs, nums, num_pos))\n\n\ttemp_g = []\n\tfor g in generate_nums:\n\t\tif generate_nums[g] >= 5:\n\t\t\ttemp_g.append(g)\n\n\treturn pairs, temp_g, copy_nums\n\n\ndef transfer_roth_num(data):  # transfer num into \"NUM\"\n\tprint(\"Transfer numbers...\")\n\tpattern = re.compile(\"\\d+,\\d+|\\d+\\.\\d+|\\d+\")\n\tpairs = {}\n\tgenerate_nums = {}\n\tcopy_nums = 0\n\tfor key in data:\n\t\td = data[key]\n\t\tnums = []\n\t\tinput_seq = []\n\t\tseg = d[\"sQuestion\"].strip().split(\" \")\n\t\tequations = d[\"lEquations\"]\n\n\t\tfor s in seg:\n\t\t\tpos = re.search(pattern, s)\n\t\t\tif pos:\n\t\t\t\tif pos.start() > 0:\n\t\t\t\t\tinput_seq.append(s[:pos.start()])\n\t\t\t\tnum = s[pos.start(): pos.end()]\n\t\t\t\tnums.append(num.replace(\",\", \"\"))\n\t\t\t\tinput_seq.append(\"NUM\")\n\t\t\t\tif pos.end() < len(s):\n\t\t\t\t\tinput_seq.append(s[pos.end():])\n\t\t\telse:\n\t\t\t\tinput_seq.append(s)\n\n\t\tif copy_nums < len(nums):\n\t\t\tcopy_nums = len(nums)\n\t\teq_segs = []\n\t\ttemp_eq = \"\"\n\t\tfor e in equations:\n\t\t\tif e not in \"()+-*/\":\n\t\t\t\ttemp_eq += e\n\t\t\telif temp_eq != \"\":\n\t\t\t\tcount_eq = []\n\t\t\t\tfor n_idx, n in enumerate(nums):\n\t\t\t\t\tif abs(float(n) - float(temp_eq)) < 1e-4:\n\t\t\t\t\t\tcount_eq.append(n_idx)\n\t\t\t\t\t\tif n != temp_eq:\n\t\t\t\t\t\t\tnums[n_idx] = temp_eq\n\t\t\t\tif len(count_eq) == 0:\n\t\t\t\t\tflag = True\n\t\t\t\t\tfor gn in generate_nums:\n\t\t\t\t\t\tif abs(float(gn) - float(temp_eq)) < 1e-4:\n\t\t\t\t\t\t\tgenerate_nums[gn] += 1\n\t\t\t\t\t\t\tif temp_eq != gn:\n\t\t\t\t\t\t\t\ttemp_eq = gn\n\t\t\t\t\t\t\tflag = False\n\t\t\t\t\tif flag:\n\t\t\t\t\t\tgenerate_nums[temp_eq] = 0\n\t\t\t\t\teq_segs.append(temp_eq)\n\t\t\t\telif len(count_eq) == 1:\n\t\t\t\t\teq_segs.append(\"N\"+str(count_eq[0]))\n\t\t\t\telse:\n\t\t\t\t\teq_segs.append(temp_eq)\n\t\t\t\teq_segs.append(e)\n\t\t\t\ttemp_eq = \"\"\n\t\t\telse:\n\t\t\t\teq_segs.append(e)\n\t\tif temp_eq != \"\":\n\t\t\tcount_eq = []\n\t\t\tfor n_idx, n in enumerate(nums):\n\t\t\t\tif abs(float(n) - float(temp_eq)) < 1e-4:\n\t\t\t\t\tcount_eq.append(n_idx)\n\t\t\t\t\tif n != temp_eq:\n\t\t\t\t\t\tnums[n_idx] = temp_eq\n\t\t\tif len(count_eq) == 0:\n\t\t\t\tflag = True\n\t\t\t\tfor gn in generate_nums:\n\t\t\t\t\tif abs(float(gn) - float(temp_eq)) < 1e-4:\n\t\t\t\t\t\tgenerate_nums[gn] += 1\n\t\t\t\t\t\tif temp_eq != gn:\n\t\t\t\t\t\t\ttemp_eq = gn\n\t\t\t\t\t\tflag = False\n\t\t\t\tif flag:\n\t\t\t\t\tgenerate_nums[temp_eq] = 0\n\t\t\t\teq_segs.append(temp_eq)\n\t\t\telif len(count_eq) == 1:\n\t\t\t\teq_segs.append(\"N\" + str(count_eq[0]))\n\t\t\telse:\n\t\t\t\teq_segs.append(temp_eq)\n\n\t\tnum_pos = []\n\t\tfor i, j in enumerate(input_seq):\n\t\t\tif j == \"NUM\":\n\t\t\t\tnum_pos.append(i)\n\t\tif len(nums) != 0:\n\t\t\tpairs[key] = (input_seq, eq_segs, nums, num_pos)\n\n\ttemp_g = []\n\tfor g in generate_nums:\n\t\tif generate_nums[g] >= 5:\n\t\t\ttemp_g.append(g)\n\n\treturn pairs, temp_g, copy_nums\n\n\n# Return a list of indexes, one for each word in the sentence, plus EOS\ndef indexes_from_sentence(lang, sentence, tree=False):\n\tres = []\n\tfor word in sentence:\n\t\tif len(word) == 0:\n\t\t\tcontinue\n\t\tif word in lang.word2index:\n\t\t\tres.append(lang.word2index[word])\n\t\telse:\n\t\t\tres.append(lang.word2index[\"UNK\"])\n\tif \"EOS\" in lang.index2word and not tree:\n\t\tres.append(lang.word2index[\"EOS\"])\n\treturn res\n\ndef sentence_from_indexes(lang, indexes):\n\tsent = []\n\tfor ind in indexes:\n\t\tsent.append(lang.index2word[ind])\n\treturn sent\n\n\ndef prepare_data(config, logger, pairs_trained, pairs_tested, trim_min_count, generate_nums, copy_nums, input_lang=None, output_lang=None, tree=False):\n\tif input_lang == None:\n\t\tinput_lang = Lang()\n\tif output_lang == None:\n\t\toutput_lang = Lang()\n\n\ttest_pairs = []\n\ttrain_pairs = None\n\n\tif pairs_trained != None:\n\t\ttrain_pairs = []\n\t\tfor pair in pairs_trained:\n\t\t\tif not tree:\n\t\t\t\tinput_lang.add_sen_to_vocab(pair[0])\n\t\t\t\toutput_lang.add_sen_to_vocab(pair[1])\n\t\t\telif pair[-1]:\n\t\t\t\tinput_lang.add_sen_to_vocab(pair[0])\n\t\t\t\toutput_lang.add_sen_to_vocab(pair[1])\n\n\tif config.embedding == 'bert' or config.embedding == 'roberta':\n\t\tfor pair in pairs_tested:\n\t\t\tif not tree:\n\t\t\t\tinput_lang.add_sen_to_vocab(pair[0])\n\t\t\telif pair[-1]:\n\t\t\t\tinput_lang.add_sen_to_vocab(pair[0])\n\n\tif pairs_trained != None:\n\n\t\tinput_lang.build_input_lang(logger, trim_min_count)\n\t\tif tree:\n\t\t\toutput_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n\t\telse:\n\t\t\toutput_lang.build_output_lang(generate_nums, copy_nums)\n\n\t\tfor pair in pairs_trained:\n\t\t\tnum_stack = []\n\t\t\tfor word in pair[1]: # For each token in equation\n\t\t\t\ttemp_num = []\n\t\t\t\tflag_not = True\n\t\t\t\tif word not in output_lang.index2word: # If token is not in output vocab\n\t\t\t\t\tflag_not = False\n\t\t\t\t\tfor i, j in enumerate(pair[2]):\n\t\t\t\t\t\tif j == word:\n\t\t\t\t\t\t\ttemp_num.append(i) # Append number list index of token not in output vocab\n\n\t\t\t\tif not flag_not and len(temp_num) != 0: # Equation has an unknown token and it is a number present in number list (could be default number with freq < 5)\n\t\t\t\t\tnum_stack.append(temp_num)\n\t\t\t\tif not flag_not and len(temp_num) == 0: # Equation has an unknown token but it is not a number from number list\n\t\t\t\t\tnum_stack.append([_ for _ in range(len(pair[2]))])\n\n\t\t\tnum_stack.reverse()\n\t\t\tinput_cell = indexes_from_sentence(input_lang, pair[0])\n\t\t\toutput_cell = indexes_from_sentence(output_lang, pair[1], tree)\n\t\t\ttrain_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n\t\t\t\t\t\t\t\tpair[2], pair[3], num_stack, pair[4]))\n\n\tlogger.debug('Indexed {} words in input language, {} words in output'.format(input_lang.n_words, output_lang.n_words))\n\n\tfor pair in pairs_tested:\n\t\tnum_stack = []\n\t\tfor word in pair[1]:\n\t\t\ttemp_num = []\n\t\t\tflag_not = True\n\t\t\tif word not in output_lang.index2word:\n\t\t\t\tflag_not = False\n\t\t\t\tfor i, j in enumerate(pair[2]):\n\t\t\t\t\tif j == word:\n\t\t\t\t\t\ttemp_num.append(i)\n\n\t\t\tif not flag_not and len(temp_num) != 0:\n\t\t\t\tnum_stack.append(temp_num)\n\t\t\tif not flag_not and len(temp_num) == 0:\n\t\t\t\tnum_stack.append([_ for _ in range(len(pair[2]))])\n\n\t\tnum_stack.reverse()\n\t\tinput_cell = indexes_from_sentence(input_lang, pair[0])\n\t\toutput_cell = indexes_from_sentence(output_lang, pair[1], tree)\n\t\tif config.challenge_disp:\n\t\t\ttest_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n\t\t\t\t\t\t   pair[2], pair[3], num_stack, pair[4], pair[5], pair[6], pair[7], pair[8]))\n\t\telse:\n\t\t\ttest_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n\t\t\t\t\t\t   pair[2], pair[3], num_stack, pair[4]))\n\n\treturn input_lang, output_lang, train_pairs, test_pairs\n\n\ndef prepare_de_data(pairs_trained, pairs_tested, trim_min_count, generate_nums, copy_nums, tree=False):\n\tinput_lang = Lang()\n\toutput_lang = Lang()\n\ttrain_pairs = []\n\ttest_pairs = []\n\n\tprint(\"Indexing words...\")\n\tfor pair in pairs_trained:\n\t\tinput_lang.add_sen_to_vocab(pair[0])\n\t\toutput_lang.add_sen_to_vocab(pair[1])\n\n\tinput_lang.build_input_lang(trim_min_count)\n\n\tif tree:\n\t\toutput_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n\telse:\n\t\toutput_lang.build_output_lang(generate_nums, copy_nums)\n\n\tfor pair in pairs_trained:\n\t\tnum_stack = []\n\t\tfor word in pair[1]:\n\t\t\ttemp_num = []\n\t\t\tflag_not = True\n\t\t\tif word not in output_lang.index2word:\n\t\t\t\tflag_not = False\n\t\t\t\tfor i, j in enumerate(pair[2]):\n\t\t\t\t\tif j == word:\n\t\t\t\t\t\ttemp_num.append(i)\n\n\t\t\tif not flag_not and len(temp_num) != 0:\n\t\t\t\tnum_stack.append(temp_num)\n\t\t\tif not flag_not and len(temp_num) == 0:\n\t\t\t\tnum_stack.append([_ for _ in range(len(pair[2]))])\n\n\t\tnum_stack.reverse()\n\t\tinput_cell = indexes_from_sentence(input_lang, pair[0])\n\t\t# train_pairs.append([input_cell, len(input_cell), pair[1], 0, pair[2], pair[3], num_stack, pair[4]])\n\t\ttrain_pairs.append([input_cell, len(input_cell), pair[1], 0, pair[2], pair[3], num_stack])\n\tprint('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n\tprint('Number of training data %d' % (len(train_pairs)))\n\tfor pair in pairs_tested:\n\t\tnum_stack = []\n\t\tfor word in pair[1]:\n\t\t\ttemp_num = []\n\t\t\tflag_not = True\n\t\t\tif word not in output_lang.index2word:\n\t\t\t\tflag_not = False\n\t\t\t\tfor i, j in enumerate(pair[2]):\n\t\t\t\t\tif j == word:\n\t\t\t\t\t\ttemp_num.append(i)\n\n\t\t\tif not flag_not and len(temp_num) != 0:\n\t\t\t\tnum_stack.append(temp_num)\n\t\t\tif not flag_not and len(temp_num) == 0:\n\t\t\t\tnum_stack.append([_ for _ in range(len(pair[2]))])\n\n\t\tnum_stack.reverse()\n\t\tinput_cell = indexes_from_sentence(input_lang, pair[0])\n\t\toutput_cell = indexes_from_sentence(output_lang, pair[1], tree)\n\t\t# train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n\t\t#                     pair[2], pair[3], num_stack, pair[4]))\n\t\ttest_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n\t\t\t\t\t\t   pair[2], pair[3], num_stack))\n\tprint('Number of testind data %d' % (len(test_pairs)))\n\treturn input_lang, output_lang, train_pairs, test_pairs\n\n\n# Pad a with the PAD symbol\ndef pad_seq(seq, seq_len, max_length):\n\tseq += [PAD_token for _ in range(max_length - seq_len)]\n\treturn seq\n\ndef change_num(num):\n\tnew_num = []\n\tfor item in num:\n\t\tif '/' in item:\n\t\t\tnew_str = item.split(')')[0]\n\t\t\tnew_str = new_str.split('(')[1]\n\t\t\ta = float(new_str.split('/')[0])\n\t\t\tb = float(new_str.split('/')[1])\n\t\t\tvalue = a/b\n\t\t\tnew_num.append(value)\n\t\telif '%' in item:\n\t\t\tvalue = float(item[0:-1])/100\n\t\t\tnew_num.append(value)\n\t\telse:\n\t\t\tnew_num.append(float(item))\n\treturn new_num\n\n# num net graph\ndef get_lower_num_graph(max_len, sentence_length, num_list, id_num_list,contain_zh_flag=True):\n\tdiag_ele = np.zeros(max_len)\n\tnum_list = change_num(num_list)\n\tfor i in range(sentence_length):\n\t\tdiag_ele[i] = 1\n\tgraph = np.diag(diag_ele)\n\tif not contain_zh_flag:\n\t\treturn graph\n\tfor i in range(len(id_num_list)):\n\t\tfor j in range(len(id_num_list)):\n\t\t\tif float(num_list[i]) <= float(num_list[j]):\n\t\t\t\tgraph[id_num_list[i]][id_num_list[j]] = 1\n\t\t\telse:\n\t\t\t\tgraph[id_num_list[j]][id_num_list[i]] = 1\n\treturn graph\n\ndef get_greater_num_graph(max_len, sentence_length, num_list, id_num_list,contain_zh_flag=True):\n\tdiag_ele = np.zeros(max_len)\n\tnum_list = change_num(num_list)\n\tfor i in range(sentence_length):\n\t\tdiag_ele[i] = 1\n\tgraph = np.diag(diag_ele)\n\tif not contain_zh_flag:\n\t\treturn graph\n\tfor i in range(len(id_num_list)):\n\t\tfor j in range(len(id_num_list)):\n\t\t\tif float(num_list[i]) > float(num_list[j]):\n\t\t\t\tgraph[id_num_list[i]][id_num_list[j]] = 1\n\t\t\telse:\n\t\t\t\tgraph[id_num_list[j]][id_num_list[i]] = 1\n\treturn graph\n\n# attribute between graph\ndef get_attribute_between_graph(input_batch, max_len, id_num_list, sentence_length, quantity_cell_list,contain_zh_flag=True):\n\tdiag_ele = np.zeros(max_len)\n\tfor i in range(sentence_length):\n\t\tdiag_ele[i] = 1\n\tgraph = np.diag(diag_ele)\n\t#quantity_cell_list = quantity_cell_list.extend(id_num_list)\n\tif not contain_zh_flag:\n\t\treturn graph\n\tfor i in id_num_list:\n\t\tfor j in quantity_cell_list:\n\t\t\tif i < max_len and j < max_len and j not in id_num_list and abs(i-j) < 4:\n\t\t\t\tgraph[i][j] = 1\n\t\t\t\tgraph[j][i] = 1\n\tfor i in quantity_cell_list:\n\t\tfor j in quantity_cell_list:\n\t\t\tif i < max_len and j < max_len:\n\t\t\t\tif input_batch[i] == input_batch[j]:\n\t\t\t\t\tgraph[i][j] = 1\n\t\t\t\t\tgraph[j][i] = 1\n\treturn graph\n\n# quantity between graph\ndef get_quantity_between_graph(max_len, id_num_list, sentence_length, quantity_cell_list,contain_zh_flag=True):\n\tdiag_ele = np.zeros(max_len)\n\tfor i in range(sentence_length):\n\t\tdiag_ele[i] = 1\n\tgraph = np.diag(diag_ele)\n\t#quantity_cell_list = quantity_cell_list.extend(id_num_list)\n\tif not contain_zh_flag:\n\t\treturn graph\n\tfor i in id_num_list:\n\t\tfor j in quantity_cell_list:\n\t\t\tif i < max_len and j < max_len and j not in id_num_list and abs(i-j) < 4:\n\t\t\t\tgraph[i][j] = 1\n\t\t\t\tgraph[j][i] = 1\n\tfor i in id_num_list:\n\t\tfor j in id_num_list:\n\t\t\tgraph[i][j] = 1\n\t\t\tgraph[j][i] = 1\n\treturn graph\n\n# quantity cell graph\ndef get_quantity_cell_graph(max_len, id_num_list, sentence_length, quantity_cell_list,contain_zh_flag=True):\n\tdiag_ele = np.zeros(max_len)\n\tfor i in range(sentence_length):\n\t\tdiag_ele[i] = 1\n\tgraph = np.diag(diag_ele)\n\t#quantity_cell_list = quantity_cell_list.extend(id_num_list)\n\tif not contain_zh_flag:\n\t\treturn graph\n\tfor i in id_num_list:\n\t\tfor j in quantity_cell_list:\n\t\t\tif i < max_len and j < max_len and j not in id_num_list and abs(i-j) < 4:\n\t\t\t\tgraph[i][j] = 1\n\t\t\t\tgraph[j][i] = 1\n\treturn graph\n\ndef get_single_batch_graph(input_batch, input_length,group,num_value,num_pos):\n\t# pdb.set_trace()\n\tbatch_graph = []\n\tmax_len = max(input_length)\n\tfor i in range(len(input_length)):\n\t\tinput_batch_t = input_batch[i]\n\t\tsentence_length = input_length[i]\n\t\tquantity_cell_list = group[i]\n\t\tnum_list = num_value[i]\n\t\tid_num_list = num_pos[i]\n\t\tgraph_newc = get_quantity_cell_graph(max_len, id_num_list, sentence_length, quantity_cell_list)\n\t\tgraph_greater = get_greater_num_graph(max_len, sentence_length, num_list, id_num_list)\n\t\tgraph_lower = get_lower_num_graph(max_len, sentence_length, num_list, id_num_list)\n\t\tgraph_quanbet = get_quantity_between_graph(max_len, id_num_list, sentence_length, quantity_cell_list)\n\t\tgraph_attbet = get_attribute_between_graph(input_batch_t, max_len, id_num_list, sentence_length, quantity_cell_list)\n\t\t#graph_newc1 = get_quantity_graph1(input_batch_t, max_len, id_num_list, sentence_length, quantity_cell_list)\n\t\tgraph_total = [graph_newc.tolist(),graph_greater.tolist(),graph_lower.tolist(),graph_quanbet.tolist(),graph_attbet.tolist()]\n\t\tbatch_graph.append(graph_total)\n\tbatch_graph = np.array(batch_graph)\n\treturn batch_graph\n\ndef get_single_example_graph(input_batch, input_length,group,num_value,num_pos):\n\tbatch_graph = []\n\tmax_len = input_length\n\tsentence_length = input_length\n\tquantity_cell_list = group\n\tnum_list = num_value\n\tid_num_list = num_pos\n\tgraph_newc = get_quantity_cell_graph(max_len, id_num_list, sentence_length, quantity_cell_list)\n\tgraph_quanbet = get_quantity_between_graph(max_len, id_num_list, sentence_length, quantity_cell_list)\n\tgraph_attbet = get_attribute_between_graph(input_batch, max_len, id_num_list, sentence_length, quantity_cell_list)\n\tgraph_greater = get_greater_num_graph(max_len, sentence_length, num_list, id_num_list)\n\tgraph_lower = get_greater_num_graph(max_len, sentence_length, num_list, id_num_list)\n\t#graph_newc1 = get_quantity_graph1(input_batch, max_len, id_num_list, sentence_length, quantity_cell_list)\n\tgraph_total = [graph_newc.tolist(),graph_greater.tolist(),graph_lower.tolist(),graph_quanbet.tolist(),graph_attbet.tolist()]\n\tbatch_graph.append(graph_total)\n\tbatch_graph = np.array(batch_graph)\n\treturn batch_graph\n\n# prepare the batches\ndef prepare_train_batch(pairs_to_batch, batch_size):\n\tpairs = copy.deepcopy(pairs_to_batch)\n\trandom.shuffle(pairs)  # shuffle the pairs\n\tpos = 0\n\tinput_lengths = []\n\toutput_lengths = []\n\tnums_batches = []\n\tbatches = []\n\tinput_batches = []\n\toutput_batches = []\n\tnum_stack_batches = []  # save the num stack which\n\tnum_pos_batches = []\n\tnum_size_batches = []\n\tgroup_batches = []\n\tgraph_batches = []\n\tnum_value_batches = []\n\twhile pos + batch_size < len(pairs):\n\t\tbatches.append(pairs[pos:pos+batch_size])\n\t\tpos += batch_size\n\tbatches.append(pairs[pos:])\n\n\tfor batch in batches:\n\t\tbatch = sorted(batch, key=lambda tp: tp[1], reverse=True)\n\t\tinput_length = []\n\t\toutput_length = []\n\t\tfor _, i, _, j, _, _, _,_ in batch:\n\t\t\tinput_length.append(i)\n\t\t\toutput_length.append(j)\n\t\tinput_lengths.append(input_length)\n\t\toutput_lengths.append(output_length)\n\t\tinput_len_max = input_length[0]\n\t\toutput_len_max = max(output_length)\n\t\tinput_batch = []\n\t\toutput_batch = []\n\t\tnum_batch = []\n\t\tnum_stack_batch = []\n\t\tnum_pos_batch = []\n\t\tnum_size_batch = []\n\t\tgroup_batch = []\n\t\tnum_value_batch = []\n\t\tfor i, li, j, lj, num, num_pos, num_stack, group in batch:\n\t\t\tnum_batch.append(len(num))\n\t\t\tinput_batch.append(pad_seq(i, li, input_len_max))\n\t\t\toutput_batch.append(pad_seq(j, lj, output_len_max))\n\t\t\tnum_stack_batch.append(num_stack)\n\t\t\tnum_pos_batch.append(num_pos)\n\t\t\tnum_size_batch.append(len(num_pos))\n\t\t\tnum_value_batch.append(num)\n\t\t\tgroup_batch.append(group)\n\t\t\t\n\t\tinput_batches.append(input_batch)\n\t\tnums_batches.append(num_batch)\n\t\toutput_batches.append(output_batch)\n\t\tnum_stack_batches.append(num_stack_batch)\n\t\tnum_pos_batches.append(num_pos_batch)\n\t\tnum_size_batches.append(num_size_batch)\n\t\tnum_value_batches.append(num_value_batch)\n\t\tgroup_batches.append(group_batch)\n\t\tgraph_batches.append(get_single_batch_graph(input_batch, input_length,group_batch,num_value_batch,num_pos_batch))\n\t\t\n\treturn input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches, num_value_batches, graph_batches, group_batches\n\ndef get_num_stack(eq, output_lang, num_pos):\n\tnum_stack = []\n\tfor word in eq:\n\t\ttemp_num = []\n\t\tflag_not = True\n\t\tif word not in output_lang.index2word:\n\t\t\tflag_not = False\n\t\t\tfor i, j in enumerate(num_pos):\n\t\t\t\tif j == word:\n\t\t\t\t\ttemp_num.append(i)\n\t\tif not flag_not and len(temp_num) != 0:\n\t\t\tnum_stack.append(temp_num)\n\t\tif not flag_not and len(temp_num) == 0:\n\t\t\tnum_stack.append([_ for _ in range(len(num_pos))])\n\tnum_stack.reverse()\n\treturn num_stack\n\n\ndef prepare_de_train_batch(pairs_to_batch, batch_size, output_lang, rate, english=False):\n\tpairs = []\n\tb_pairs = copy.deepcopy(pairs_to_batch)\n\tfor pair in b_pairs:\n\t\tp = copy.deepcopy(pair)\n\t\tpair[2] = check_bracket(pair[2], english)\n\n\t\ttemp_out = exchange(pair[2], rate)\n\t\ttemp_out = check_bracket(temp_out, english)\n\n\t\tp[2] = indexes_from_sentence(output_lang, pair[2])\n\t\tp[3] = len(p[2])\n\t\tpairs.append(p)\n\n\t\ttemp_out_a = allocation(pair[2], rate)\n\t\ttemp_out_a = check_bracket(temp_out_a, english)\n\n\t\tif temp_out_a != pair[2]:\n\t\t\tp = copy.deepcopy(pair)\n\t\t\tp[6] = get_num_stack(temp_out_a, output_lang, p[4])\n\t\t\tp[2] = indexes_from_sentence(output_lang, temp_out_a)\n\t\t\tp[3] = len(p[2])\n\t\t\tpairs.append(p)\n\n\t\tif temp_out != pair[2]:\n\t\t\tp = copy.deepcopy(pair)\n\t\t\tp[6] = get_num_stack(temp_out, output_lang, p[4])\n\t\t\tp[2] = indexes_from_sentence(output_lang, temp_out)\n\t\t\tp[3] = len(p[2])\n\t\t\tpairs.append(p)\n\n\t\t\tif temp_out_a != pair[2]:\n\t\t\t\tp = copy.deepcopy(pair)\n\t\t\t\ttemp_out_a = allocation(temp_out, rate)\n\t\t\t\ttemp_out_a = check_bracket(temp_out_a, english)\n\t\t\t\tif temp_out_a != temp_out:\n\t\t\t\t\tp[6] = get_num_stack(temp_out_a, output_lang, p[4])\n\t\t\t\t\tp[2] = indexes_from_sentence(output_lang, temp_out_a)\n\t\t\t\t\tp[3] = len(p[2])\n\t\t\t\t\tpairs.append(p)\n\tprint(\"this epoch training data is\", len(pairs))\n\trandom.shuffle(pairs)  # shuffle the pairs\n\tpos = 0\n\tinput_lengths = []\n\toutput_lengths = []\n\tnums_batches = []\n\tbatches = []\n\tinput_batches = []\n\toutput_batches = []\n\tnum_stack_batches = []  # save the num stack which\n\tnum_pos_batches = []\n\twhile pos + batch_size < len(pairs):\n\t\tbatches.append(pairs[pos:pos+batch_size])\n\t\tpos += batch_size\n\tbatches.append(pairs[pos:])\n\n\tfor batch in batches:\n\t\tbatch = sorted(batch, key=lambda tp: tp[1], reverse=True)\n\t\tinput_length = []\n\t\toutput_length = []\n\t\tfor _, i, _, j, _, _, _ in batch:\n\t\t\tinput_length.append(i)\n\t\t\toutput_length.append(j)\n\t\tinput_lengths.append(input_length)\n\t\toutput_lengths.append(output_length)\n\t\tinput_len_max = input_length[0]\n\t\toutput_len_max = max(output_length)\n\t\tinput_batch = []\n\t\toutput_batch = []\n\t\tnum_batch = []\n\t\tnum_stack_batch = []\n\t\tnum_pos_batch = []\n\t\tfor i, li, j, lj, num, num_pos, num_stack in batch:\n\t\t\tnum_batch.append(len(num))\n\t\t\tinput_batch.append(pad_seq(i, li, input_len_max))\n\t\t\toutput_batch.append(pad_seq(j, lj, output_len_max))\n\t\t\tnum_stack_batch.append(num_stack)\n\t\t\tnum_pos_batch.append(num_pos)\n\t\tinput_batches.append(input_batch)\n\t\tnums_batches.append(num_batch)\n\t\toutput_batches.append(output_batch)\n\t\tnum_stack_batches.append(num_stack_batch)\n\t\tnum_pos_batches.append(num_pos_batch)\n\treturn input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches\n\n\n# Multiplication exchange rate\ndef exchange(ex_copy, rate):\n\tex = copy.deepcopy(ex_copy)\n\tidx = 1\n\twhile idx < len(ex):\n\t\ts = ex[idx]\n\t\tif (s == \"*\" or s == \"+\") and random.random() < rate:\n\t\t\tlidx = idx - 1\n\t\t\tridx = idx + 1\n\t\t\tif s == \"+\":\n\t\t\t\tflag = 0\n\t\t\t\twhile not (lidx == -1 or ((ex[lidx] == \"+\" or ex[lidx] == \"-\") and flag == 0) or flag == 1):\n\t\t\t\t\tif ex[lidx] == \")\" or ex[lidx] == \"]\":\n\t\t\t\t\t\tflag -= 1\n\t\t\t\t\telif ex[lidx] == \"(\" or ex[lidx] == \"[\":\n\t\t\t\t\t\tflag += 1\n\t\t\t\t\tlidx -= 1\n\t\t\t\tif flag == 1:\n\t\t\t\t\tlidx += 2\n\t\t\t\telse:\n\t\t\t\t\tlidx += 1\n\n\t\t\t\tflag = 0\n\t\t\t\twhile not (ridx == len(ex) or ((ex[ridx] == \"+\" or ex[ridx] == \"-\") and flag == 0) or flag == -1):\n\t\t\t\t\tif ex[ridx] == \")\" or ex[ridx] == \"]\":\n\t\t\t\t\t\tflag -= 1\n\t\t\t\t\telif ex[ridx] == \"(\" or ex[ridx] == \"[\":\n\t\t\t\t\t\tflag += 1\n\t\t\t\t\tridx += 1\n\t\t\t\tif flag == -1:\n\t\t\t\t\tridx -= 2\n\t\t\t\telse:\n\t\t\t\t\tridx -= 1\n\t\t\telse:\n\t\t\t\tflag = 0\n\t\t\t\twhile not (lidx == -1\n\t\t\t\t\t\t   or ((ex[lidx] == \"+\" or ex[lidx] == \"-\" or ex[lidx] == \"*\" or ex[lidx] == \"/\") and flag == 0)\n\t\t\t\t\t\t   or flag == 1):\n\t\t\t\t\tif ex[lidx] == \")\" or ex[lidx] == \"]\":\n\t\t\t\t\t\tflag -= 1\n\t\t\t\t\telif ex[lidx] == \"(\" or ex[lidx] == \"[\":\n\t\t\t\t\t\tflag += 1\n\t\t\t\t\tlidx -= 1\n\t\t\t\tif flag == 1:\n\t\t\t\t\tlidx += 2\n\t\t\t\telse:\n\t\t\t\t\tlidx += 1\n\n\t\t\t\tflag = 0\n\t\t\t\twhile not (ridx == len(ex)\n\t\t\t\t\t\t   or ((ex[ridx] == \"+\" or ex[ridx] == \"-\" or ex[ridx] == \"*\" or ex[ridx] == \"/\") and flag == 0)\n\t\t\t\t\t\t   or flag == -1):\n\t\t\t\t\tif ex[ridx] == \")\" or ex[ridx] == \"]\":\n\t\t\t\t\t\tflag -= 1\n\t\t\t\t\telif ex[ridx] == \"(\" or ex[ridx] == \"[\":\n\t\t\t\t\t\tflag += 1\n\t\t\t\t\tridx += 1\n\t\t\t\tif flag == -1:\n\t\t\t\t\tridx -= 2\n\t\t\t\telse:\n\t\t\t\t\tridx -= 1\n\t\t\tif lidx > 0 and ((s == \"+\" and ex[lidx - 1] == \"-\") or (s == \"*\" and ex[lidx - 1] == \"/\")):\n\t\t\t\tlidx -= 1\n\t\t\t\tex = ex[:lidx] + ex[idx:ridx + 1] + ex[lidx:idx] + ex[ridx + 1:]\n\t\t\telse:\n\t\t\t\tex = ex[:lidx] + ex[idx + 1:ridx + 1] + [s] + ex[lidx:idx] + ex[ridx + 1:]\n\t\t\tidx = ridx\n\t\tidx += 1\n\treturn ex\n\n\ndef check_bracket(x, english=False):\n\tif english:\n\t\tfor idx, s in enumerate(x):\n\t\t\tif s == '[':\n\t\t\t\tx[idx] = '('\n\t\t\telif s == '}':\n\t\t\t\tx[idx] = ')'\n\t\ts = x[0]\n\t\tidx = 0\n\t\tif s == \"(\":\n\t\t\tflag = 1\n\t\t\ttemp_idx = idx + 1\n\t\t\twhile flag > 0 and temp_idx < len(x):\n\t\t\t\tif x[temp_idx] == \")\":\n\t\t\t\t\tflag -= 1\n\t\t\t\telif x[temp_idx] == \"(\":\n\t\t\t\t\tflag += 1\n\t\t\t\ttemp_idx += 1\n\t\t\tif temp_idx == len(x):\n\t\t\t\tx = x[idx + 1:temp_idx - 1]\n\t\t\telif x[temp_idx] != \"*\" and x[temp_idx] != \"/\":\n\t\t\t\tx = x[idx + 1:temp_idx - 1] + x[temp_idx:]\n\t\twhile True:\n\t\t\ty = len(x)\n\t\t\tfor idx, s in enumerate(x):\n\t\t\t\tif s == \"+\" and idx + 1 < len(x) and x[idx + 1] == \"(\":\n\t\t\t\t\tflag = 1\n\t\t\t\t\ttemp_idx = idx + 2\n\t\t\t\t\twhile flag > 0 and temp_idx < len(x):\n\t\t\t\t\t\tif x[temp_idx] == \")\":\n\t\t\t\t\t\t\tflag -= 1\n\t\t\t\t\t\telif x[temp_idx] == \"(\":\n\t\t\t\t\t\t\tflag += 1\n\t\t\t\t\t\ttemp_idx += 1\n\t\t\t\t\tif temp_idx == len(x):\n\t\t\t\t\t\tx = x[:idx + 1] + x[idx + 2:temp_idx - 1]\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif x[temp_idx] != \"*\" and x[temp_idx] != \"/\":\n\t\t\t\t\t\tx = x[:idx + 1] + x[idx + 2:temp_idx - 1] + x[temp_idx:]\n\t\t\t\t\t\tbreak\n\t\t\tif y == len(x):\n\t\t\t\tbreak\n\t\treturn x\n\n\tlx = len(x)\n\tfor idx, s in enumerate(x):\n\t\tif s == \"[\":\n\t\t\tflag_b = 0\n\t\t\tflag = False\n\t\t\ttemp_idx = idx\n\t\t\twhile temp_idx < lx:\n\t\t\t\tif x[temp_idx] == \"]\":\n\t\t\t\t\tflag_b += 1\n\t\t\t\telif x[temp_idx] == \"[\":\n\t\t\t\t\tflag_b -= 1\n\t\t\t\tif x[temp_idx] == \"(\" or x[temp_idx] == \"[\":\n\t\t\t\t\tflag = True\n\t\t\t\tif x[temp_idx] == \"]\" and flag_b == 0:\n\t\t\t\t\tbreak\n\t\t\t\ttemp_idx += 1\n\t\t\tif not flag:\n\t\t\t\tx[idx] = \"(\"\n\t\t\t\tx[temp_idx] = \")\"\n\t\t\t\tcontinue\n\t\tif s == \"(\":\n\t\t\tflag_b = 0\n\t\t\tflag = False\n\t\t\ttemp_idx = idx\n\t\t\twhile temp_idx < lx:\n\t\t\t\tif x[temp_idx] == \")\":\n\t\t\t\t\tflag_b += 1\n\t\t\t\telif x[temp_idx] == \"(\":\n\t\t\t\t\tflag_b -= 1\n\t\t\t\tif x[temp_idx] == \"[\":\n\t\t\t\t\tflag = True\n\t\t\t\tif x[temp_idx] == \")\" and flag_b == 0:\n\t\t\t\t\tbreak\n\t\t\t\ttemp_idx += 1\n\t\t\tif not flag:\n\t\t\t\tx[idx] = \"[\"\n\t\t\t\tx[temp_idx] = \"]\"\n\treturn x\n\n\n# Multiplication allocation rate\ndef allocation(ex_copy, rate):\n\tex = copy.deepcopy(ex_copy)\n\tidx = 1\n\tlex = len(ex)\n\twhile idx < len(ex):\n\t\tif (ex[idx] == \"/\" or ex[idx] == \"*\") and (ex[idx - 1] == \"]\" or ex[idx - 1] == \")\"):\n\t\t\tridx = idx + 1\n\t\t\tr_allo = []\n\t\t\tr_last = []\n\t\t\tflag = 0\n\t\t\tflag_mmd = False\n\t\t\twhile ridx < lex:\n\t\t\t\tif ex[ridx] == \"(\" or ex[ridx] == \"[\":\n\t\t\t\t\tflag += 1\n\t\t\t\telif ex[ridx] == \")\" or ex[ridx] == \"]\":\n\t\t\t\t\tflag -= 1\n\t\t\t\tif flag == 0:\n\t\t\t\t\tif ex[ridx] == \"+\" or ex[ridx] == \"-\":\n\t\t\t\t\t\tr_last = ex[ridx:]\n\t\t\t\t\t\tr_allo = ex[idx + 1: ridx]\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif ex[ridx] == \"*\" or ex[ridx] == \"/\":\n\t\t\t\t\t\tflag_mmd = True\n\t\t\t\t\t\tr_last = [\")\"] + ex[ridx:]\n\t\t\t\t\t\tr_allo = ex[idx + 1: ridx]\n\t\t\t\t\t\tbreak\n\t\t\t\telif flag == -1:\n\t\t\t\t\tr_last = ex[ridx:]\n\t\t\t\t\tr_allo = ex[idx + 1: ridx]\n\t\t\t\t\tbreak\n\t\t\t\tridx += 1\n\t\t\tif len(r_allo) == 0:\n\t\t\t\tr_allo = ex[idx + 1:]\n\t\t\tflag = 0\n\t\t\tlidx = idx - 1\n\t\t\tflag_al = False\n\t\t\tflag_md = False\n\t\t\twhile lidx > 0:\n\t\t\t\tif ex[lidx] == \"(\" or ex[lidx] == \"[\":\n\t\t\t\t\tflag -= 1\n\t\t\t\telif ex[lidx] == \")\" or ex[lidx] == \"]\":\n\t\t\t\t\tflag += 1\n\t\t\t\tif flag == 1:\n\t\t\t\t\tif ex[lidx] == \"+\" or ex[lidx] == \"-\":\n\t\t\t\t\t\tflag_al = True\n\t\t\t\tif flag == 0:\n\t\t\t\t\tbreak\n\t\t\t\tlidx -= 1\n\t\t\tif lidx != 0 and ex[lidx - 1] == \"/\":\n\t\t\t\tflag_al = False\n\t\t\tif not flag_al:\n\t\t\t\tidx += 1\n\t\t\t\tcontinue\n\t\t\telif random.random() < rate:\n\t\t\t\ttemp_idx = lidx + 1\n\t\t\t\ttemp_res = ex[:lidx]\n\t\t\t\tif flag_mmd:\n\t\t\t\t\ttemp_res += [\"(\"]\n\t\t\t\tif lidx - 1 > 0:\n\t\t\t\t\tif ex[lidx - 1] == \"-\" or ex[lidx - 1] == \"*\" or ex[lidx - 1] == \"/\":\n\t\t\t\t\t\tflag_md = True\n\t\t\t\t\t\ttemp_res += [\"(\"]\n\t\t\t\tflag = 0\n\t\t\t\tlidx += 1\n\t\t\t\twhile temp_idx < idx - 1:\n\t\t\t\t\tif ex[temp_idx] == \"(\" or ex[temp_idx] == \"[\":\n\t\t\t\t\t\tflag -= 1\n\t\t\t\t\telif ex[temp_idx] == \")\" or ex[temp_idx] == \"]\":\n\t\t\t\t\t\tflag += 1\n\t\t\t\t\tif flag == 0:\n\t\t\t\t\t\tif ex[temp_idx] == \"+\" or ex[temp_idx] == \"-\":\n\t\t\t\t\t\t\ttemp_res += ex[lidx: temp_idx] + [ex[idx]] + r_allo + [ex[temp_idx]]\n\t\t\t\t\t\t\tlidx = temp_idx + 1\n\t\t\t\t\ttemp_idx += 1\n\t\t\t\ttemp_res += ex[lidx: temp_idx] + [ex[idx]] + r_allo\n\t\t\t\tif flag_md:\n\t\t\t\t\ttemp_res += [\")\"]\n\t\t\t\ttemp_res += r_last\n\t\t\t\treturn temp_res\n\t\tif ex[idx] == \"*\" and (ex[idx + 1] == \"[\" or ex[idx + 1] == \"(\"):\n\t\t\tlidx = idx - 1\n\t\t\tl_allo = []\n\t\t\ttemp_res = []\n\t\t\tflag = 0\n\t\t\tflag_md = False  # flag for x or /\n\t\t\twhile lidx > 0:\n\t\t\t\tif ex[lidx] == \"(\" or ex[lidx] == \"[\":\n\t\t\t\t\tflag += 1\n\t\t\t\telif ex[lidx] == \")\" or ex[lidx] == \"]\":\n\t\t\t\t\tflag -= 1\n\t\t\t\tif flag == 0:\n\t\t\t\t\tif ex[lidx] == \"+\":\n\t\t\t\t\t\ttemp_res = ex[:lidx + 1]\n\t\t\t\t\t\tl_allo = ex[lidx + 1: idx]\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif ex[lidx] == \"-\":\n\t\t\t\t\t\tflag_md = True  # flag for -\n\t\t\t\t\t\ttemp_res = ex[:lidx] + [\"(\"]\n\t\t\t\t\t\tl_allo = ex[lidx + 1: idx]\n\t\t\t\t\t\tbreak\n\t\t\t\telif flag == 1:\n\t\t\t\t\ttemp_res = ex[:lidx + 1]\n\t\t\t\t\tl_allo = ex[lidx + 1: idx]\n\t\t\t\t\tbreak\n\t\t\t\tlidx -= 1\n\t\t\tif len(l_allo) == 0:\n\t\t\t\tl_allo = ex[:idx]\n\t\t\tflag = 0\n\t\t\tridx = idx + 1\n\t\t\tflag_al = False\n\t\t\tall_res = []\n\t\t\twhile ridx < lex:\n\t\t\t\tif ex[ridx] == \"(\" or ex[ridx] == \"[\":\n\t\t\t\t\tflag -= 1\n\t\t\t\telif ex[ridx] == \")\" or ex[ridx] == \"]\":\n\t\t\t\t\tflag += 1\n\t\t\t\tif flag == 1:\n\t\t\t\t\tif ex[ridx] == \"+\" or ex[ridx] == \"-\":\n\t\t\t\t\t\tflag_al = True\n\t\t\t\tif flag == 0:\n\t\t\t\t\tbreak\n\t\t\t\tridx += 1\n\t\t\tif not flag_al:\n\t\t\t\tidx += 1\n\t\t\t\tcontinue\n\t\t\telif random.random() < rate:\n\t\t\t\ttemp_idx = idx + 1\n\t\t\t\tflag = 0\n\t\t\t\tlidx = temp_idx + 1\n\t\t\t\twhile temp_idx < idx - 1:\n\t\t\t\t\tif ex[temp_idx] == \"(\" or ex[temp_idx] == \"[\":\n\t\t\t\t\t\tflag -= 1\n\t\t\t\t\telif ex[temp_idx] == \")\" or ex[temp_idx] == \"]\":\n\t\t\t\t\t\tflag += 1\n\t\t\t\t\tif flag == 1:\n\t\t\t\t\t\tif ex[temp_idx] == \"+\" or ex[temp_idx] == \"-\":\n\t\t\t\t\t\t\tall_res += l_allo + [ex[idx]] + ex[lidx: temp_idx] + [ex[temp_idx]]\n\t\t\t\t\t\t\tlidx = temp_idx + 1\n\t\t\t\t\tif flag == 0:\n\t\t\t\t\t\tbreak\n\t\t\t\t\ttemp_idx += 1\n\t\t\t\tif flag_md:\n\t\t\t\t\ttemp_res += all_res + [\")\"]\n\t\t\t\telif ex[temp_idx + 1] == \"*\" or ex[temp_idx + 1] == \"/\":\n\t\t\t\t\ttemp_res += [\"(\"] + all_res + [\")\"]\n\t\t\t\ttemp_res += ex[temp_idx + 1:]\n\t\t\t\treturn temp_res\n\t\tidx += 1\n\treturn ex","metadata":{"execution":{"iopub.status.busy":"2024-11-16T10:53:07.278585Z","iopub.execute_input":"2024-11-16T10:53:07.279170Z","iopub.status.idle":"2024-11-16T10:53:07.601295Z","shell.execute_reply.started":"2024-11-16T10:53:07.279123Z","shell.execute_reply":"2024-11-16T10:53:07.600276Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## args.py","metadata":{}},{"cell_type":"code","source":"def build_parser():\n\t# Data loading parameters\n\tparser = argparse.ArgumentParser(description='Run Single sequence model')\n\n\tparser.add_argument('-mode', type=str, default='train', choices=['train', 'test'], help='Modes: train, test')\n\n\t# Run Config\n\tparser.add_argument('-run_name', type=str, default='debug', help='run name for logs')\n\tparser.add_argument('-dataset', type=str, default='asdiv-a_fold0_final', help='Dataset')\n\tparser.add_argument('-outputs', dest='outputs', action='store_true', help='Show full validation outputs')\n\tparser.add_argument('-no-outputs', dest='outputs', action='store_false', help='Do not show full validation outputs')\n\tparser.set_defaults(outputs=True)\n\tparser.add_argument('-results', dest='results', action='store_true', help='Store results')\n\tparser.add_argument('-no-results', dest='results', action='store_false', help='Do not store results')\n\tparser.set_defaults(results=True)\n\n\t# Meta Attributes\n\t# parser.add_argument('-vocab_size', type=int, default=30000, help='Vocabulary size to consider')\n\tparser.add_argument('-trim_threshold', type=int, default=1, help='Remove words with frequency less than this from vocab')\n\n\t# Device Configuration\n\tparser.add_argument('-gpu', type=int, default=2, help='Specify the gpu to use')\n\tparser.add_argument('-seed', type=int, default=6174, help='Default seed to set')\n\tparser.add_argument('-logging', type=int, default=1, help='Set to 0 if you do not require logging')\n\tparser.add_argument('-ckpt', type=str, default='model', help='Checkpoint file name')\n\tparser.add_argument('-save_model', dest='save_model',action='store_true', help='To save the model')\n\tparser.add_argument('-no-save_model', dest='save_model', action='store_false', help='Dont save the model')\n\tparser.set_defaults(save_model=True)\n\t# parser.add_argument('-log_fmt', type=str, default='%(asctime)s | %(levelname)s | %(name)s | %(message)s', help='Specify format of the logger')\n\n\t# Model parameters\n\t# parser.add_argument('-cell_type', type=str, default='gru', help='RNN cell for encoder, default: gru')\n\tparser.add_argument('-embedding', type=str, default='roberta', choices=['bert', 'roberta', 'word2vec', 'random'], help='Embeddings')\n\tparser.add_argument('-emb_name', type=str, default='roberta-base', choices=['bert-base-uncased', 'roberta-base'], help='Which pre-trained model')\n\tparser.add_argument('-embedding_size', type=int, default=768, help='Embedding dimensions of inputs')\n\tparser.add_argument('-emb_lr', type=float, default=1e-5, help='Larning rate to train embeddings')\n\tparser.add_argument('-freeze_emb', dest='freeze_emb', action='store_true', help='Freeze embedding weights')\n\tparser.add_argument('-no-freeze_emb', dest='freeze_emb', action='store_false', help='Train embedding weights')\n\tparser.set_defaults(freeze_emb=False)\n\tparser.add_argument('-word2vec_bin', type=str, default='/datadrive/global_files/GoogleNews-vectors-negative300.bin', help='Binary file of word2vec')\n\n\tparser.add_argument('-cell_type', type=str, default='lstm', help='RNN cell for encoder and decoder, default: lstm')\n\tparser.add_argument('-hidden_size', type=int, default=384, help='Number of hidden units in each layer')\n\tparser.add_argument('-depth', type=int, default=2, help='Number of layers in each encoder')\n\tparser.add_argument('-lr', type=float, default=1e-3, help='Learning rate')\n\tparser.add_argument('-batch_size', type=int, default=8, help='Batch size')\n\tparser.add_argument('-weight_decay', type=float, default=1e-5, help='Weight Decay')\n\tparser.add_argument('-beam_size', type=float, default=5, help='Beam Size')\n\tparser.add_argument('-epochs', type=int, default=70, help='Maximum # of training epochs')\t\n\tparser.add_argument('-dropout', type=float, default=0.5, help= 'Dropout probability for input/output/state units (0.0: no dropout)')\n\t\n\t# parser.add_argument('-max_length', type=int, default=100, help='Specify max decode steps: Max length string to output')\n\t# parser.add_argument('-init_range', type=float, default=0.08, help='Initialization range for seq2seq model')\n\t# parser.add_argument('-bidirectional', dest='bidirectional', action='store_true', help='Bidirectionality in LSTMs')\n\t# parser.add_argument('-no-bidirectional', dest='bidirectional', action='store_false', help='Bidirectionality in LSTMs')\n\t# parser.set_defaults(bidirectional=False)\n\t\n\t# parser.add_argument('-max_grad_norm', type=float, default=0.25, help='Clip gradients to this norm')\n\t# parser.add_argument('-opt', type=str, default='adam', choices=['adam', 'adadelta', 'sgd', 'asgd'], help='Optimizer for training')\n\n\t# parser.add_argument('-grade_disp', dest='grade_disp', action='store_true', help='Display grade information in validation outputs')\n\t# parser.add_argument('-no-grade_disp', dest='grade_disp', action='store_false', help='Don\\'t display grade information')\n\t# parser.set_defaults(grade_disp=True)\n\t# parser.add_argument('-type_disp', dest='type_disp', action='store_true', help='Display Type information in validation outputs')\n\t# parser.add_argument('-no-type_disp', dest='type_disp', action='store_false', help='Don\\'t display Type information')\n\t# parser.set_defaults(type_disp=True)\n\tparser.add_argument('-nums_disp', dest='nums_disp', action='store_true', help='Display number of numbers information in validation outputs')\n\tparser.add_argument('-no-nums_disp', dest='nums_disp', action='store_false', help='Don\\'t display number of numbers information')\n\tparser.set_defaults(nums_disp=True)\n\tparser.add_argument('-challenge_disp', dest='challenge_disp', action='store_true', help='Display information in validation outputs')\n\tparser.add_argument('-no-challenge_disp', dest='challenge_disp', action='store_false', help='Don\\'t display information')\n\tparser.set_defaults(challenge_disp=False)\n\n\tparser.add_argument('-show_train_acc', dest='show_train_acc', action='store_true', help='Calculate the train accuracy')\n\tparser.add_argument('-no-show_train_acc', dest='show_train_acc', action='store_false', help='Don\\'t calculate the train accuracy')\n\tparser.set_defaults(show_train_acc=True)\n\n\tparser.add_argument('-full_cv', dest='full_cv', action='store_true', help='5-fold CV')\n\tparser.add_argument('-no-full_cv', dest='full_cv', action='store_false', help='No 5-fold CV')\n\tparser.set_defaults(full_cv=False)\n\n\tparser.add_argument('-len_generate_nums', type=int, default=0, help='store length of generate_nums')\n\tparser.add_argument('-copy_nums', type=int, default=0, help='store copy_nums')\n\t\n\treturn parser\n\ndef parse_arguments(arg_dict=None):\n    parser = build_parser()\n    if arg_dict:\n        # Override default values with provided dictionary values\n        args = parser.parse_args([])\n        for key, value in arg_dict.items():\n            setattr(args, key, value)\n        return args\n    else:\n        return parser.parse_args()  # If no dictionary is provided, use default command line arguments","metadata":{"execution":{"iopub.status.busy":"2024-11-16T10:53:07.603357Z","iopub.execute_input":"2024-11-16T10:53:07.603704Z","iopub.status.idle":"2024-11-16T10:53:07.631316Z","shell.execute_reply.started":"2024-11-16T10:53:07.603668Z","shell.execute_reply":"2024-11-16T10:53:07.630175Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## train and evaluate","metadata":{}},{"cell_type":"code","source":"MAX_OUTPUT_LENGTH = 45\nMAX_INPUT_LENGTH = 120\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass Beam:  # the class save the beam node\n\tdef __init__(self, score, input_var, hidden, all_output):\n\t\tself.score = score\n\t\tself.input_var = input_var\n\t\tself.hidden = hidden\n\t\tself.all_output = all_output\n\n\ndef time_since(s):  # compute time\n\tm = math.floor(s / 60)\n\ts -= m * 60\n\th = math.floor(m / 60)\n\tm -= h * 60\n\treturn '%dh %dm %ds' % (h, m, s)\n\n\ndef generate_rule_mask(decoder_input, nums_batch, word2index, batch_size, nums_start, copy_nums, generate_nums,\n\t\t\t\t\t   english):\n\trule_mask = torch.FloatTensor(batch_size, nums_start + copy_nums).fill_(-float(\"1e12\"))\n\tif english:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + \\\n\t\t\t\t\t  [word2index[\"(\"]] + generate_nums\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start:\n\t\t\t\tres += [word2index[\")\"], word2index[\"+\"], word2index[\"-\"],\n\t\t\t\t\t\tword2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] in generate_nums:\n\t\t\t\tres += [word2index[\")\"], word2index[\"+\"], word2index[\"-\"],\n\t\t\t\t\t\tword2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] == word2index[\"(\"]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] +\\\n\t\t\t\t  [word2index[\"(\"]] + generate_nums\n\t\t\telif decoder_input[i] == word2index[\")\"]:\n\t\t\t\tres += [word2index[\")\"], word2index[\"+\"], word2index[\"-\"],\n\t\t\t\t\t\tword2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + [word2index[\"(\"]] + generate_nums\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\telse:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + \\\n\t\t\t\t\t  [word2index[\"[\"], word2index[\"(\"]] + generate_nums\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [word2index[\"]\"], word2index[\")\"], word2index[\"+\"],\n\t\t\t\t\t\tword2index[\"-\"], word2index[\"/\"], word2index[\"^\"],\n\t\t\t\t\t\tword2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] == word2index[\"[\"] or decoder_input[i] == word2index[\"(\"]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] +\\\n\t\t\t\t  [word2index[\"(\"]] + generate_nums\n\t\t\telif decoder_input[i] == word2index[\")\"]:\n\t\t\t\tres += [word2index[\"]\"], word2index[\")\"], word2index[\"+\"],\n\t\t\t\t\t\tword2index[\"-\"], word2index[\"/\"], word2index[\"^\"],\n\t\t\t\t\t\tword2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"]\"]:\n\t\t\t\tres += [word2index[\"+\"], word2index[\"*\"], word2index[\"-\"], word2index[\"/\"], word2index[\"EOS\"]]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"],\n\t\t\t\t\t\t\t\t\t  word2index[\"*\"], word2index[\"^\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] +\\\n\t\t\t\t  [word2index[\"[\"], word2index[\"(\"]] + generate_nums\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\treturn rule_mask\n\n\ndef generate_pre_tree_seq_rule_mask(decoder_input, nums_batch, word2index, batch_size, nums_start, copy_nums,\n\t\t\t\t\t\t\t\t\tgenerate_nums, english):\n\trule_mask = torch.FloatTensor(batch_size, nums_start + copy_nums).fill_(-float(\"1e12\"))\n\tif english:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t  [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\telse:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t  [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"^\"]]\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"],\n\t\t\t\t\t\tword2index[\"^\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"],\n\t\t\t\t\t\t\t\t\t  word2index[\"^\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"^\"]]\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\treturn rule_mask\n\n\ndef generate_post_tree_seq_rule_mask(decoder_input, nums_batch, word2index, batch_size, nums_start, copy_nums,\n\t\t\t\t\t\t\t\t\t generate_nums, english):\n\trule_mask = torch.FloatTensor(batch_size, nums_start + copy_nums).fill_(-float(\"1e12\"))\n\tif english:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums +\\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\telse:\n\t\tif decoder_input[0] == word2index[\"SOS\"]:\n\t\t\tfor i in range(batch_size):\n\t\t\t\tres = [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums\n\t\t\t\tfor j in res:\n\t\t\t\t\trule_mask[i, j] = 0\n\t\t\treturn rule_mask\n\t\tfor i in range(batch_size):\n\t\t\tres = []\n\t\t\tif decoder_input[i] >= nums_start or decoder_input[i] in generate_nums:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"^\"]\n\t\t\t\t\t\t]\n\t\t\telif decoder_input[i] == word2index[\"EOS\"] or decoder_input[i] == PAD_token:\n\t\t\t\tres += [PAD_token]\n\t\t\telif decoder_input[i] in [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"],\n\t\t\t\t\t\t\t\t\t  word2index[\"^\"]]:\n\t\t\t\tres += [_ for _ in range(nums_start, nums_start + nums_batch[i])] + generate_nums + \\\n\t\t\t\t\t   [word2index[\"+\"], word2index[\"-\"], word2index[\"/\"], word2index[\"*\"], word2index[\"^\"],\n\t\t\t\t\t\tword2index[\"EOS\"]\n\t\t\t\t\t\t]\n\t\t\tfor j in res:\n\t\t\t\trule_mask[i, j] = 0\n\treturn rule_mask\n\n\ndef generate_tree_input(target, decoder_output, nums_stack_batch, num_start, unk):\n\t# when the decoder input is copied num but the num has two pos, chose the max\n\ttarget_input = copy.deepcopy(target)\n\tfor i in range(len(target)):\n\t\tif target[i] == unk:\n\t\t\tnum_stack = nums_stack_batch[i].pop()\n\t\t\tmax_score = -float(\"1e12\")\n\t\t\tfor num in num_stack:\n\t\t\t\tif decoder_output[i, num_start + num] > max_score:\n\t\t\t\t\ttarget[i] = num + num_start\n\t\t\t\t\tmax_score = decoder_output[i, num_start + num]\n\t\tif target_input[i] >= num_start:\n\t\t\ttarget_input[i] = 0\n\treturn torch.LongTensor(target), torch.LongTensor(target_input)\n\n\ndef generate_decoder_input(target, decoder_output, nums_stack_batch, num_start, unk):\n\t# when the decoder input is copied num but the num has two pos, chose the max\n\tif USE_CUDA:\n\t\tdecoder_output = decoder_output.cpu()\n\tfor i in range(target.size(0)):\n\t\tif target[i] == unk:\n\t\t\tnum_stack = nums_stack_batch[i].pop()\n\t\t\tmax_score = -float(\"1e12\")\n\t\t\tfor num in num_stack:\n\t\t\t\tif decoder_output[i, num_start + num] > max_score:\n\t\t\t\t\ttarget[i] = num + num_start\n\t\t\t\t\tmax_score = decoder_output[i, num_start + num]\n\treturn target\n\n\ndef mask_num(encoder_outputs, decoder_input, embedding_size, nums_start, copy_nums, num_pos):\n\t# mask the decoder input number and return the mask tensor and the encoder position Hidden vector\n\tup_num_start = decoder_input >= nums_start\n\tdown_num_end = decoder_input < (nums_start + copy_nums)\n\tnum_mask = up_num_start == down_num_end\n\tnum_mask_encoder = num_mask < 1\n\tnum_mask_encoder = num_mask_encoder.unsqueeze(1)  # ByteTensor size: B x 1\n\trepeat_dims = [1] * num_mask_encoder.dim()\n\trepeat_dims[1] = embedding_size\n\tnum_mask_encoder = num_mask_encoder.repeat(*repeat_dims)  # B x 1 -> B x Decoder_embedding_size\n\n\tall_embedding = encoder_outputs.transpose(0, 1).contiguous()\n\tall_embedding = all_embedding.view(-1, encoder_outputs.size(2))  # S x B x H -> (B x S) x H\n\tindices = decoder_input - nums_start\n\tindices = indices * num_mask.long()  # 0 or the num pos in sentence\n\tindices = indices.tolist()\n\tfor k in range(len(indices)):\n\t\tindices[k] = num_pos[k][indices[k]]\n\tindices = torch.LongTensor(indices)\n\tif USE_CUDA:\n\t\tindices = indices.cuda()\n\tbatch_size = decoder_input.size(0)\n\tsen_len = encoder_outputs.size(0)\n\tbatch_num = torch.LongTensor(range(batch_size))\n\tbatch_num = batch_num * sen_len\n\tif USE_CUDA:\n\t\tbatch_num = batch_num.cuda()\n\tindices = batch_num + indices\n\tnum_encoder = all_embedding.index_select(0, indices)\n\treturn num_mask, num_encoder, num_mask_encoder\n\n\ndef out_equation(test, output_lang, num_list, num_stack=None):\n\ttest = test[:-1]\n\tmax_index = len(output_lang.index2word) - 1\n\ttest_str = \"\"\n\tfor i in test:\n\t\tif i < max_index:\n\t\t\tc = output_lang.index2word[i]\n\t\t\tif c == \"^\":\n\t\t\t\ttest_str += \"**\"\n\t\t\telif c == \"[\":\n\t\t\t\ttest_str += \"(\"\n\t\t\telif c == \"]\":\n\t\t\t\ttest_str += \")\"\n\t\t\telif c[0] == \"N\":\n\t\t\t\tif int(c[1:]) >= len(num_list):\n\t\t\t\t\treturn None\n\t\t\t\tx = num_list[int(c[1:])]\n\t\t\t\tif x[-1] == \"%\":\n\t\t\t\t\ttest_str += \"(\" + x[:-1] + \"/100\" + \")\"\n\t\t\t\telse:\n\t\t\t\t\ttest_str += x\n\t\t\telse:\n\t\t\t\ttest_str += c\n\t\telse:\n\t\t\tif len(num_stack) == 0:\n\t\t\t\tprint(test_str, num_list)\n\t\t\t\treturn \"\"\n\t\t\tn_pos = num_stack.pop()\n\t\t\ttest_str += num_list[n_pos[0]]\n\treturn test_str\n\n\ndef compute_prefix_tree_result(test_res, test_tar, output_lang, num_list, num_stack):\n\t# print(test_res, test_tar)\n\n\tif len(num_stack) == 0 and test_res == test_tar:\n\t\treturn True, True, test_res, test_tar\n\ttest = out_expression_list(test_res, output_lang, num_list)\n\ttar = out_expression_list(test_tar, output_lang, num_list, copy.deepcopy(num_stack))\n\t# print(test, tar)\n\tif test is None:\n\t\treturn False, False, test, tar\n\tif test == tar:\n\t\treturn True, True, test, tar\n\ttry:\n\t\tif abs(compute_prefix_expression(test) - compute_prefix_expression(tar)) < 1e-4:\n\t\t\treturn True, False, test, tar\n\t\telse:\n\t\t\treturn False, False, test, tar\n\texcept:\n\t\treturn False, False, test, tar\n\n\ndef compute_postfix_tree_result(test_res, test_tar, output_lang, num_list, num_stack):\n\t# print(test_res, test_tar)\n\n\tif len(num_stack) == 0 and test_res == test_tar:\n\t\treturn True, True, test_res, test_tar\n\ttest = out_expression_list(test_res, output_lang, num_list)\n\ttar = out_expression_list(test_tar, output_lang, num_list, copy.deepcopy(num_stack))\n\t# print(test, tar)\n\tif test is None:\n\t\treturn False, False, test, tar\n\tif test == tar:\n\t\treturn True, True, test, tar\n\ttry:\n\t\tif abs(compute_postfix_expression(test) - compute_postfix_expression(tar)) < 1e-4:\n\t\t\treturn True, False, test, tar\n\t\telse:\n\t\t\treturn False, False, test, tar\n\texcept:\n\t\treturn False, False, test, tar\n\n\ndef compute_result(test_res, test_tar, output_lang, num_list, num_stack):\n\tif len(num_stack) == 0 and test_res == test_tar:\n\t\treturn True, True\n\ttest = out_equation(test_res, output_lang, num_list)\n\ttar = out_equation(test_tar, output_lang, num_list, copy.deepcopy(num_stack))\n\tif test is None:\n\t\treturn False, False\n\tif test == tar:\n\t\treturn True, True\n\ttry:\n\t\tif abs(eval(test) - eval(tar)) < 1e-4:\n\t\t\treturn True, False\n\t\telse:\n\t\t\treturn False, False\n\texcept:\n\t\treturn False, False\n\n\ndef get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size, hidden_size):\n\tindices = list()\n\tsen_len = encoder_outputs.size(0)\n\tmasked_index = []\n\ttemp_1 = [1 for _ in range(hidden_size)]\n\ttemp_0 = [0 for _ in range(hidden_size)]\n\tfor b in range(batch_size):\n\t\tfor i in num_pos[b]:\n\t\t\tindices.append(i + b * sen_len)\n\t\t\tmasked_index.append(temp_0)\n\t\tindices += [0 for _ in range(len(num_pos[b]), num_size)]\n\t\tmasked_index += [temp_1 for _ in range(len(num_pos[b]), num_size)]\n\tindices = torch.LongTensor(indices)\n\tmasked_index = torch.BoolTensor(masked_index)\n\tmasked_index = masked_index.view(batch_size, num_size, hidden_size)\n\tif USE_CUDA:\n\t\tindices = indices.cuda()\n\t\tmasked_index = masked_index.cuda()\n\tall_outputs = encoder_outputs.transpose(0, 1).contiguous()\n\tall_embedding = all_outputs.view(-1, encoder_outputs.size(2))  # S x B x H -> (B x S) x H\n\tall_num = all_embedding.index_select(0, indices)\n\tall_num = all_num.view(batch_size, num_size, hidden_size)\n\treturn all_num.masked_fill_(masked_index, 0.0)\n\n\ndef train_attn(input_batch, input_length, target_batch, target_length, num_batch, nums_stack_batch, copy_nums,\n\t\t\t   generate_nums, encoder, decoder, encoder_optimizer, decoder_optimizer, output_lang, clip=0,\n\t\t\t   use_teacher_forcing=1, beam_size=1, english=False):\n\tseq_mask = []\n\tmax_len = max(input_length)\n\tfor i in input_length:\n\t\tseq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n\tseq_mask = torch.BoolTensor(seq_mask)\n\n\tnum_start = output_lang.n_words - copy_nums - 2\n\tunk = output_lang.word2index[\"UNK\"]\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).transpose(0, 1)\n\ttarget = torch.LongTensor(target_batch).transpose(0, 1)\n\n\tbatch_size = len(input_length)\n\n\tencoder.train()\n\tdecoder.train()\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\tseq_mask = seq_mask.cuda()\n\n\t# Zero gradients of both optimizers\n\tencoder_optimizer.zero_grad()\n\tdecoder_optimizer.zero_grad()\n\t# Run words through encoder\n\tencoder_outputs, encoder_hidden = encoder(input_var, input_length, None)\n\n\t# Prepare input and output variables\n\tdecoder_input = torch.LongTensor([output_lang.word2index[\"SOS\"]] * batch_size)\n\n\tdecoder_hidden = encoder_hidden[:decoder.n_layers]  # Use last (forward) hidden state from encoder\n\n\tmax_target_length = max(target_length)\n\tall_decoder_outputs = torch.zeros(max_target_length, batch_size, decoder.output_size)\n\n\t# Move new Variables to CUDA\n\tif USE_CUDA:\n\t\tall_decoder_outputs = all_decoder_outputs.cuda()\n\n\tif random.random() < use_teacher_forcing:\n\t\t# Run through decoder one time step at a time\n\t\tfor t in range(max_target_length):\n\t\t\tif USE_CUDA:\n\t\t\t\tdecoder_input = decoder_input.cuda()\n\n\t\t\tdecoder_output, decoder_hidden = decoder(\n\t\t\t\tdecoder_input, decoder_hidden, encoder_outputs, seq_mask)\n\t\t\tall_decoder_outputs[t] = decoder_output\n\t\t\tdecoder_input = generate_decoder_input(\n\t\t\t\ttarget[t], decoder_output, nums_stack_batch, num_start, unk)\n\t\t\ttarget[t] = decoder_input\n\telse:\n\t\tbeam_list = list()\n\t\tscore = torch.zeros(batch_size)\n\t\tif USE_CUDA:\n\t\t\tscore = score.cuda()\n\t\tbeam_list.append(Beam(score, decoder_input, decoder_hidden, all_decoder_outputs))\n\t\t# Run through decoder one time step at a time\n\t\tfor t in range(max_target_length):\n\t\t\tbeam_len = len(beam_list)\n\t\t\tbeam_scores = torch.zeros(batch_size, decoder.output_size * beam_len)\n\t\t\tall_hidden = torch.zeros(decoder_hidden.size(0), batch_size * beam_len, decoder_hidden.size(2))\n\t\t\tall_outputs = torch.zeros(max_target_length, batch_size * beam_len, decoder.output_size)\n\t\t\tif USE_CUDA:\n\t\t\t\tbeam_scores = beam_scores.cuda()\n\t\t\t\tall_hidden = all_hidden.cuda()\n\t\t\t\tall_outputs = all_outputs.cuda()\n\n\t\t\tfor b_idx in range(len(beam_list)):\n\t\t\t\tdecoder_input = beam_list[b_idx].input_var\n\t\t\t\tdecoder_hidden = beam_list[b_idx].hidden\n\n\t\t\t\trule_mask = generate_rule_mask(decoder_input, num_batch, output_lang.word2index, batch_size,\n\t\t\t\t\t\t\t\t\t\t\t   num_start, copy_nums, generate_nums, english)\n\t\t\t\tif USE_CUDA:\n\t\t\t\t\trule_mask = rule_mask.cuda()\n\t\t\t\t\tdecoder_input = decoder_input.cuda()\n\n\t\t\t\tdecoder_output, decoder_hidden = decoder(\n\t\t\t\t\tdecoder_input, decoder_hidden, encoder_outputs, seq_mask)\n\n\t\t\t\tscore = f.log_softmax(decoder_output, dim=1) + rule_mask\n\t\t\t\tbeam_score = beam_list[b_idx].score\n\t\t\t\tbeam_score = beam_score.unsqueeze(1)\n\t\t\t\trepeat_dims = [1] * beam_score.dim()\n\t\t\t\trepeat_dims[1] = score.size(1)\n\t\t\t\tbeam_score = beam_score.repeat(*repeat_dims)\n\t\t\t\tscore += beam_score\n\t\t\t\tbeam_scores[:, b_idx * decoder.output_size: (b_idx + 1) * decoder.output_size] = score\n\t\t\t\tall_hidden[:, b_idx * batch_size:(b_idx + 1) * batch_size, :] = decoder_hidden\n\n\t\t\t\tbeam_list[b_idx].all_output[t] = decoder_output\n\t\t\t\tall_outputs[:, batch_size * b_idx: batch_size * (b_idx + 1), :] = \\\n\t\t\t\t\tbeam_list[b_idx].all_output\n\t\t\ttopv, topi = beam_scores.topk(beam_size, dim=1)\n\t\t\tbeam_list = list()\n\n\t\t\tfor k in range(beam_size):\n\t\t\t\ttemp_topk = topi[:, k]\n\t\t\t\ttemp_input = temp_topk % decoder.output_size\n\t\t\t\ttemp_input = temp_input.data\n\t\t\t\tif USE_CUDA:\n\t\t\t\t\ttemp_input = temp_input.cpu()\n\t\t\t\ttemp_beam_pos = temp_topk / decoder.output_size\n\n\t\t\t\tindices = torch.LongTensor(range(batch_size))\n\t\t\t\tif USE_CUDA:\n\t\t\t\t\tindices = indices.cuda()\n\t\t\t\tindices += temp_beam_pos * batch_size\n\n\t\t\t\ttemp_hidden = all_hidden.index_select(1, indices)\n\t\t\t\ttemp_output = all_outputs.index_select(1, indices)\n\n\t\t\t\tbeam_list.append(Beam(topv[:, k], temp_input, temp_hidden, temp_output))\n\t\tall_decoder_outputs = beam_list[0].all_output\n\n\t\tfor t in range(max_target_length):\n\t\t\ttarget[t] = generate_decoder_input(\n\t\t\t\ttarget[t], all_decoder_outputs[t], nums_stack_batch, num_start, unk)\n\t# Loss calculation and backpropagation\n\n\tif USE_CUDA:\n\t\ttarget = target.cuda()\n\n\tloss = masked_cross_entropy(\n\t\tall_decoder_outputs.transpose(0, 1).contiguous(),  # -> batch x seq\n\t\ttarget.transpose(0, 1).contiguous(),  # -> batch x seq\n\t\ttarget_length\n\t)\n\n\tloss.backward()\n\treturn_loss = loss.item()\n\n\t# Clip gradient norms\n\tif clip:\n\t\ttorch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n\t\ttorch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n\n\t# Update parameters with optimizers\n\tencoder_optimizer.step()\n\tdecoder_optimizer.step()\n\n\treturn return_loss\n\n\ndef evaluate_attn(input_seq, input_length, num_list, copy_nums, generate_nums, encoder, decoder, output_lang,\n\t\t\t\t  beam_size=1, english=False, max_length=MAX_OUTPUT_LENGTH):\n\tseq_mask = torch.BoolTensor(1, input_length).fill_(0)\n\tnum_start = output_lang.n_words - copy_nums - 2\n\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_seq).unsqueeze(1)\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\tseq_mask = seq_mask.cuda()\n\n\t# Set to not-training mode to disable dropout\n\tencoder.eval()\n\tdecoder.eval()\n\n\t# Run through encoder\n\tencoder_outputs, encoder_hidden = encoder(input_var, [input_length], None)\n\n\t# Create starting vectors for decoder\n\tdecoder_input = torch.LongTensor([output_lang.word2index[\"SOS\"]])  # SOS\n\tdecoder_hidden = encoder_hidden[:decoder.n_layers]  # Use last (forward) hidden state from encoder\n\tbeam_list = list()\n\tscore = 0\n\tbeam_list.append(Beam(score, decoder_input, decoder_hidden, []))\n\n\t# Run through decoder\n\tfor di in range(max_length):\n\t\ttemp_list = list()\n\t\tbeam_len = len(beam_list)\n\t\tfor xb in beam_list:\n\t\t\tif int(xb.input_var[0]) == output_lang.word2index[\"EOS\"]:\n\t\t\t\ttemp_list.append(xb)\n\t\t\t\tbeam_len -= 1\n\t\tif beam_len == 0:\n\t\t\treturn beam_list[0].all_output\n\t\tbeam_scores = torch.zeros(decoder.output_size * beam_len)\n\t\thidden_size_0 = decoder_hidden.size(0)\n\t\thidden_size_2 = decoder_hidden.size(2)\n\t\tall_hidden = torch.zeros(beam_len, hidden_size_0, 1, hidden_size_2)\n\t\tif USE_CUDA:\n\t\t\tbeam_scores = beam_scores.cuda()\n\t\t\tall_hidden = all_hidden.cuda()\n\t\tall_outputs = []\n\t\tcurrent_idx = -1\n\n\t\tfor b_idx in range(len(beam_list)):\n\t\t\tdecoder_input = beam_list[b_idx].input_var\n\t\t\tif int(decoder_input[0]) == output_lang.word2index[\"EOS\"]:\n\t\t\t\tcontinue\n\t\t\tcurrent_idx += 1\n\t\t\tdecoder_hidden = beam_list[b_idx].hidden\n\n\t\t\t# rule_mask = generate_rule_mask(decoder_input, [num_list], output_lang.word2index,\n\t\t\t#                                1, num_start, copy_nums, generate_nums, english)\n\t\t\tif USE_CUDA:\n\t\t\t\t# rule_mask = rule_mask.cuda()\n\t\t\t\tdecoder_input = decoder_input.cuda()\n\n\t\t\tdecoder_output, decoder_hidden = decoder(\n\t\t\t\tdecoder_input, decoder_hidden, encoder_outputs, seq_mask)\n\t\t\t# score = f.log_softmax(decoder_output, dim=1) + rule_mask.squeeze()\n\t\t\tscore = f.log_softmax(decoder_output, dim=1)\n\t\t\tscore += beam_list[b_idx].score\n\t\t\tbeam_scores[current_idx * decoder.output_size: (current_idx + 1) * decoder.output_size] = score\n\t\t\tall_hidden[current_idx] = decoder_hidden\n\t\t\tall_outputs.append(beam_list[b_idx].all_output)\n\t\ttopv, topi = beam_scores.topk(beam_size)\n\n\t\tfor k in range(beam_size):\n\t\t\tword_n = int(topi[k])\n\t\t\tword_input = word_n % decoder.output_size\n\t\t\ttemp_input = torch.LongTensor([word_input])\n\t\t\tindices = int(word_n / decoder.output_size)\n\n\t\t\ttemp_hidden = all_hidden[indices]\n\t\t\ttemp_output = all_outputs[indices]+[word_input]\n\t\t\ttemp_list.append(Beam(float(topv[k]), temp_input, temp_hidden, temp_output))\n\n\t\ttemp_list = sorted(temp_list, key=lambda x: x.score, reverse=True)\n\n\t\tif len(temp_list) < beam_size:\n\t\t\tbeam_list = temp_list\n\t\telse:\n\t\t\tbeam_list = temp_list[:beam_size]\n\treturn beam_list[0].all_output\n\n\ndef copy_list(l):\n\tr = []\n\tif len(l) == 0:\n\t\treturn r\n\tfor i in l:\n\t\tif type(i) is list:\n\t\t\tr.append(copy_list(i))\n\t\telse:\n\t\t\tr.append(i)\n\treturn r\n\n\nclass TreeBeam:  # the class save the beam node\n\tdef __init__(self, score, node_stack, embedding_stack, left_childs, out):\n\t\tself.score = score\n\t\tself.embedding_stack = copy_list(embedding_stack)\n\t\tself.node_stack = copy_list(node_stack)\n\t\tself.left_childs = copy_list(left_childs)\n\t\tself.out = copy.deepcopy(out)\n\n\nclass TreeEmbedding:  # the class save the tree\n\tdef __init__(self, embedding, terminal=False):\n\t\tself.embedding = embedding\n\t\tself.terminal = terminal\n\n\ndef train_tree(config, input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, num_value_batch, group_batch, generate_nums,\n\t\t\t   embedding, encoder, predict, generate, merge, embedding_optimizer, encoder_optimizer, predict_optimizer, generate_optimizer,\n\t\t\t   merge_optimizer, input_lang, output_lang, num_pos, batch_graph, english=False):\n\tnum_mask = []\n\tmax_num_size = max(num_size_batch) + len(generate_nums)\n\tfor i in num_size_batch:\n\t\td = i + len(generate_nums)\n\t\tnum_mask.append([0] * d + [1] * (max_num_size - d))\n\tnum_mask = torch.BoolTensor(num_mask)\n\n\tunk = output_lang.word2index[\"UNK\"]\n\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).transpose(0, 1)\n\n\ttarget = torch.LongTensor(target_batch).transpose(0, 1)\n\tbatch_graph = torch.LongTensor(batch_graph)\n\n\tpadding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n\tbatch_size = len(input_length)\n\n\tembedding.train()\n\tencoder.train()\n\tpredict.train()\n\tgenerate.train()\n\tmerge.train()\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\t# seq_mask = seq_mask.cuda()\n\t\tpadding_hidden = padding_hidden.cuda()\n\t\tnum_mask = num_mask.cuda()\n\t\t# batch_graph = batch_graph.cuda()\n\n\t# Zero gradients of both optimizers\n\tembedding_optimizer.zero_grad()\n\tencoder_optimizer.zero_grad()\n\tpredict_optimizer.zero_grad()\n\tgenerate_optimizer.zero_grad()\n\tmerge_optimizer.zero_grad()\n\t# Run words through encoder\n\n\torig_idx = None\n\tembedded = None\n\tif config.embedding == 'bert' or config.embedding == 'roberta':\n\t\tcontextual_input = index_batch_to_words(input_batch, input_length, input_lang)\n\t\tinput_seq1, input_len1, token_ids, index_retrieve = embedding(contextual_input)\n\t\t\n\t\tnew_group_batch = []\n\t\tfor bat in range(len(group_batch)):\n\t\t\ttry:\n\t\t\t\tnew_group_batch.append([index_retrieve[bat][index1] for index1 in group_batch[bat] if index1 < len(index_retrieve[bat])])\n\t\t\texcept:\n\t\t\t\tpdb.set_trace()\n\n\t\tbatch_graph = get_single_batch_graph(token_ids.cpu().tolist(), input_len1, new_group_batch, num_value_batch, num_pos)\n\t\tbatch_graph = torch.LongTensor(batch_graph)\n\n\t\tinput_seq1 = input_seq1.transpose(0,1)\n\t\tembedded, input_length, orig_idx = sort_by_len(input_seq1, input_len1, gpu_init_pytorch(config.gpu))\n\telse:\n\t\tembedded = embedding(input_var)\n\n\tif USE_CUDA:\n\t\tbatch_graph = batch_graph.cuda()\n\n\tencoder_outputs, problem_output = encoder(embedded, input_length, orig_idx, batch_graph)\n\n\t# sequence mask for attention\n\tseq_mask = []\n\tmax_len = max(input_length)\n\tfor i in input_length:\n\t\tseq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n\tseq_mask = torch.BoolTensor(seq_mask)\n\n\tif USE_CUDA:\n\t\tseq_mask = seq_mask.cuda()\n\n\t# Prepare input and output variables\n\tnode_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]\n\n\tmax_target_length = max(target_length)\n\n\tall_node_outputs = []\n\t# all_leafs = []\n\n\tcopy_num_len = [len(_) for _ in num_pos]\n\tnum_size = max(copy_num_len)\n\tall_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  encoder.hidden_size)\n\n\tnum_start = output_lang.num_start\n\tembeddings_stacks = [[] for _ in range(batch_size)]\n\tleft_childs = [None for _ in range(batch_size)]\n\tfor t in range(max_target_length):\n\t\tnum_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n\t\t\tnode_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask)\n\n\t\t# all_leafs.append(p_leaf)\n\t\toutputs = torch.cat((op, num_score), 1)\n\t\tall_node_outputs.append(outputs)\n\n\t\ttarget_t, generate_input = generate_tree_input(target[t].tolist(), outputs, nums_stack_batch, num_start, unk)\n\t\ttarget[t] = target_t\n\t\tif USE_CUDA:\n\t\t\tgenerate_input = generate_input.cuda()\n\t\tleft_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)\n\t\tleft_childs = []\n\t\tfor idx, l, r, node_stack, i, o in zip(range(batch_size), left_child.split(1), right_child.split(1),\n\t\t\t\t\t\t\t\t\t\t\t   node_stacks, target[t].tolist(), embeddings_stacks):\n\t\t\tif len(node_stack) != 0:\n\t\t\t\tnode = node_stack.pop()\n\t\t\telse:\n\t\t\t\tleft_childs.append(None)\n\t\t\t\tcontinue\n\n\t\t\tif i < num_start:\n\t\t\t\tnode_stack.append(TreeNode(r))\n\t\t\t\tnode_stack.append(TreeNode(l, left_flag=True))\n\t\t\t\to.append(TreeEmbedding(node_label[idx].unsqueeze(0), False))\n\t\t\telse:\n\t\t\t\tcurrent_num = current_nums_embeddings[idx, i - num_start].unsqueeze(0)\n\t\t\t\twhile len(o) > 0 and o[-1].terminal:\n\t\t\t\t\tsub_stree = o.pop()\n\t\t\t\t\top = o.pop()\n\t\t\t\t\tcurrent_num = merge(op.embedding, sub_stree.embedding, current_num)\n\t\t\t\to.append(TreeEmbedding(current_num, True))\n\t\t\tif len(o) > 0 and o[-1].terminal:\n\t\t\t\tleft_childs.append(o[-1].embedding)\n\t\t\telse:\n\t\t\t\tleft_childs.append(None)\n\n\t# all_leafs = torch.stack(all_leafs, dim=1)  # B x S x 2\n\tall_node_outputs = torch.stack(all_node_outputs, dim=1)  # B x S x N\n\n\ttarget = target.transpose(0, 1).contiguous()\n\tif USE_CUDA:\n\t\t# all_leafs = all_leafs.cuda()\n\t\tall_node_outputs = all_node_outputs.cuda()\n\t\ttarget = target.cuda()\n\n\t# op_target = target < num_start\n\t# loss_0 = masked_cross_entropy_without_logit(all_leafs, op_target.long(), target_length)\n\tloss = masked_cross_entropy(all_node_outputs, target, target_length)\n\t# loss = loss_0 + loss_1\n\tloss.backward()\n\t# clip the grad\n\t# torch.nn.utils.clip_grad_norm_(encoder.parameters(), 5)\n\t# torch.nn.utils.clip_grad_norm_(predict.parameters(), 5)\n\t# torch.nn.utils.clip_grad_norm_(generate.parameters(), 5)\n\n\t# Update parameters with optimizers\n\tembedding_optimizer.step()\n\tencoder_optimizer.step()\n\tpredict_optimizer.step()\n\tgenerate_optimizer.step()\n\tmerge_optimizer.step()\n\treturn loss.item()  # , loss_0.item(), loss_1.item()\n\n\n# def evaluate_tree(input_batch, input_length, generate_nums, encoder, predict, generate, merge, output_lang, num_pos, batch_graph, beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):\ndef evaluate_tree(config, input_batch, input_length, generate_nums, embedding, encoder, predict, generate, merge, input_lang, output_lang, num_value, num_pos, batch_graph, group_example, beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):\n\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).unsqueeze(1)\n\tbatch_graph = torch.LongTensor(batch_graph)\n\n\tnum_mask = torch.BoolTensor(1, len(num_pos) + len(generate_nums)).fill_(0)\n\n\t# Set to not-training mode to disable dropout\n\tembedding.eval()\n\tencoder.eval()\n\tpredict.eval()\n\tgenerate.eval()\n\tmerge.eval()\n\n\tpadding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n\n\tbatch_size = 1\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\t# seq_mask = seq_mask.cuda()\n\t\tpadding_hidden = padding_hidden.cuda()\n\t\tnum_mask = num_mask.cuda()\n\t\t# batch_graph = batch_graph.cuda()\n\t# Run words through encoder\n\n\tembedded = None\n\torig_idx = None\n\tif config.embedding == 'bert' or config.embedding == 'roberta':\n\t\tcontextual_input = index_batch_to_words([input_batch], [input_length], input_lang)\n\t\tinput_seq1, input_len1, token_ids, index_retrieve = embedding(contextual_input)\n\n\t\ttry:\n\t\t\tnew_group_example = [index_retrieve[0][index1] for index1 in group_example if index1 < len(index_retrieve[0])]\n\t\texcept:\n\t\t\tpdb.set_trace()\n\n\t\tbatch_graph = get_single_example_graph(token_ids.cpu().tolist()[0], input_len1[0], new_group_example, num_value, num_pos)\n\t\tbatch_graph = torch.LongTensor(batch_graph)\n\n\t\tinput_seq1 = input_seq1.transpose(0,1)\n\t\tembedded, input_length, orig_idx = sort_by_len(input_seq1, input_len1, gpu_init_pytorch(config.gpu))\n\t\tinput_length = input_length[0]\n\telse:\n\t\tembedded = embedding(input_var)\n\n\tif USE_CUDA:\n\t\tbatch_graph = batch_graph.cuda()\n\n\tencoder_outputs, problem_output = encoder(embedded, [input_length], orig_idx, batch_graph)\n\t# encoder_outputs, problem_output = encoder(input_var, [input_length], batch_graph)\n\n\tseq_mask = torch.BoolTensor(1, input_length).fill_(0)\n\n\tif USE_CUDA:\n\t\tseq_mask = seq_mask.cuda()\n\n\t# Prepare input and output variables\n\tnode_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]\n\n\tnum_size = len(num_pos)\n\tall_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  encoder.hidden_size)\n\tnum_start = output_lang.num_start\n\t# B x P x N\n\tembeddings_stacks = [[] for _ in range(batch_size)]\n\tleft_childs = [None for _ in range(batch_size)]\n\n\tbeams = [TreeBeam(0.0, node_stacks, embeddings_stacks, left_childs, [])]\n\n\tfor t in range(max_length):\n\t\tcurrent_beams = []\n\t\twhile len(beams) > 0:\n\t\t\tb = beams.pop()\n\t\t\tif len(b.node_stack[0]) == 0:\n\t\t\t\tcurrent_beams.append(b)\n\t\t\t\tcontinue\n\t\t\t# left_childs = torch.stack(b.left_childs)\n\t\t\tleft_childs = b.left_childs\n\n\t\t\tnum_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n\t\t\t\tb.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden,\n\t\t\t\tseq_mask, num_mask)\n\n\t\t\t# leaf = p_leaf[:, 0].unsqueeze(1)\n\t\t\t# repeat_dims = [1] * leaf.dim()\n\t\t\t# repeat_dims[1] = op.size(1)\n\t\t\t# leaf = leaf.repeat(*repeat_dims)\n\t\t\t#\n\t\t\t# non_leaf = p_leaf[:, 1].unsqueeze(1)\n\t\t\t# repeat_dims = [1] * non_leaf.dim()\n\t\t\t# repeat_dims[1] = num_score.size(1)\n\t\t\t# non_leaf = non_leaf.repeat(*repeat_dims)\n\t\t\t#\n\t\t\t# p_leaf = torch.cat((leaf, non_leaf), dim=1)\n\t\t\tout_score = nn.functional.log_softmax(torch.cat((op, num_score), dim=1), dim=1)\n\n\t\t\t# out_score = p_leaf * out_score\n\n\t\t\ttopv, topi = out_score.topk(min(beam_size, out_score.size()[1]))\n\n\t\t\t# is_leaf = int(topi[0])\n\t\t\t# if is_leaf:\n\t\t\t#     topv, topi = op.topk(1)\n\t\t\t#     out_token = int(topi[0])\n\t\t\t# else:\n\t\t\t#     topv, topi = num_score.topk(1)\n\t\t\t#     out_token = int(topi[0]) + num_start\n\n\t\t\tfor tv, ti in zip(topv.split(1, dim=1), topi.split(1, dim=1)):\n\t\t\t\tcurrent_node_stack = copy_list(b.node_stack)\n\t\t\t\tcurrent_left_childs = []\n\t\t\t\tcurrent_embeddings_stacks = copy_list(b.embedding_stack)\n\t\t\t\tcurrent_out = copy.deepcopy(b.out)\n\n\t\t\t\tout_token = int(ti)\n\t\t\t\tcurrent_out.append(out_token)\n\n\t\t\t\tnode = current_node_stack[0].pop()\n\n\t\t\t\tif out_token < num_start:\n\t\t\t\t\tgenerate_input = torch.LongTensor([out_token])\n\t\t\t\t\tif USE_CUDA:\n\t\t\t\t\t\tgenerate_input = generate_input.cuda()\n\t\t\t\t\tleft_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)\n\n\t\t\t\t\tcurrent_node_stack[0].append(TreeNode(right_child))\n\t\t\t\t\tcurrent_node_stack[0].append(TreeNode(left_child, left_flag=True))\n\n\t\t\t\t\tcurrent_embeddings_stacks[0].append(TreeEmbedding(node_label[0].unsqueeze(0), False))\n\t\t\t\telse:\n\t\t\t\t\tcurrent_num = current_nums_embeddings[0, out_token - num_start].unsqueeze(0)\n\n\t\t\t\t\twhile len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:\n\t\t\t\t\t\tsub_stree = current_embeddings_stacks[0].pop()\n\t\t\t\t\t\top = current_embeddings_stacks[0].pop()\n\t\t\t\t\t\tcurrent_num = merge(op.embedding, sub_stree.embedding, current_num)\n\t\t\t\t\tcurrent_embeddings_stacks[0].append(TreeEmbedding(current_num, True))\n\t\t\t\tif len(current_embeddings_stacks[0]) > 0 and current_embeddings_stacks[0][-1].terminal:\n\t\t\t\t\tcurrent_left_childs.append(current_embeddings_stacks[0][-1].embedding)\n\t\t\t\telse:\n\t\t\t\t\tcurrent_left_childs.append(None)\n\t\t\t\tcurrent_beams.append(TreeBeam(b.score+float(tv), current_node_stack, current_embeddings_stacks,\n\t\t\t\t\t\t\t\t\t\t\t  current_left_childs, current_out))\n\t\tbeams = sorted(current_beams, key=lambda x: x.score, reverse=True)\n\t\tbeams = beams[:beam_size]\n\t\tflag = True\n\t\tfor b in beams:\n\t\t\tif len(b.node_stack[0]) != 0:\n\t\t\t\tflag = False\n\t\tif flag:\n\t\t\tbreak\n\n\treturn beams[0].out\n\n\ndef topdown_train_tree(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch,\n\t\t\t\t\t   generate_nums, encoder, predict, generate, encoder_optimizer, predict_optimizer,\n\t\t\t\t\t   generate_optimizer, output_lang, num_pos, english=False):\n\t# sequence mask for attention\n\tseq_mask = []\n\tmax_len = max(input_length)\n\tfor i in input_length:\n\t\tseq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n\tseq_mask = torch.BoolTensor(seq_mask)\n\n\tnum_mask = []\n\tmax_num_size = max(num_size_batch) + len(generate_nums)\n\tfor i in num_size_batch:\n\t\td = i + len(generate_nums)\n\t\tnum_mask.append([0] * d + [1] * (max_num_size - d))\n\tnum_mask = torch.BoolTensor(num_mask)\n\n\tunk = output_lang.word2index[\"UNK\"]\n\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).transpose(0, 1)\n\n\ttarget = torch.LongTensor(target_batch).transpose(0, 1)\n\n\tpadding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n\tbatch_size = len(input_length)\n\n\tencoder.train()\n\tpredict.train()\n\tgenerate.train()\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\tseq_mask = seq_mask.cuda()\n\t\tpadding_hidden = padding_hidden.cuda()\n\t\tnum_mask = num_mask.cuda()\n\n\t# Zero gradients of both optimizers\n\tencoder_optimizer.zero_grad()\n\tpredict_optimizer.zero_grad()\n\tgenerate_optimizer.zero_grad()\n\t# Run words through encoder\n\n\tencoder_outputs, problem_output = encoder(input_var, input_length)\n\t# Prepare input and output variables\n\tnode_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]\n\n\tmax_target_length = max(target_length)\n\n\tall_node_outputs = []\n\t# all_leafs = []\n\n\tcopy_num_len = [len(_) for _ in num_pos]\n\tnum_size = max(copy_num_len)\n\tall_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, num_pos, batch_size, num_size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  encoder.hidden_size)\n\n\tnum_start = output_lang.num_start\n\tleft_childs = [None for _ in range(batch_size)]\n\tfor t in range(max_target_length):\n\t\tnum_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n\t\t\tnode_stacks, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden, seq_mask, num_mask)\n\n\t\t# all_leafs.append(p_leaf)\n\t\toutputs = torch.cat((op, num_score), 1)\n\t\tall_node_outputs.append(outputs)\n\n\t\ttarget_t, generate_input = generate_tree_input(target[t].tolist(), outputs, nums_stack_batch, num_start, unk)\n\t\ttarget[t] = target_t\n\t\tif USE_CUDA:\n\t\t\tgenerate_input = generate_input.cuda()\n\t\tleft_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)\n\t\tfor idx, l, r, node_stack, i in zip(range(batch_size), left_child.split(1), right_child.split(1),\n\t\t\t\t\t\t\t\t\t\t\tnode_stacks, target[t].tolist()):\n\t\t\tif len(node_stack) != 0:\n\t\t\t\tnode = node_stack.pop()\n\t\t\telse:\n\t\t\t\tcontinue\n\n\t\t\tif i < num_start:\n\t\t\t\tnode_stack.append(TreeNode(r))\n\t\t\t\tnode_stack.append(TreeNode(l, left_flag=True))\n\n\t# all_leafs = torch.stack(all_leafs, dim=1)  # B x S x 2\n\tall_node_outputs = torch.stack(all_node_outputs, dim=1)  # B x S x N\n\n\ttarget = target.transpose(0, 1).contiguous()\n\tif USE_CUDA:\n\t\t# all_leafs = all_leafs.cuda()\n\t\tall_node_outputs = all_node_outputs.cuda()\n\t\ttarget = target.cuda()\n\n\t# op_target = target < num_start\n\t# loss_0 = masked_cross_entropy_without_logit(all_leafs, op_target.long(), target_length)\n\tloss = masked_cross_entropy(all_node_outputs, target, target_length)\n\t# loss = loss_0 + loss_1\n\tloss.backward()\n\t# clip the grad\n\t# torch.nn.utils.clip_grad_norm_(encoder.parameters(), 5)\n\t# torch.nn.utils.clip_grad_norm_(predict.parameters(), 5)\n\t# torch.nn.utils.clip_grad_norm_(generate.parameters(), 5)\n\n\t# Update parameters with optimizers\n\tencoder_optimizer.step()\n\tpredict_optimizer.step()\n\tgenerate_optimizer.step()\n\treturn loss.item()  # , loss_0.item(), loss_1.item()\n\n\ndef topdown_evaluate_tree(input_batch, input_length, generate_nums, encoder, predict, generate, output_lang, num_pos,\n\t\t\t\t\t\t  beam_size=5, english=False, max_length=MAX_OUTPUT_LENGTH):\n\n\tseq_mask = torch.BoolTensor(1, input_length).fill_(0)\n\t# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n\tinput_var = torch.LongTensor(input_batch).unsqueeze(1)\n\n\tnum_mask = torch.BoolTensor(1, len(num_pos) + len(generate_nums)).fill_(0)\n\n\t# Set to not-training mode to disable dropout\n\tencoder.eval()\n\tpredict.eval()\n\tgenerate.eval()\n\n\tpadding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n\n\tbatch_size = 1\n\n\tif USE_CUDA:\n\t\tinput_var = input_var.cuda()\n\t\tseq_mask = seq_mask.cuda()\n\t\tpadding_hidden = padding_hidden.cuda()\n\t\tnum_mask = num_mask.cuda()\n\t# Run words through encoder\n\n\tencoder_outputs, problem_output = encoder(input_var, [input_length])\n\n\t# Prepare input and output variables\n\tnode_stacks = [[TreeNode(_)] for _ in problem_output.split(1, dim=0)]\n\n\tnum_size = len(num_pos)\n\tall_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs, [num_pos], batch_size, num_size,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  encoder.hidden_size)\n\tnum_start = output_lang.num_start\n\t# B x P x N\n\tembeddings_stacks = [[] for _ in range(batch_size)]\n\tleft_childs = [None for _ in range(batch_size)]\n\n\tbeams = [TreeBeam(0.0, node_stacks, embeddings_stacks, left_childs, [])]\n\n\tfor t in range(max_length):\n\t\tcurrent_beams = []\n\t\twhile len(beams) > 0:\n\t\t\tb = beams.pop()\n\t\t\tif len(b.node_stack[0]) == 0:\n\t\t\t\tcurrent_beams.append(b)\n\t\t\t\tcontinue\n\t\t\t# left_childs = torch.stack(b.left_childs)\n\n\t\t\tnum_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n\t\t\t\tb.node_stack, left_childs, encoder_outputs, all_nums_encoder_outputs, padding_hidden,\n\t\t\t\tseq_mask, num_mask)\n\n\t\t\t# leaf = p_leaf[:, 0].unsqueeze(1)\n\t\t\t# repeat_dims = [1] * leaf.dim()\n\t\t\t# repeat_dims[1] = op.size(1)\n\t\t\t# leaf = leaf.repeat(*repeat_dims)\n\t\t\t#\n\t\t\t# non_leaf = p_leaf[:, 1].unsqueeze(1)\n\t\t\t# repeat_dims = [1] * non_leaf.dim()\n\t\t\t# repeat_dims[1] = num_score.size(1)\n\t\t\t# non_leaf = non_leaf.repeat(*repeat_dims)\n\t\t\t#\n\t\t\t# p_leaf = torch.cat((leaf, non_leaf), dim=1)\n\t\t\tout_score = nn.functional.log_softmax(torch.cat((op, num_score), dim=1), dim=1)\n\n\t\t\t# out_score = p_leaf * out_score\n\n\t\t\ttopv, topi = out_score.topk(beam_size)\n\n\t\t\t# is_leaf = int(topi[0])\n\t\t\t# if is_leaf:\n\t\t\t#     topv, topi = op.topk(1)\n\t\t\t#     out_token = int(topi[0])\n\t\t\t# else:\n\t\t\t#     topv, topi = num_score.topk(1)\n\t\t\t#     out_token = int(topi[0]) + num_start\n\n\t\t\tfor tv, ti in zip(topv.split(1, dim=1), topi.split(1, dim=1)):\n\t\t\t\tcurrent_node_stack = copy_list(b.node_stack)\n\t\t\t\tcurrent_out = copy.deepcopy(b.out)\n\n\t\t\t\tout_token = int(ti)\n\t\t\t\tcurrent_out.append(out_token)\n\n\t\t\t\tnode = current_node_stack[0].pop()\n\n\t\t\t\tif out_token < num_start:\n\t\t\t\t\tgenerate_input = torch.LongTensor([out_token])\n\t\t\t\t\tif USE_CUDA:\n\t\t\t\t\t\tgenerate_input = generate_input.cuda()\n\t\t\t\t\tleft_child, right_child, node_label = generate(current_embeddings, generate_input, current_context)\n\n\t\t\t\t\tcurrent_node_stack[0].append(TreeNode(right_child))\n\t\t\t\t\tcurrent_node_stack[0].append(TreeNode(left_child, left_flag=True))\n\n\t\t\t\tcurrent_beams.append(TreeBeam(b.score+float(tv), current_node_stack, embeddings_stacks, left_childs,\n\t\t\t\t\t\t\t\t\t\t\t  current_out))\n\t\tbeams = sorted(current_beams, key=lambda x: x.score, reverse=True)\n\t\tbeams = beams[:beam_size]\n\t\tflag = True\n\t\tfor b in beams:\n\t\t\tif len(b.node_stack[0]) != 0:\n\t\t\t\tflag = False\n\t\tif flag:\n\t\t\tbreak\n\n\treturn beams[0].out\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T10:53:08.278095Z","iopub.execute_input":"2024-11-16T10:53:08.278520Z","iopub.status.idle":"2024-11-16T10:53:08.824619Z","shell.execute_reply.started":"2024-11-16T10:53:08.278462Z","shell.execute_reply":"2024-11-16T10:53:08.823451Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## main.py","metadata":{}},{"cell_type":"code","source":"log_folder = 'logs'\nmodel_folder = 'models'\noutputs_folder = 'outputs'\nresult_folder = './out/'\ndata_path = '/kaggle/input/svampdata/data/'\nboard_path = './runs/'\n\ndef read_json(path):\n\twith open(path,'r') as f:\n\t\tfile = json.load(f)\n\treturn file\n\nUSE_CUDA = True\n\ndef get_new_fold(data,pairs,group):\n\tnew_fold = []\n\tfor item,pair,g in zip(data, pairs, group):\n\t\tpair = list(pair)\n\t\tpair.append(g['group_num'])\n\t\tpair = tuple(pair)\n\t\tnew_fold.append(pair)\n\treturn new_fold\n\ndef change_num(num):\n\tnew_num = []\n\tfor item in num:\n\t\tif '/' in item:\n\t\t\tnew_str = item.split(')')[0]\n\t\t\tnew_str = new_str.split('(')[1]\n\t\t\ta = float(new_str.split('/')[0])\n\t\t\tb = float(new_str.split('/')[1])\n\t\t\tvalue = a/b\n\t\t\tnew_num.append(value)\n\t\telif '%' in item:\n\t\t\tvalue = float(item[0:-1])/100\n\t\t\tnew_num.append(value)\n\t\telse:\n\t\t\tnew_num.append(float(item))\n\treturn new_num\n\nkaggle_args = {\n    'debug': False,\n    'mode': 'train',\n    'gpu': 0,\n    'dropout': 0.1,\n    'heads': 4,\n    'encoder_layers': 1,\n    'decoder_layers': 1,\n    'd_model': 768,\n    'd_ff': 256,\n    'lr': 0.0001,\n    'emb_lr': 1e-5,\n    'batch_size': 32,\n    'epochs': 10,\n    'embedding': 'roberta',\n    'emb_name': 'roberta-base',\n    'mawps_vocab': True,\n    'dataset': 'mawps-asdiv-a_svamp',\n    'run_name': 'mawps_try1',\n    'logging': 0\n}\n\n\n\n\nconfig =  parse_arguments(kaggle_args)\n\nmode = config.mode\n\nif mode == 'train':\n    is_train = True\nelse:\n    is_train = False\n\n''' Set seed for reproducibility'''\nnp.random.seed(config.seed)\ntorch.manual_seed(config.seed)\nrandom.seed(config.seed)\n\n'''GPU initialization'''\ndevice = gpu_init_pytorch(config.gpu)\n\nif config.full_cv:\n    global data_path\n    data_name = config.dataset\n    data_path = data_path + data_name + '/'\n    config.val_result_path = os.path.join(result_folder, 'CV_results_{}.json'.format(data_name))\n    fold_acc_score = 0.0\n    folds_scores = []\n    best_acc = []\n    for z in range(5):\n        run_name = config.run_name + '_fold' + str(z)\n        config.dataset = 'fold' + str(z)\n        config.log_path = os.path.join(log_folder, run_name)\n        config.model_path = os.path.join(model_folder, run_name)\n        config.board_path = os.path.join(board_path, run_name)\n        config.outputs_path = os.path.join(outputs_folder, run_name)\n\n        vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n        vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n        config_file = os.path.join(config.model_path, 'config.p')\n        log_file = os.path.join(config.log_path, 'log.txt')\n\n        if config.results:\n            config.result_path = os.path.join(result_folder, 'val_results_{}.json'.format(config.dataset))\n\n        create_save_directories(config.log_path)\n        create_save_directories(config.model_path)\n        create_save_directories(config.outputs_path)\n\n        logger = get_logger(run_name, log_file, logging.DEBUG)\n\n        logger.info('Experiment Name: {}'.format(config.run_name))\n        logger.debug('Created Relevant Directories')\n\n        logger.info('Loading Data...')\n\n        train_ls, dev_ls = load_raw_data(data_path, config.dataset, is_train)\n\n        pairs_trained, pairs_tested, generate_nums, copy_nums = transfer_num(train_ls, dev_ls, config.challenge_disp)\n\n        logger.debug('Data Loaded...')\n        logger.debug('Number of Training Examples: {}'.format(len(pairs_trained)))\n        logger.debug('Number of Testing Examples: {}'.format(len(pairs_tested)))\n        logger.debug('Extra Numbers: {}'.format(generate_nums))\n        logger.debug('Maximum Number of Numbers: {}'.format(copy_nums))\n\n        logger.info('Creating Vocab...')\n        input_lang = None\n        output_lang = None\n\n        input_lang, output_lang, train_pairs, test_pairs = prepare_data(config, logger, pairs_trained, pairs_tested, config.trim_threshold, generate_nums, copy_nums, input_lang, output_lang, tree=True)\n\n        checkpoint = get_latest_checkpoint(config.model_path, logger)\n\n        with open(vocab1_path, 'wb') as f:\n            pickle.dump(input_lang, f, protocol=pickle.HIGHEST_PROTOCOL)\n        with open(vocab2_path, 'wb') as f:\n            pickle.dump(output_lang, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Vocab saved at {}'.format(vocab1_path))\n\n        generate_num_ids = []\n        for num in generate_nums:\n            generate_num_ids.append(output_lang.word2index[num])\n\n        config.len_generate_nums = len(generate_nums)\n        config.copy_nums = copy_nums\n\n        with open(config_file, 'wb') as f:\n            pickle.dump(vars(config), f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Config File Saved')\n\n        logger.info('Initializing Models...')\n\n        # Initialize models\n        embedding = None\n        if config.embedding == 'bert':\n            embedding = BertEncoder(config.emb_name, device, config.freeze_emb)\n        elif config.embedding == 'roberta':\n            embedding = RobertaEncoder(config.emb_name, device, config.freeze_emb)\n        else:\n            embedding = Embedding(config, input_lang, input_size=input_lang.n_words, embedding_size=config.embedding_size, dropout=config.dropout)\n\n        encoder = EncoderSeq(cell_type=config.cell_type, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        predict = Prediction(hidden_size=config.hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums), input_size=len(generate_nums), dropout=config.dropout)\n        generate = GenerateNode(hidden_size=config.hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums), embedding_size=config.embedding_size, dropout=config.dropout)\n        merge = Merge(hidden_size=config.hidden_size, embedding_size=config.embedding_size, dropout=config.dropout)\n        # the embedding layer is  only for generated number embeddings, operators, and paddings\n\n        logger.debug('Models Initialized')\n        logger.info('Initializing Optimizers...')\n\n        embedding_optimizer = torch.optim.Adam(embedding.parameters(), lr=config.emb_lr, weight_decay=config.weight_decay)\n        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        predict_optimizer = torch.optim.Adam(predict.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        generate_optimizer = torch.optim.Adam(generate.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        merge_optimizer = torch.optim.Adam(merge.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n\n        logger.debug('Optimizers Initialized')\n        logger.info('Initializing Schedulers...')\n\n        embedding_scheduler = torch.optim.lr_scheduler.StepLR(embedding_optimizer, step_size=20, gamma=0.5)\n        encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n        predict_scheduler = torch.optim.lr_scheduler.StepLR(predict_optimizer, step_size=20, gamma=0.5)\n        generate_scheduler = torch.optim.lr_scheduler.StepLR(generate_optimizer, step_size=20, gamma=0.5)\n        merge_scheduler = torch.optim.lr_scheduler.StepLR(merge_optimizer, step_size=20, gamma=0.5)\n\n        logger.debug('Schedulers Initialized')\n\n        logger.info('Loading Models on GPU {}...'.format(config.gpu))\n\n        # Move models to GPU\n        if USE_CUDA:\n            embedding.to(device)\n            encoder.to(device)\n            predict.to(device)\n            generate.to(device)\n            merge.to(device)\n\n        logger.debug('Models loaded on GPU {}'.format(config.gpu))\n\n        max_value_corr = 0\n        len_total_eval = 0\n        max_val_acc = 0.0\n        max_train_acc = 0.0\n        eq_acc = 0.0\n        best_epoch = -1\n        min_train_loss = float('inf')\n\n        logger.info('Starting Training Procedure')\n\n        for epoch in range(config.epochs):\n            loss_total = 0\n            input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches, num_value_batches, graph_batches, group_batches = prepare_train_batch(train_pairs, config.batch_size)\n\n            od = OrderedDict()\n            od['Epoch'] = epoch + 1\n            print_log(logger, od)\n\n            start = time.time()\n            for idx in range(len(input_lengths)):\n                loss = train_tree(\n                    config, input_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n                    num_stack_batches[idx], num_size_batches[idx], num_value_batches[idx], group_batches[idx], generate_num_ids, embedding, encoder, predict, generate, merge,\n                    embedding_optimizer, encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, input_lang, output_lang, \n                    num_pos_batches[idx], graph_batches[idx])\n                loss_total += loss\n                print(\"Completed {} / {}...\".format(idx, len(input_lengths)), end = '\\r', flush = True)\n\n            embedding_scheduler.step()\n            encoder_scheduler.step()\n            predict_scheduler.step()\n            generate_scheduler.step()\n            merge_scheduler.step()\n\n            logger.debug('Training for epoch {} completed...\\nTime Taken: {}'.format(epoch, time_since(time.time() - start)))\n\n            if loss_total / len(input_lengths) < min_train_loss:\n                min_train_loss = loss_total / len(input_lengths)\n\n            train_value_ac = 0\n            train_equation_ac = 0\n            train_eval_total = 1\n            if config.show_train_acc:\n                train_eval_total = 0\n                logger.info('Computing Train Accuracy')\n                start = time.time()\n                with torch.no_grad():\n                    for train_batch in train_pairs:\n                        batch_graph = get_single_example_graph(train_batch[0], train_batch[1], train_batch[7], train_batch[4], train_batch[5])\n                        train_res = evaluate_tree(config, train_batch[0], train_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                                 merge, input_lang, output_lang, train_batch[4], train_batch[5], batch_graph, test_batch[7], beam_size=config.beam_size)\n                        train_val_ac, train_equ_ac, _, _ = compute_prefix_tree_result(train_res, train_batch[2], output_lang, train_batch[4], train_batch[6])\n\n                        if train_val_ac:\n                            train_value_ac += 1\n                        if train_equ_ac:\n                            train_equation_ac += 1\n                        train_eval_total += 1\n\n                logger.debug('Train Accuracy Computed...\\nTime Taken: {}'.format(time_since(time.time() - start)))\n\n            logger.info('Starting Validation')\n\n            value_ac = 0\n            equation_ac = 0\n            eval_total = 0\n            start = time.time()\n\n            with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                f_out.write('---------------------------------------\\n')\n                f_out.write('Epoch: ' + str(epoch) + '\\n')\n                f_out.write('---------------------------------------\\n')\n                f_out.close()\n\n            ex_num = 0\n            for test_batch in test_pairs:\n                batch_graph = get_single_example_graph(test_batch[0], test_batch[1], test_batch[7], test_batch[4], test_batch[5])\n                test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                         merge, input_lang, output_lang, test_batch[4], test_batch[5], batch_graph, test_batch[7], beam_size=config.beam_size)\n                val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n                cur_result = 0\n                if val_ac:\n                    value_ac += 1\n                    cur_result = 1\n                if equ_ac:\n                    equation_ac += 1\n                eval_total += 1\n\n                with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                    f_out.write('Example: ' + str(ex_num) + '\\n')\n                    f_out.write('Source: ' + stack_to_string(sentence_from_indexes(input_lang, test_batch[0])) + '\\n')\n                    f_out.write('Target: ' + stack_to_string(sentence_from_indexes(output_lang, test_batch[2])) + '\\n')\n                    f_out.write('Generated: ' + stack_to_string(sentence_from_indexes(output_lang, test_res)) + '\\n')\n                    if config.nums_disp:\n                        src_nums = len(test_batch[4])\n                        tgt_nums = 0\n                        pred_nums = 0\n                        for k_tgt in sentence_from_indexes(output_lang, test_batch[2]):\n                            if k_tgt not in ['+', '-', '*', '/']:\n                                tgt_nums += 1\n                        for k_pred in sentence_from_indexes(output_lang, test_res):\n                            if k_pred not in ['+', '-', '*', '/']:\n                                pred_nums += 1\n                        f_out.write('Numbers in question: ' + str(src_nums) + '\\n')\n                        f_out.write('Numbers in Target Equation: ' + str(tgt_nums) + '\\n')\n                        f_out.write('Numbers in Predicted Equation: ' + str(pred_nums) + '\\n')\n                    f_out.write('Result: ' + str(cur_result) + '\\n' + '\\n')\n                    f_out.close()\n\n                ex_num+=1\n\n            if float(train_value_ac) / train_eval_total > max_train_acc:\n                max_train_acc = float(train_value_ac) / train_eval_total\n\n            if float(value_ac) / eval_total > max_val_acc:\n                max_value_corr = value_ac\n                len_total_eval = eval_total\n                max_val_acc = float(value_ac) / eval_total\n                eq_acc = float(equation_ac) / eval_total\n                best_epoch = epoch+1\n\n                state = {\n                        'epoch' : epoch,\n                        'best_epoch': best_epoch-1,\n                        'embedding_state_dict': embedding.state_dict(),\n                        'encoder_state_dict': encoder.state_dict(),\n                        'predict_state_dict': predict.state_dict(),\n                        'generate_state_dict': generate.state_dict(),\n                        'merge_state_dict': merge.state_dict(),\n                        'embedding_optimizer_state_dict': embedding_optimizer.state_dict(),\n                        'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n                        'predict_optimizer_state_dict': predict_optimizer.state_dict(),\n                        'generate_optimizer_state_dict': generate_optimizer.state_dict(),\n                        'merge_optimizer_state_dict': merge_optimizer.state_dict(),\n                        'embedding_scheduler_state_dict': embedding_scheduler.state_dict(),\n                        'encoder_scheduler_state_dict': encoder_scheduler.state_dict(),\n                        'predict_scheduler_state_dict': predict_scheduler.state_dict(),\n                        'generate_scheduler_state_dict': generate_scheduler.state_dict(),\n                        'merge_scheduler_state_dict': merge_scheduler.state_dict(),\n                        'voc1': input_lang,\n                        'voc2': output_lang,\n                        'train_loss_epoch' : loss_total / len(input_lengths),\n                        'min_train_loss' : min_train_loss,\n                        'val_acc_epoch' : float(value_ac) / eval_total,\n                        'max_val_acc' : max_val_acc,\n                        'equation_acc' : eq_acc,\n                        'max_train_acc' : max_train_acc,\n                        'generate_nums' : generate_nums\n                    }\n\n                if config.save_model:\n                    save_checkpoint(state, epoch, logger, config.model_path, config.ckpt)\n\n            od = OrderedDict()\n            od['Epoch'] = epoch + 1\n            od['best_epoch'] = best_epoch\n            od['train_loss_epoch'] = loss_total / len(input_lengths)\n            od['min_train_loss'] = min_train_loss\n            od['train_acc_epoch'] = float(train_value_ac) / train_eval_total\n            od['max_train_acc'] = max_train_acc\n            od['val_acc_epoch'] = float(value_ac) / eval_total\n            od['equation_acc_epoch'] = float(equation_ac) / eval_total\n            od['max_val_acc'] = max_val_acc\n            od['equation_acc'] = eq_acc\n            print_log(logger, od)\n\n            logger.debug('Validation Completed...\\nTime Taken: {}'.format(time_since(time.time() - start)))\n\n        if config.results:\n            store_results(config, max_train_acc, max_val_acc, eq_acc, min_train_loss, best_epoch)\n            logger.info('Scores saved at {}'.format(config.result_path))\n\n        best_acc.append((max_value_corr, len_total_eval))\n\n    total_value_corr = 0\n    total_len = 0\n    for w in range(len(best_acc)):\n        folds_scores.append(float(best_acc[w][0])/best_acc[w][1])\n        total_value_corr += best_acc[w][0]\n        total_len += best_acc[w][1]\n    fold_acc_score = float(total_value_corr)/total_len\n\n    store_val_results(config, fold_acc_score, folds_scores)\n    logger.info('Final Val score: {}'.format(fold_acc_score))\n\n\nelse:\n    run_name = config.run_name\n    config.log_path = os.path.join(log_folder, run_name)\n    config.model_path = os.path.join(model_folder, run_name)\n    config.board_path = os.path.join(board_path, run_name)\n    config.outputs_path = os.path.join(outputs_folder, run_name)\n\n    vocab1_path = os.path.join(config.model_path, 'vocab1.p')\n    vocab2_path = os.path.join(config.model_path, 'vocab2.p')\n    config_file = os.path.join(config.model_path, 'config.p')\n    log_file = os.path.join(config.log_path, 'log.txt')\n\n    if config.results:\n        config.result_path = os.path.join(result_folder, 'val_results_{}.json'.format(config.dataset))\n\n    if is_train:\n        create_save_directories(config.log_path)\n        create_save_directories(config.model_path)\n        create_save_directories(config.outputs_path)\n    else:\n        create_save_directories(config.log_path)\n        create_save_directories(config.result_path)\n\n    logger = get_logger(run_name, log_file, logging.DEBUG)\n\n    logger.info('Experiment Name: {}'.format(config.run_name))\n    logger.debug('Created Relevant Directories')\n\n    logger.info('Loading Data...')\n\n    train_ls, dev_ls = load_raw_data(data_path, config.dataset, is_train)\n\n    pairs_trained, pairs_tested, generate_nums, copy_nums = transfer_num(train_ls, dev_ls, config.challenge_disp)\n\n    logger.debug('Data Loaded...')\n    if is_train:\n        logger.debug('Number of Training Examples: {}'.format(len(pairs_trained)))\n    logger.debug('Number of Testing Examples: {}'.format(len(pairs_tested)))\n    logger.debug('Extra Numbers: {}'.format(generate_nums))\n    logger.debug('Maximum Number of Numbers: {}'.format(copy_nums))\n\n    if is_train:\n        logger.info('Creating Vocab...')\n        input_lang = None\n        output_lang = None\n    else:\n        logger.info('Loading Vocab File...')\n\n        with open(vocab1_path, 'rb') as f:\n            input_lang = pickle.load(f)\n        with open(vocab2_path, 'rb') as f:\n            output_lang = pickle.load(f)\n\n        logger.info('Vocab Files loaded from {}\\nNumber of Words: {}'.format(vocab1_path, input_lang.n_words))\n\n    input_lang, output_lang, train_pairs, test_pairs = prepare_data(config, logger, pairs_trained, pairs_tested, config.trim_threshold, generate_nums, copy_nums, input_lang, output_lang, tree=True)\n\n    checkpoint = get_latest_checkpoint(config.model_path, logger)\n\n    if is_train:\n        with open(vocab1_path, 'wb') as f:\n            pickle.dump(input_lang, f, protocol=pickle.HIGHEST_PROTOCOL)\n        with open(vocab2_path, 'wb') as f:\n            pickle.dump(output_lang, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Vocab saved at {}'.format(vocab1_path))\n\n        generate_num_ids = []\n        for num in generate_nums:\n            generate_num_ids.append(output_lang.word2index[num])\n\n        config.len_generate_nums = len(generate_nums)\n        config.copy_nums = copy_nums\n\n        with open(config_file, 'wb') as f:\n            pickle.dump(vars(config), f, protocol=pickle.HIGHEST_PROTOCOL)\n\n        logger.debug('Config File Saved')\n\n        logger.info('Initializing Models...')\n\n        # Initialize models\n        embedding = None\n        if config.embedding == 'bert':\n            embedding = BertEncoder(config.emb_name, device, config.freeze_emb)\n        elif config.embedding == 'roberta':\n            embedding = RobertaEncoder(config.emb_name, device, config.freeze_emb)\n        else:\n            embedding = Embedding(config, input_lang, input_size=input_lang.n_words, embedding_size=config.embedding_size, dropout=config.dropout)\n\n        encoder = EncoderSeq(cell_type=config.cell_type, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        predict = Prediction(hidden_size=config.hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums), input_size=len(generate_nums), dropout=config.dropout)\n        generate = GenerateNode(hidden_size=config.hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums), embedding_size=config.embedding_size, dropout=config.dropout)\n        merge = Merge(hidden_size=config.hidden_size, embedding_size=config.embedding_size, dropout=config.dropout)\n        # the embedding layer is  only for generated number embeddings, operators, and paddings\n\n        logger.debug('Models Initialized')\n        logger.info('Initializing Optimizers...')\n\n        embedding_optimizer = torch.optim.Adam(embedding.parameters(), lr=config.emb_lr, weight_decay=config.weight_decay)\n        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        predict_optimizer = torch.optim.Adam(predict.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        generate_optimizer = torch.optim.Adam(generate.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n        merge_optimizer = torch.optim.Adam(merge.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n\n        logger.debug('Optimizers Initialized')\n        logger.info('Initializing Schedulers...')\n\n        embedding_scheduler = torch.optim.lr_scheduler.StepLR(embedding_optimizer, step_size=20, gamma=0.5)\n        encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n        predict_scheduler = torch.optim.lr_scheduler.StepLR(predict_optimizer, step_size=20, gamma=0.5)\n        generate_scheduler = torch.optim.lr_scheduler.StepLR(generate_optimizer, step_size=20, gamma=0.5)\n        merge_scheduler = torch.optim.lr_scheduler.StepLR(merge_optimizer, step_size=20, gamma=0.5)\n\n        logger.debug('Schedulers Initialized')\n\n        logger.info('Loading Models on GPU {}...'.format(config.gpu))\n\n        # Move models to GPU\n        if USE_CUDA:\n            embedding.to(device)\n            encoder.to(device)\n            predict.to(device)\n            generate.to(device)\n            merge.to(device)\n\n        logger.debug('Models loaded on GPU {}'.format(config.gpu))\n\n        max_val_acc = 0.0\n        max_train_acc = 0.0\n        eq_acc = 0.0\n        best_epoch = -1\n        min_train_loss = float('inf')\n\n        logger.info('Starting Training Procedure')\n\n        for epoch in range(config.epochs):\n            loss_total = 0\n            input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches, num_value_batches, graph_batches, group_batches = prepare_train_batch(train_pairs, config.batch_size)\n\n            od = OrderedDict()\n            od['Epoch'] = epoch + 1\n            print_log(logger, od)\n\n            start = time.time()\n            for idx in range(len(input_lengths)):\n                loss = train_tree(\n                    config, input_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n                    num_stack_batches[idx], num_size_batches[idx], num_value_batches[idx], group_batches[idx], generate_num_ids, embedding, encoder, predict, generate, merge,\n                    embedding_optimizer, encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, input_lang, output_lang, \n                    num_pos_batches[idx], graph_batches[idx])\n                loss_total += loss\n                print(\"Completed {} / {}...\".format(idx, len(input_lengths)), end = '\\r', flush = True)\n\n            embedding_scheduler.step()\n            encoder_scheduler.step()\n            predict_scheduler.step()\n            generate_scheduler.step()\n            merge_scheduler.step()\n\n            logger.debug('Training for epoch {} completed...\\nTime Taken: {}'.format(epoch, time_since(time.time() - start)))\n\n            if loss_total / len(input_lengths) < min_train_loss:\n                min_train_loss = loss_total / len(input_lengths)\n\n            train_value_ac = 0\n            train_equation_ac = 0\n            train_eval_total = 1\n            if config.show_train_acc:\n                train_eval_total = 0\n                logger.info('Computing Train Accuracy')\n                start = time.time()\n                with torch.no_grad():\n                    for train_batch in train_pairs:\n                        batch_graph = get_single_example_graph(train_batch[0], train_batch[1], train_batch[7], train_batch[4], train_batch[5])\n                        train_res = evaluate_tree(config, train_batch[0], train_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                                 merge, input_lang, output_lang, train_batch[4], train_batch[5], batch_graph, train_batch[7], beam_size=config.beam_size)\n                        train_val_ac, train_equ_ac, _, _ = compute_prefix_tree_result(train_res, train_batch[2], output_lang, train_batch[4], train_batch[6])\n\n                        if train_val_ac:\n                            train_value_ac += 1\n                        if train_equ_ac:\n                            train_equation_ac += 1\n                        train_eval_total += 1\n\n                logger.debug('Train Accuracy Computed...\\nTime Taken: {}'.format(time_since(time.time() - start)))\n\n            logger.info('Starting Validation')\n\n            value_ac = 0\n            equation_ac = 0\n            eval_total = 0\n            start = time.time()\n\n            with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                f_out.write('---------------------------------------\\n')\n                f_out.write('Epoch: ' + str(epoch) + '\\n')\n                f_out.write('---------------------------------------\\n')\n                f_out.close()\n\n            ex_num = 0\n            for test_batch in test_pairs:\n                batch_graph = get_single_example_graph(test_batch[0], test_batch[1], test_batch[7], test_batch[4], test_batch[5])\n                test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                         merge, input_lang, output_lang, test_batch[4], test_batch[5], batch_graph, test_batch[7], beam_size=config.beam_size)\n                val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n                cur_result = 0\n                if val_ac:\n                    value_ac += 1\n                    cur_result = 1\n                if equ_ac:\n                    equation_ac += 1\n                eval_total += 1\n\n                with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                    f_out.write('Example: ' + str(ex_num) + '\\n')\n                    f_out.write('Source: ' + stack_to_string(sentence_from_indexes(input_lang, test_batch[0])) + '\\n')\n                    f_out.write('Target: ' + stack_to_string(sentence_from_indexes(output_lang, test_batch[2])) + '\\n')\n                    f_out.write('Generated: ' + stack_to_string(sentence_from_indexes(output_lang, test_res)) + '\\n')\n                    if config.challenge_disp:\n                        f_out.write('Type: ' + test_batch[8] + '\\n')\n                        f_out.write('Variation Type: ' + test_batch[9] + '\\n')\n                        f_out.write('Annotator: ' + test_batch[10] + '\\n')\n                        f_out.write('Alternate: ' + str(test_batch[11]) + '\\n')\n                    if config.nums_disp:\n                        src_nums = len(test_batch[4])\n                        tgt_nums = 0\n                        pred_nums = 0\n                        for k_tgt in sentence_from_indexes(output_lang, test_batch[2]):\n                            if k_tgt not in ['+', '-', '*', '/']:\n                                tgt_nums += 1\n                        for k_pred in sentence_from_indexes(output_lang, test_res):\n                            if k_pred not in ['+', '-', '*', '/']:\n                                pred_nums += 1\n                        f_out.write('Numbers in question: ' + str(src_nums) + '\\n')\n                        f_out.write('Numbers in Target Equation: ' + str(tgt_nums) + '\\n')\n                        f_out.write('Numbers in Predicted Equation: ' + str(pred_nums) + '\\n')\n                    f_out.write('Result: ' + str(cur_result) + '\\n' + '\\n')\n                    f_out.close()\n\n                ex_num+=1\n\n            if float(train_value_ac) / train_eval_total > max_train_acc:\n                    max_train_acc = float(train_value_ac) / train_eval_total\n\n            if float(value_ac) / eval_total > max_val_acc:\n                max_val_acc = float(value_ac) / eval_total\n                eq_acc = float(equation_ac) / eval_total\n                best_epoch = epoch+1\n\n                state = {\n                        'epoch' : epoch,\n                        'best_epoch': best_epoch-1,\n                        'embedding_state_dict': embedding.state_dict(),\n                        'encoder_state_dict': encoder.state_dict(),\n                        'predict_state_dict': predict.state_dict(),\n                        'generate_state_dict': generate.state_dict(),\n                        'merge_state_dict': merge.state_dict(),\n                        'embedding_optimizer_state_dict': embedding_optimizer.state_dict(),\n                        'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n                        'predict_optimizer_state_dict': predict_optimizer.state_dict(),\n                        'generate_optimizer_state_dict': generate_optimizer.state_dict(),\n                        'merge_optimizer_state_dict': merge_optimizer.state_dict(),\n                        'embedding_scheduler_state_dict': embedding_scheduler.state_dict(),\n                        'encoder_scheduler_state_dict': encoder_scheduler.state_dict(),\n                        'predict_scheduler_state_dict': predict_scheduler.state_dict(),\n                        'generate_scheduler_state_dict': generate_scheduler.state_dict(),\n                        'merge_scheduler_state_dict': merge_scheduler.state_dict(),\n                        'voc1': input_lang,\n                        'voc2': output_lang,\n                        'train_loss_epoch' : loss_total / len(input_lengths),\n                        'min_train_loss' : min_train_loss,\n                        'val_acc_epoch' : float(value_ac) / eval_total,\n                        'max_val_acc' : max_val_acc,\n                        'equation_acc' : eq_acc,\n                        'max_train_acc' : max_train_acc,\n                        'generate_nums' : generate_nums\n                    }\n\n                if config.save_model:\n                    save_checkpoint(state, epoch, logger, config.model_path, config.ckpt)\n\n            od = OrderedDict()\n            od['Epoch'] = epoch + 1\n            od['best_epoch'] = best_epoch\n            od['train_loss_epoch'] = loss_total / len(input_lengths)\n            od['min_train_loss'] = min_train_loss\n            od['train_acc_epoch'] = float(train_value_ac) / train_eval_total\n            od['max_train_acc'] = max_train_acc\n            od['val_acc_epoch'] = float(value_ac) / eval_total\n            od['equation_acc_epoch'] = float(equation_ac) / eval_total\n            od['max_val_acc'] = max_val_acc\n            od['equation_acc'] = eq_acc\n            print_log(logger, od)\n\n            logger.debug('Validation Completed...\\nTime Taken: {}'.format(time_since(time.time() - start)))\n\n        if config.results:\n            store_results(config, max_train_acc, max_val_acc, eq_acc, min_train_loss, best_epoch)\n            logger.info('Scores saved at {}'.format(config.result_path))\n\n    else:\n        gpu = config.gpu\n        mode = config.mode\n        dataset = config.dataset\n        batch_size = config.batch_size\n        old_run_name = config.run_name\n        with open(config_file, 'rb') as f:\n            config = AttrDict(pickle.load(f))\n            config.gpu = gpu\n            config.mode = mode\n            config.dataset = dataset\n            config.batch_size = batch_size\n\n        logger.info('Initializing Models...')\n\n        # Initialize models\n        embedding = None\n        if config.embedding == 'bert':\n            embedding = BertEncoder(config.emb_name, device, config.freeze_emb)\n        elif config.embedding == 'roberta':\n            embedding = RobertaEncoder(config.emb_name, device, config.freeze_emb)\n        else:\n            embedding = Embedding(config, input_lang, input_size=input_lang.n_words, embedding_size=config.embedding_size, dropout=config.dropout)\n\n        # encoder = EncoderSeq(input_size=input_lang.n_words, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        encoder = EncoderSeq(cell_type=config.cell_type, embedding_size=config.embedding_size, hidden_size=config.hidden_size, n_layers=config.depth, dropout=config.dropout)\n        predict = Prediction(hidden_size=config.hidden_size, op_nums=output_lang.n_words - config.copy_nums - 1 - config.len_generate_nums, input_size=config.len_generate_nums, dropout=config.dropout)\n        generate = GenerateNode(hidden_size=config.hidden_size, op_nums=output_lang.n_words - config.copy_nums - 1 - config.len_generate_nums, embedding_size=config.embedding_size, dropout=config.dropout)\n        merge = Merge(hidden_size=config.hidden_size, embedding_size=config.embedding_size, dropout=config.dropout)\n        # the embedding layer is only for generated number embeddings, operators, and paddings\n\n        logger.debug('Models Initialized')\n\n        epoch_offset, min_train_loss, max_train_acc, max_val_acc, equation_acc, best_epoch, generate_nums = load_checkpoint(config, embedding, encoder, predict, generate, merge, config.mode, checkpoint, logger, device)\n\n        logger.info('Prediction from')\n        od = OrderedDict()\n        od['epoch'] = epoch_offset\n        od['min_train_loss'] = min_train_loss\n        od['max_train_acc'] = max_train_acc\n        od['max_val_acc'] = max_val_acc\n        od['equation_acc'] = equation_acc\n        od['best_epoch'] = best_epoch\n        print_log(logger, od)\n\n        generate_num_ids = []\n        for num in generate_nums:\n            generate_num_ids.append(output_lang.word2index[num])\n\n        value_ac = 0\n        equation_ac = 0\n        eval_total = 0\n        start = time.time()\n\n        with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n            f_out.write('---------------------------------------\\n')\n            f_out.write('Test Name: ' + old_run_name + '\\n')\n            f_out.write('---------------------------------------\\n')\n            f_out.close()\n\n        test_res_ques, test_res_act, test_res_gen, test_res_scores = [], [], [], []\n\n        ex_num = 0\n        for test_batch in test_pairs:\n            batch_graph = get_single_example_graph(test_batch[0], test_batch[1], test_batch[7], test_batch[4], test_batch[5])\n            test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                                     merge, input_lang, output_lang, test_batch[4], test_batch[5], batch_graph, test_batch[7], beam_size=config.beam_size)\n            val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n            cur_result = 0\n            if val_ac:\n                value_ac += 1\n                cur_result = 1\n            if equ_ac:\n                equation_ac += 1\n            eval_total += 1\n\n            with open(config.outputs_path + '/outputs.txt', 'a') as f_out:\n                f_out.write('Example: ' + str(ex_num) + '\\n')\n                f_out.write('Source: ' + stack_to_string(sentence_from_indexes(input_lang, test_batch[0])) + '\\n')\n                f_out.write('Target: ' + stack_to_string(sentence_from_indexes(output_lang, test_batch[2])) + '\\n')\n                f_out.write('Generated: ' + stack_to_string(sentence_from_indexes(output_lang, test_res)) + '\\n')\n                if config.nums_disp:\n                    src_nums = len(test_batch[4])\n                    tgt_nums = 0\n                    pred_nums = 0\n                    for k_tgt in sentence_from_indexes(output_lang, test_batch[2]):\n                        if k_tgt not in ['+', '-', '*', '/']:\n                            tgt_nums += 1\n                    for k_pred in sentence_from_indexes(output_lang, test_res):\n                        if k_pred not in ['+', '-', '*', '/']:\n                            pred_nums += 1\n                    f_out.write('Numbers in question: ' + str(src_nums) + '\\n')\n                    f_out.write('Numbers in Target Equation: ' + str(tgt_nums) + '\\n')\n                    f_out.write('Numbers in Predicted Equation: ' + str(pred_nums) + '\\n')\n                f_out.write('Result: ' + str(cur_result) + '\\n' + '\\n')\n                f_out.close()\n\n            ex_num+=1\n\n        results_df = pd.DataFrame([test_res_ques, test_res_act, test_res_gen, test_res_scores]).transpose()\n        results_df.columns = ['Question', 'Actual Equation', 'Generated Equation', 'Score']\n        csv_file_path = os.path.join(config.outputs_path, config.dataset+'.csv')\n        results_df.to_csv(csv_file_path, index = False)\n        logger.info('Accuracy: {}'.format(sum(test_res_scores)/len(test_res_scores)))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T10:53:08.939500Z","iopub.execute_input":"2024-11-16T10:53:08.939950Z","iopub.status.idle":"2024-11-16T11:50:30.590907Z","shell.execute_reply.started":"2024-11-16T10:53:08.939907Z","shell.execute_reply":"2024-11-16T11:50:30.589733Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-11-16 10:53:09,111 | INFO | 3377399906.py: 416 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-16 10:53:09,111 | INFO | 3377399906.py: 416 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-16 10:53:09,111 | INFO | 3377399906.py: 416 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-16 10:53:09,111 | INFO | 3377399906.py: 416 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-16 10:53:09,111 | INFO | 3377399906.py: 416 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-16 10:53:09,111 | INFO | 3377399906.py: 416 : <module>() ::\t Experiment Name: mawps_try1\n2024-11-16 10:53:09,118 | DEBUG | 3377399906.py: 417 : <module>() ::\t Created Relevant Directories\n2024-11-16 10:53:09,118 | DEBUG | 3377399906.py: 417 : <module>() ::\t Created Relevant Directories\n2024-11-16 10:53:09,118 | DEBUG | 3377399906.py: 417 : <module>() ::\t Created Relevant Directories\n2024-11-16 10:53:09,118 | DEBUG | 3377399906.py: 417 : <module>() ::\t Created Relevant Directories\n2024-11-16 10:53:09,118 | DEBUG | 3377399906.py: 417 : <module>() ::\t Created Relevant Directories\n2024-11-16 10:53:09,118 | DEBUG | 3377399906.py: 417 : <module>() ::\t Created Relevant Directories\n2024-11-16 10:53:09,124 | INFO | 3377399906.py: 419 : <module>() ::\t Loading Data...\n2024-11-16 10:53:09,124 | INFO | 3377399906.py: 419 : <module>() ::\t Loading Data...\n2024-11-16 10:53:09,124 | INFO | 3377399906.py: 419 : <module>() ::\t Loading Data...\n2024-11-16 10:53:09,124 | INFO | 3377399906.py: 419 : <module>() ::\t Loading Data...\n2024-11-16 10:53:09,124 | INFO | 3377399906.py: 419 : <module>() ::\t Loading Data...\n2024-11-16 10:53:09,124 | INFO | 3377399906.py: 419 : <module>() ::\t Loading Data...\n","output_type":"stream"},{"name":"stdout","text":"Transfer numbers...\n","output_type":"stream"},{"name":"stderr","text":"2024-11-16 10:53:11,529 | DEBUG | 3377399906.py: 425 : <module>() ::\t Data Loaded...\n2024-11-16 10:53:11,529 | DEBUG | 3377399906.py: 425 : <module>() ::\t Data Loaded...\n2024-11-16 10:53:11,529 | DEBUG | 3377399906.py: 425 : <module>() ::\t Data Loaded...\n2024-11-16 10:53:11,529 | DEBUG | 3377399906.py: 425 : <module>() ::\t Data Loaded...\n2024-11-16 10:53:11,529 | DEBUG | 3377399906.py: 425 : <module>() ::\t Data Loaded...\n2024-11-16 10:53:11,529 | DEBUG | 3377399906.py: 425 : <module>() ::\t Data Loaded...\n2024-11-16 10:53:11,535 | DEBUG | 3377399906.py: 427 : <module>() ::\t Number of Training Examples: 3138\n2024-11-16 10:53:11,535 | DEBUG | 3377399906.py: 427 : <module>() ::\t Number of Training Examples: 3138\n2024-11-16 10:53:11,535 | DEBUG | 3377399906.py: 427 : <module>() ::\t Number of Training Examples: 3138\n2024-11-16 10:53:11,535 | DEBUG | 3377399906.py: 427 : <module>() ::\t Number of Training Examples: 3138\n2024-11-16 10:53:11,535 | DEBUG | 3377399906.py: 427 : <module>() ::\t Number of Training Examples: 3138\n2024-11-16 10:53:11,535 | DEBUG | 3377399906.py: 427 : <module>() ::\t Number of Training Examples: 3138\n2024-11-16 10:53:11,542 | DEBUG | 3377399906.py: 428 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-16 10:53:11,542 | DEBUG | 3377399906.py: 428 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-16 10:53:11,542 | DEBUG | 3377399906.py: 428 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-16 10:53:11,542 | DEBUG | 3377399906.py: 428 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-16 10:53:11,542 | DEBUG | 3377399906.py: 428 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-16 10:53:11,542 | DEBUG | 3377399906.py: 428 : <module>() ::\t Number of Testing Examples: 1000\n2024-11-16 10:53:11,549 | DEBUG | 3377399906.py: 429 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-16 10:53:11,549 | DEBUG | 3377399906.py: 429 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-16 10:53:11,549 | DEBUG | 3377399906.py: 429 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-16 10:53:11,549 | DEBUG | 3377399906.py: 429 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-16 10:53:11,549 | DEBUG | 3377399906.py: 429 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-16 10:53:11,549 | DEBUG | 3377399906.py: 429 : <module>() ::\t Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']\n2024-11-16 10:53:11,554 | DEBUG | 3377399906.py: 430 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-16 10:53:11,554 | DEBUG | 3377399906.py: 430 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-16 10:53:11,554 | DEBUG | 3377399906.py: 430 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-16 10:53:11,554 | DEBUG | 3377399906.py: 430 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-16 10:53:11,554 | DEBUG | 3377399906.py: 430 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-16 10:53:11,554 | DEBUG | 3377399906.py: 430 : <module>() ::\t Maximum Number of Numbers: 7\n2024-11-16 10:53:11,559 | INFO | 3377399906.py: 433 : <module>() ::\t Creating Vocab...\n2024-11-16 10:53:11,559 | INFO | 3377399906.py: 433 : <module>() ::\t Creating Vocab...\n2024-11-16 10:53:11,559 | INFO | 3377399906.py: 433 : <module>() ::\t Creating Vocab...\n2024-11-16 10:53:11,559 | INFO | 3377399906.py: 433 : <module>() ::\t Creating Vocab...\n2024-11-16 10:53:11,559 | INFO | 3377399906.py: 433 : <module>() ::\t Creating Vocab...\n2024-11-16 10:53:11,559 | INFO | 3377399906.py: 433 : <module>() ::\t Creating Vocab...\n","output_type":"stream"},{"name":"stdout","text":"keep_words 4069 / 4069 = 1.0000\n","output_type":"stream"},{"name":"stderr","text":"2024-11-16 10:53:13,255 | DEBUG | 1049939482.py: 1086 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-16 10:53:13,255 | DEBUG | 1049939482.py: 1086 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-16 10:53:13,255 | DEBUG | 1049939482.py: 1086 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-16 10:53:13,255 | DEBUG | 1049939482.py: 1086 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-16 10:53:13,255 | DEBUG | 1049939482.py: 1086 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-16 10:53:13,255 | DEBUG | 1049939482.py: 1086 : prepare_data() ::\t Indexed 4072 words in input language, 21 words in output\n2024-11-16 10:53:13,358 | DEBUG | 1049939482.py: 374 : get_latest_checkpoint() ::\t Checkpoint found at : models/mawps_try1/model.pt\n2024-11-16 10:53:13,358 | DEBUG | 1049939482.py: 374 : get_latest_checkpoint() ::\t Checkpoint found at : models/mawps_try1/model.pt\n2024-11-16 10:53:13,358 | DEBUG | 1049939482.py: 374 : get_latest_checkpoint() ::\t Checkpoint found at : models/mawps_try1/model.pt\n2024-11-16 10:53:13,358 | DEBUG | 1049939482.py: 374 : get_latest_checkpoint() ::\t Checkpoint found at : models/mawps_try1/model.pt\n2024-11-16 10:53:13,358 | DEBUG | 1049939482.py: 374 : get_latest_checkpoint() ::\t Checkpoint found at : models/mawps_try1/model.pt\n2024-11-16 10:53:13,358 | DEBUG | 1049939482.py: 374 : get_latest_checkpoint() ::\t Checkpoint found at : models/mawps_try1/model.pt\n2024-11-16 10:53:13,369 | DEBUG | 3377399906.py: 456 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-16 10:53:13,369 | DEBUG | 3377399906.py: 456 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-16 10:53:13,369 | DEBUG | 3377399906.py: 456 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-16 10:53:13,369 | DEBUG | 3377399906.py: 456 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-16 10:53:13,369 | DEBUG | 3377399906.py: 456 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-16 10:53:13,369 | DEBUG | 3377399906.py: 456 : <module>() ::\t Vocab saved at models/mawps_try1/vocab1.p\n2024-11-16 10:53:13,376 | DEBUG | 3377399906.py: 468 : <module>() ::\t Config File Saved\n2024-11-16 10:53:13,376 | DEBUG | 3377399906.py: 468 : <module>() ::\t Config File Saved\n2024-11-16 10:53:13,376 | DEBUG | 3377399906.py: 468 : <module>() ::\t Config File Saved\n2024-11-16 10:53:13,376 | DEBUG | 3377399906.py: 468 : <module>() ::\t Config File Saved\n2024-11-16 10:53:13,376 | DEBUG | 3377399906.py: 468 : <module>() ::\t Config File Saved\n2024-11-16 10:53:13,376 | DEBUG | 3377399906.py: 468 : <module>() ::\t Config File Saved\n2024-11-16 10:53:13,382 | INFO | 3377399906.py: 470 : <module>() ::\t Initializing Models...\n2024-11-16 10:53:13,382 | INFO | 3377399906.py: 470 : <module>() ::\t Initializing Models...\n2024-11-16 10:53:13,382 | INFO | 3377399906.py: 470 : <module>() ::\t Initializing Models...\n2024-11-16 10:53:13,382 | INFO | 3377399906.py: 470 : <module>() ::\t Initializing Models...\n2024-11-16 10:53:13,382 | INFO | 3377399906.py: 470 : <module>() ::\t Initializing Models...\n2024-11-16 10:53:13,382 | INFO | 3377399906.py: 470 : <module>() ::\t Initializing Models...\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2024-11-16 10:53:13,924 | DEBUG | 3377399906.py: 487 : <module>() ::\t Models Initialized\n2024-11-16 10:53:13,924 | DEBUG | 3377399906.py: 487 : <module>() ::\t Models Initialized\n2024-11-16 10:53:13,924 | DEBUG | 3377399906.py: 487 : <module>() ::\t Models Initialized\n2024-11-16 10:53:13,924 | DEBUG | 3377399906.py: 487 : <module>() ::\t Models Initialized\n2024-11-16 10:53:13,924 | DEBUG | 3377399906.py: 487 : <module>() ::\t Models Initialized\n2024-11-16 10:53:13,924 | DEBUG | 3377399906.py: 487 : <module>() ::\t Models Initialized\n2024-11-16 10:53:13,931 | INFO | 3377399906.py: 488 : <module>() ::\t Initializing Optimizers...\n2024-11-16 10:53:13,931 | INFO | 3377399906.py: 488 : <module>() ::\t Initializing Optimizers...\n2024-11-16 10:53:13,931 | INFO | 3377399906.py: 488 : <module>() ::\t Initializing Optimizers...\n2024-11-16 10:53:13,931 | INFO | 3377399906.py: 488 : <module>() ::\t Initializing Optimizers...\n2024-11-16 10:53:13,931 | INFO | 3377399906.py: 488 : <module>() ::\t Initializing Optimizers...\n2024-11-16 10:53:13,931 | INFO | 3377399906.py: 488 : <module>() ::\t Initializing Optimizers...\n2024-11-16 10:53:13,939 | DEBUG | 3377399906.py: 496 : <module>() ::\t Optimizers Initialized\n2024-11-16 10:53:13,939 | DEBUG | 3377399906.py: 496 : <module>() ::\t Optimizers Initialized\n2024-11-16 10:53:13,939 | DEBUG | 3377399906.py: 496 : <module>() ::\t Optimizers Initialized\n2024-11-16 10:53:13,939 | DEBUG | 3377399906.py: 496 : <module>() ::\t Optimizers Initialized\n2024-11-16 10:53:13,939 | DEBUG | 3377399906.py: 496 : <module>() ::\t Optimizers Initialized\n2024-11-16 10:53:13,939 | DEBUG | 3377399906.py: 496 : <module>() ::\t Optimizers Initialized\n2024-11-16 10:53:13,944 | INFO | 3377399906.py: 497 : <module>() ::\t Initializing Schedulers...\n2024-11-16 10:53:13,944 | INFO | 3377399906.py: 497 : <module>() ::\t Initializing Schedulers...\n2024-11-16 10:53:13,944 | INFO | 3377399906.py: 497 : <module>() ::\t Initializing Schedulers...\n2024-11-16 10:53:13,944 | INFO | 3377399906.py: 497 : <module>() ::\t Initializing Schedulers...\n2024-11-16 10:53:13,944 | INFO | 3377399906.py: 497 : <module>() ::\t Initializing Schedulers...\n2024-11-16 10:53:13,944 | INFO | 3377399906.py: 497 : <module>() ::\t Initializing Schedulers...\n2024-11-16 10:53:13,952 | DEBUG | 3377399906.py: 505 : <module>() ::\t Schedulers Initialized\n2024-11-16 10:53:13,952 | DEBUG | 3377399906.py: 505 : <module>() ::\t Schedulers Initialized\n2024-11-16 10:53:13,952 | DEBUG | 3377399906.py: 505 : <module>() ::\t Schedulers Initialized\n2024-11-16 10:53:13,952 | DEBUG | 3377399906.py: 505 : <module>() ::\t Schedulers Initialized\n2024-11-16 10:53:13,952 | DEBUG | 3377399906.py: 505 : <module>() ::\t Schedulers Initialized\n2024-11-16 10:53:13,952 | DEBUG | 3377399906.py: 505 : <module>() ::\t Schedulers Initialized\n2024-11-16 10:53:13,957 | INFO | 3377399906.py: 507 : <module>() ::\t Loading Models on GPU 0...\n2024-11-16 10:53:13,957 | INFO | 3377399906.py: 507 : <module>() ::\t Loading Models on GPU 0...\n2024-11-16 10:53:13,957 | INFO | 3377399906.py: 507 : <module>() ::\t Loading Models on GPU 0...\n2024-11-16 10:53:13,957 | INFO | 3377399906.py: 507 : <module>() ::\t Loading Models on GPU 0...\n2024-11-16 10:53:13,957 | INFO | 3377399906.py: 507 : <module>() ::\t Loading Models on GPU 0...\n2024-11-16 10:53:13,957 | INFO | 3377399906.py: 507 : <module>() ::\t Loading Models on GPU 0...\n2024-11-16 10:53:14,158 | DEBUG | 3377399906.py: 517 : <module>() ::\t Models loaded on GPU 0\n2024-11-16 10:53:14,158 | DEBUG | 3377399906.py: 517 : <module>() ::\t Models loaded on GPU 0\n2024-11-16 10:53:14,158 | DEBUG | 3377399906.py: 517 : <module>() ::\t Models loaded on GPU 0\n2024-11-16 10:53:14,158 | DEBUG | 3377399906.py: 517 : <module>() ::\t Models loaded on GPU 0\n2024-11-16 10:53:14,158 | DEBUG | 3377399906.py: 517 : <module>() ::\t Models loaded on GPU 0\n2024-11-16 10:53:14,158 | DEBUG | 3377399906.py: 517 : <module>() ::\t Models loaded on GPU 0\n2024-11-16 10:53:14,164 | INFO | 3377399906.py: 525 : <module>() ::\t Starting Training Procedure\n2024-11-16 10:53:14,164 | INFO | 3377399906.py: 525 : <module>() ::\t Starting Training Procedure\n2024-11-16 10:53:14,164 | INFO | 3377399906.py: 525 : <module>() ::\t Starting Training Procedure\n2024-11-16 10:53:14,164 | INFO | 3377399906.py: 525 : <module>() ::\t Starting Training Procedure\n2024-11-16 10:53:14,164 | INFO | 3377399906.py: 525 : <module>() ::\t Starting Training Procedure\n2024-11-16 10:53:14,164 | INFO | 3377399906.py: 525 : <module>() ::\t Starting Training Procedure\n2024-11-16 10:53:24,907 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n2024-11-16 10:53:24,907 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n2024-11-16 10:53:24,907 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n2024-11-16 10:53:24,907 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n2024-11-16 10:53:24,907 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n2024-11-16 10:53:24,907 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 10:54:19,955 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 10:54:19,955 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 10:54:19,955 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 10:54:19,955 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 10:54:19,955 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 10:54:19,955 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 0 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 10:54:19,962 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 10:54:19,962 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 10:54:19,962 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 10:54:19,962 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 10:54:19,962 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 10:54:19,962 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 10:58:14,126 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 54s\n2024-11-16 10:58:14,126 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 54s\n2024-11-16 10:58:14,126 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 54s\n2024-11-16 10:58:14,126 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 54s\n2024-11-16 10:58:14,126 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 54s\n2024-11-16 10:58:14,126 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 54s\n2024-11-16 10:58:14,133 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 10:58:14,133 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 10:58:14,133 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 10:58:14,133 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 10:58:14,133 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 10:58:14,133 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 10:59:35,899 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 10:59:35,899 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 10:59:35,899 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 10:59:35,899 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 10:59:35,899 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 10:59:35,899 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 10:59:39,542 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.34374572231312\t\n min train loss: 1.34374572231312\t\n train acc epoch: 0.049394518801784575\t\n max train acc: 0.049394518801784575\t\n val acc epoch: 0.056\t\n equation acc epoch: 0.046\t\n max val acc: 0.056\t\n equation acc: 0.046\t\n2024-11-16 10:59:39,542 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.34374572231312\t\n min train loss: 1.34374572231312\t\n train acc epoch: 0.049394518801784575\t\n max train acc: 0.049394518801784575\t\n val acc epoch: 0.056\t\n equation acc epoch: 0.046\t\n max val acc: 0.056\t\n equation acc: 0.046\t\n2024-11-16 10:59:39,542 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.34374572231312\t\n min train loss: 1.34374572231312\t\n train acc epoch: 0.049394518801784575\t\n max train acc: 0.049394518801784575\t\n val acc epoch: 0.056\t\n equation acc epoch: 0.046\t\n max val acc: 0.056\t\n equation acc: 0.046\t\n2024-11-16 10:59:39,542 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.34374572231312\t\n min train loss: 1.34374572231312\t\n train acc epoch: 0.049394518801784575\t\n max train acc: 0.049394518801784575\t\n val acc epoch: 0.056\t\n equation acc epoch: 0.046\t\n max val acc: 0.056\t\n equation acc: 0.046\t\n2024-11-16 10:59:39,542 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.34374572231312\t\n min train loss: 1.34374572231312\t\n train acc epoch: 0.049394518801784575\t\n max train acc: 0.049394518801784575\t\n val acc epoch: 0.056\t\n equation acc epoch: 0.046\t\n max val acc: 0.056\t\n equation acc: 0.046\t\n2024-11-16 10:59:39,542 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 1\t\n best epoch: 1\t\n train loss epoch: 1.34374572231312\t\n min train loss: 1.34374572231312\t\n train acc epoch: 0.049394518801784575\t\n max train acc: 0.049394518801784575\t\n val acc epoch: 0.056\t\n equation acc epoch: 0.046\t\n max val acc: 0.056\t\n equation acc: 0.046\t\n2024-11-16 10:59:39,548 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 25s\n2024-11-16 10:59:39,548 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 25s\n2024-11-16 10:59:39,548 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 25s\n2024-11-16 10:59:39,548 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 25s\n2024-11-16 10:59:39,548 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 25s\n2024-11-16 10:59:39,548 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 25s\n2024-11-16 10:59:49,855 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n2024-11-16 10:59:49,855 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n2024-11-16 10:59:49,855 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n2024-11-16 10:59:49,855 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n2024-11-16 10:59:49,855 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n2024-11-16 10:59:49,855 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:00:45,316 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:00:45,316 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:00:45,316 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:00:45,316 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:00:45,316 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:00:45,316 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 1 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:00:45,323 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:00:45,323 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:00:45,323 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:00:45,323 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:00:45,323 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:00:45,323 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:04:04,616 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:04:04,616 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:04:04,616 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:04:04,616 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:04:04,616 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:04:04,616 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:04:04,623 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:04:04,623 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:04:04,623 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:04:04,623 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:04:04,623 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:04:04,623 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:05:23,168 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:05:23,168 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:05:23,168 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:05:23,168 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:05:23,168 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:05:23,168 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:05:27,738 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.8199572045393665\t\n min train loss: 0.8199572045393665\t\n train acc epoch: 0.5226258763543659\t\n max train acc: 0.5226258763543659\t\n val acc epoch: 0.202\t\n equation acc epoch: 0.184\t\n max val acc: 0.202\t\n equation acc: 0.184\t\n2024-11-16 11:05:27,738 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.8199572045393665\t\n min train loss: 0.8199572045393665\t\n train acc epoch: 0.5226258763543659\t\n max train acc: 0.5226258763543659\t\n val acc epoch: 0.202\t\n equation acc epoch: 0.184\t\n max val acc: 0.202\t\n equation acc: 0.184\t\n2024-11-16 11:05:27,738 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.8199572045393665\t\n min train loss: 0.8199572045393665\t\n train acc epoch: 0.5226258763543659\t\n max train acc: 0.5226258763543659\t\n val acc epoch: 0.202\t\n equation acc epoch: 0.184\t\n max val acc: 0.202\t\n equation acc: 0.184\t\n2024-11-16 11:05:27,738 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.8199572045393665\t\n min train loss: 0.8199572045393665\t\n train acc epoch: 0.5226258763543659\t\n max train acc: 0.5226258763543659\t\n val acc epoch: 0.202\t\n equation acc epoch: 0.184\t\n max val acc: 0.202\t\n equation acc: 0.184\t\n2024-11-16 11:05:27,738 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.8199572045393665\t\n min train loss: 0.8199572045393665\t\n train acc epoch: 0.5226258763543659\t\n max train acc: 0.5226258763543659\t\n val acc epoch: 0.202\t\n equation acc epoch: 0.184\t\n max val acc: 0.202\t\n equation acc: 0.184\t\n2024-11-16 11:05:27,738 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 2\t\n best epoch: 2\t\n train loss epoch: 0.8199572045393665\t\n min train loss: 0.8199572045393665\t\n train acc epoch: 0.5226258763543659\t\n max train acc: 0.5226258763543659\t\n val acc epoch: 0.202\t\n equation acc epoch: 0.184\t\n max val acc: 0.202\t\n equation acc: 0.184\t\n2024-11-16 11:05:27,744 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:05:27,744 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:05:27,744 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:05:27,744 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:05:27,744 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:05:27,744 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:05:38,081 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n2024-11-16 11:05:38,081 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n2024-11-16 11:05:38,081 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n2024-11-16 11:05:38,081 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n2024-11-16 11:05:38,081 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n2024-11-16 11:05:38,081 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:06:33,329 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:06:33,329 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:06:33,329 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:06:33,329 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:06:33,329 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:06:33,329 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 2 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:06:33,336 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:06:33,336 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:06:33,336 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:06:33,336 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:06:33,336 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:06:33,336 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:09:35,895 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 2s\n2024-11-16 11:09:35,895 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 2s\n2024-11-16 11:09:35,895 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 2s\n2024-11-16 11:09:35,895 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 2s\n2024-11-16 11:09:35,895 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 2s\n2024-11-16 11:09:35,895 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 2s\n2024-11-16 11:09:35,902 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:09:35,902 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:09:35,902 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:09:35,902 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:09:35,902 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:09:35,902 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:10:45,483 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:10:45,483 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:10:45,483 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:10:45,483 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:10:45,483 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:10:45,483 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:10:49,119 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.5284828098133357\t\n min train loss: 0.5284828098133357\t\n train acc epoch: 0.7358189929891651\t\n max train acc: 0.7358189929891651\t\n val acc epoch: 0.28\t\n equation acc epoch: 0.254\t\n max val acc: 0.28\t\n equation acc: 0.254\t\n2024-11-16 11:10:49,119 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.5284828098133357\t\n min train loss: 0.5284828098133357\t\n train acc epoch: 0.7358189929891651\t\n max train acc: 0.7358189929891651\t\n val acc epoch: 0.28\t\n equation acc epoch: 0.254\t\n max val acc: 0.28\t\n equation acc: 0.254\t\n2024-11-16 11:10:49,119 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.5284828098133357\t\n min train loss: 0.5284828098133357\t\n train acc epoch: 0.7358189929891651\t\n max train acc: 0.7358189929891651\t\n val acc epoch: 0.28\t\n equation acc epoch: 0.254\t\n max val acc: 0.28\t\n equation acc: 0.254\t\n2024-11-16 11:10:49,119 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.5284828098133357\t\n min train loss: 0.5284828098133357\t\n train acc epoch: 0.7358189929891651\t\n max train acc: 0.7358189929891651\t\n val acc epoch: 0.28\t\n equation acc epoch: 0.254\t\n max val acc: 0.28\t\n equation acc: 0.254\t\n2024-11-16 11:10:49,119 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.5284828098133357\t\n min train loss: 0.5284828098133357\t\n train acc epoch: 0.7358189929891651\t\n max train acc: 0.7358189929891651\t\n val acc epoch: 0.28\t\n equation acc epoch: 0.254\t\n max val acc: 0.28\t\n equation acc: 0.254\t\n2024-11-16 11:10:49,119 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 3\t\n best epoch: 3\t\n train loss epoch: 0.5284828098133357\t\n min train loss: 0.5284828098133357\t\n train acc epoch: 0.7358189929891651\t\n max train acc: 0.7358189929891651\t\n val acc epoch: 0.28\t\n equation acc epoch: 0.254\t\n max val acc: 0.28\t\n equation acc: 0.254\t\n2024-11-16 11:10:49,127 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:10:49,127 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:10:49,127 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:10:49,127 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:10:49,127 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:10:49,127 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:10:59,593 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n2024-11-16 11:10:59,593 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n2024-11-16 11:10:59,593 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n2024-11-16 11:10:59,593 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n2024-11-16 11:10:59,593 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n2024-11-16 11:10:59,593 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:11:54,993 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:11:54,993 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:11:54,993 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:11:54,993 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:11:54,993 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:11:54,993 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 3 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:11:55,000 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:11:55,000 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:11:55,000 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:11:55,000 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:11:55,000 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:11:55,000 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:14:54,375 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 59s\n2024-11-16 11:14:54,375 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 59s\n2024-11-16 11:14:54,375 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 59s\n2024-11-16 11:14:54,375 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 59s\n2024-11-16 11:14:54,375 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 59s\n2024-11-16 11:14:54,375 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 59s\n2024-11-16 11:14:54,381 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:14:54,381 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:14:54,381 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:14:54,381 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:14:54,381 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:14:54,381 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:16:03,864 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:16:03,864 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:16:03,864 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:16:03,864 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:16:03,864 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:16:03,864 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:16:07,723 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.3882518252640059\t\n min train loss: 0.3882518252640059\t\n train acc epoch: 0.8601019757807521\t\n max train acc: 0.8601019757807521\t\n val acc epoch: 0.33\t\n equation acc epoch: 0.301\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:16:07,723 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.3882518252640059\t\n min train loss: 0.3882518252640059\t\n train acc epoch: 0.8601019757807521\t\n max train acc: 0.8601019757807521\t\n val acc epoch: 0.33\t\n equation acc epoch: 0.301\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:16:07,723 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.3882518252640059\t\n min train loss: 0.3882518252640059\t\n train acc epoch: 0.8601019757807521\t\n max train acc: 0.8601019757807521\t\n val acc epoch: 0.33\t\n equation acc epoch: 0.301\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:16:07,723 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.3882518252640059\t\n min train loss: 0.3882518252640059\t\n train acc epoch: 0.8601019757807521\t\n max train acc: 0.8601019757807521\t\n val acc epoch: 0.33\t\n equation acc epoch: 0.301\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:16:07,723 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.3882518252640059\t\n min train loss: 0.3882518252640059\t\n train acc epoch: 0.8601019757807521\t\n max train acc: 0.8601019757807521\t\n val acc epoch: 0.33\t\n equation acc epoch: 0.301\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:16:07,723 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 4\t\n best epoch: 4\t\n train loss epoch: 0.3882518252640059\t\n min train loss: 0.3882518252640059\t\n train acc epoch: 0.8601019757807521\t\n max train acc: 0.8601019757807521\t\n val acc epoch: 0.33\t\n equation acc epoch: 0.301\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:16:07,729 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:16:07,729 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:16:07,729 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:16:07,729 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:16:07,729 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:16:07,729 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 13s\n2024-11-16 11:16:18,241 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n2024-11-16 11:16:18,241 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n2024-11-16 11:16:18,241 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n2024-11-16 11:16:18,241 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n2024-11-16 11:16:18,241 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n2024-11-16 11:16:18,241 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:17:13,299 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:17:13,299 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:17:13,299 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:17:13,299 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:17:13,299 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:17:13,299 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 4 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:17:13,305 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:17:13,305 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:17:13,305 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:17:13,305 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:17:13,305 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:17:13,305 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:20:35,903 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 22s\n2024-11-16 11:20:35,903 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 22s\n2024-11-16 11:20:35,903 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 22s\n2024-11-16 11:20:35,903 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 22s\n2024-11-16 11:20:35,903 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 22s\n2024-11-16 11:20:35,903 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 22s\n2024-11-16 11:20:35,910 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:20:35,910 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:20:35,910 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:20:35,910 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:20:35,910 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:20:35,910 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:22:00,440 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.3011274714813088\t\n min train loss: 0.3011274714813088\t\n train acc epoch: 0.8750796685787126\t\n max train acc: 0.8750796685787126\t\n val acc epoch: 0.304\t\n equation acc epoch: 0.268\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:22:00,440 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.3011274714813088\t\n min train loss: 0.3011274714813088\t\n train acc epoch: 0.8750796685787126\t\n max train acc: 0.8750796685787126\t\n val acc epoch: 0.304\t\n equation acc epoch: 0.268\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:22:00,440 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.3011274714813088\t\n min train loss: 0.3011274714813088\t\n train acc epoch: 0.8750796685787126\t\n max train acc: 0.8750796685787126\t\n val acc epoch: 0.304\t\n equation acc epoch: 0.268\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:22:00,440 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.3011274714813088\t\n min train loss: 0.3011274714813088\t\n train acc epoch: 0.8750796685787126\t\n max train acc: 0.8750796685787126\t\n val acc epoch: 0.304\t\n equation acc epoch: 0.268\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:22:00,440 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.3011274714813088\t\n min train loss: 0.3011274714813088\t\n train acc epoch: 0.8750796685787126\t\n max train acc: 0.8750796685787126\t\n val acc epoch: 0.304\t\n equation acc epoch: 0.268\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:22:00,440 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 5\t\n best epoch: 4\t\n train loss epoch: 0.3011274714813088\t\n min train loss: 0.3011274714813088\t\n train acc epoch: 0.8750796685787126\t\n max train acc: 0.8750796685787126\t\n val acc epoch: 0.304\t\n equation acc epoch: 0.268\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:22:00,446 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:22:00,446 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:22:00,446 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:22:00,446 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:22:00,446 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:22:00,446 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:22:10,959 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n2024-11-16 11:22:10,959 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n2024-11-16 11:22:10,959 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n2024-11-16 11:22:10,959 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n2024-11-16 11:22:10,959 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n2024-11-16 11:22:10,959 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:23:06,426 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:23:06,426 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:23:06,426 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:23:06,426 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:23:06,426 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:23:06,426 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 5 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:23:06,432 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:23:06,432 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:23:06,432 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:23:06,432 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:23:06,432 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:23:06,432 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:26:19,947 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 13s\n2024-11-16 11:26:19,947 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 13s\n2024-11-16 11:26:19,947 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 13s\n2024-11-16 11:26:19,947 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 13s\n2024-11-16 11:26:19,947 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 13s\n2024-11-16 11:26:19,947 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 13s\n2024-11-16 11:26:19,954 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:26:19,954 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:26:19,954 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:26:19,954 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:26:19,954 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:26:19,954 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:27:35,688 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.2599172421420614\t\n min train loss: 0.2599172421420614\t\n train acc epoch: 0.8855959209687699\t\n max train acc: 0.8855959209687699\t\n val acc epoch: 0.298\t\n equation acc epoch: 0.27\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:27:35,688 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.2599172421420614\t\n min train loss: 0.2599172421420614\t\n train acc epoch: 0.8855959209687699\t\n max train acc: 0.8855959209687699\t\n val acc epoch: 0.298\t\n equation acc epoch: 0.27\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:27:35,688 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.2599172421420614\t\n min train loss: 0.2599172421420614\t\n train acc epoch: 0.8855959209687699\t\n max train acc: 0.8855959209687699\t\n val acc epoch: 0.298\t\n equation acc epoch: 0.27\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:27:35,688 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.2599172421420614\t\n min train loss: 0.2599172421420614\t\n train acc epoch: 0.8855959209687699\t\n max train acc: 0.8855959209687699\t\n val acc epoch: 0.298\t\n equation acc epoch: 0.27\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:27:35,688 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.2599172421420614\t\n min train loss: 0.2599172421420614\t\n train acc epoch: 0.8855959209687699\t\n max train acc: 0.8855959209687699\t\n val acc epoch: 0.298\t\n equation acc epoch: 0.27\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:27:35,688 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 6\t\n best epoch: 4\t\n train loss epoch: 0.2599172421420614\t\n min train loss: 0.2599172421420614\t\n train acc epoch: 0.8855959209687699\t\n max train acc: 0.8855959209687699\t\n val acc epoch: 0.298\t\n equation acc epoch: 0.27\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:27:35,695 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 15s\n2024-11-16 11:27:35,695 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 15s\n2024-11-16 11:27:35,695 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 15s\n2024-11-16 11:27:35,695 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 15s\n2024-11-16 11:27:35,695 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 15s\n2024-11-16 11:27:35,695 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 15s\n2024-11-16 11:27:45,636 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n2024-11-16 11:27:45,636 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n2024-11-16 11:27:45,636 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n2024-11-16 11:27:45,636 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n2024-11-16 11:27:45,636 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n2024-11-16 11:27:45,636 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:28:41,143 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:28:41,143 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:28:41,143 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:28:41,143 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:28:41,143 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:28:41,143 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 6 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:28:41,149 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:28:41,149 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:28:41,149 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:28:41,149 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:28:41,149 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:28:41,149 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:32:00,439 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:32:00,439 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:32:00,439 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:32:00,439 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:32:00,439 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:32:00,439 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 19s\n2024-11-16 11:32:00,446 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:32:00,446 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:32:00,446 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:32:00,446 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:32:00,446 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:32:00,446 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:33:23,516 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n best epoch: 4\t\n train loss epoch: 0.2113770952805726\t\n min train loss: 0.2113770952805726\t\n train acc epoch: 0.9037603569152326\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.31\t\n equation acc epoch: 0.278\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:33:23,516 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n best epoch: 4\t\n train loss epoch: 0.2113770952805726\t\n min train loss: 0.2113770952805726\t\n train acc epoch: 0.9037603569152326\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.31\t\n equation acc epoch: 0.278\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:33:23,516 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n best epoch: 4\t\n train loss epoch: 0.2113770952805726\t\n min train loss: 0.2113770952805726\t\n train acc epoch: 0.9037603569152326\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.31\t\n equation acc epoch: 0.278\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:33:23,516 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n best epoch: 4\t\n train loss epoch: 0.2113770952805726\t\n min train loss: 0.2113770952805726\t\n train acc epoch: 0.9037603569152326\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.31\t\n equation acc epoch: 0.278\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:33:23,516 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n best epoch: 4\t\n train loss epoch: 0.2113770952805726\t\n min train loss: 0.2113770952805726\t\n train acc epoch: 0.9037603569152326\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.31\t\n equation acc epoch: 0.278\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:33:23,516 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 7\t\n best epoch: 4\t\n train loss epoch: 0.2113770952805726\t\n min train loss: 0.2113770952805726\t\n train acc epoch: 0.9037603569152326\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.31\t\n equation acc epoch: 0.278\t\n max val acc: 0.33\t\n equation acc: 0.301\t\n2024-11-16 11:33:23,523 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:33:23,523 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:33:23,523 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:33:23,523 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:33:23,523 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:33:23,523 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 23s\n2024-11-16 11:33:33,302 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n2024-11-16 11:33:33,302 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n2024-11-16 11:33:33,302 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n2024-11-16 11:33:33,302 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n2024-11-16 11:33:33,302 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n2024-11-16 11:33:33,302 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:34:28,413 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:34:28,413 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:34:28,413 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:34:28,413 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:34:28,413 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:34:28,413 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 7 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:34:28,419 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:34:28,419 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:34:28,419 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:34:28,419 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:34:28,419 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:34:28,419 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:37:55,751 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 27s\n2024-11-16 11:37:55,751 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 27s\n2024-11-16 11:37:55,751 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 27s\n2024-11-16 11:37:55,751 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 27s\n2024-11-16 11:37:55,751 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 27s\n2024-11-16 11:37:55,751 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 27s\n2024-11-16 11:37:55,758 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:37:55,758 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:37:55,758 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:37:55,758 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:37:55,758 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:37:55,758 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:39:14,638 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:39:14,638 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:39:14,638 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:39:14,638 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:39:14,638 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:39:14,638 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:39:18,506 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.1866040711121803\t\n min train loss: 0.1866040711121803\t\n train acc epoch: 0.8957934990439771\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.339\t\n equation acc epoch: 0.307\t\n max val acc: 0.339\t\n equation acc: 0.307\t\n2024-11-16 11:39:18,506 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.1866040711121803\t\n min train loss: 0.1866040711121803\t\n train acc epoch: 0.8957934990439771\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.339\t\n equation acc epoch: 0.307\t\n max val acc: 0.339\t\n equation acc: 0.307\t\n2024-11-16 11:39:18,506 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.1866040711121803\t\n min train loss: 0.1866040711121803\t\n train acc epoch: 0.8957934990439771\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.339\t\n equation acc epoch: 0.307\t\n max val acc: 0.339\t\n equation acc: 0.307\t\n2024-11-16 11:39:18,506 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.1866040711121803\t\n min train loss: 0.1866040711121803\t\n train acc epoch: 0.8957934990439771\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.339\t\n equation acc epoch: 0.307\t\n max val acc: 0.339\t\n equation acc: 0.307\t\n2024-11-16 11:39:18,506 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.1866040711121803\t\n min train loss: 0.1866040711121803\t\n train acc epoch: 0.8957934990439771\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.339\t\n equation acc epoch: 0.307\t\n max val acc: 0.339\t\n equation acc: 0.307\t\n2024-11-16 11:39:18,506 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 8\t\n best epoch: 8\t\n train loss epoch: 0.1866040711121803\t\n min train loss: 0.1866040711121803\t\n train acc epoch: 0.8957934990439771\t\n max train acc: 0.9037603569152326\t\n val acc epoch: 0.339\t\n equation acc epoch: 0.307\t\n max val acc: 0.339\t\n equation acc: 0.307\t\n2024-11-16 11:39:18,522 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 22s\n2024-11-16 11:39:18,522 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 22s\n2024-11-16 11:39:18,522 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 22s\n2024-11-16 11:39:18,522 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 22s\n2024-11-16 11:39:18,522 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 22s\n2024-11-16 11:39:18,522 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 22s\n2024-11-16 11:39:29,012 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n2024-11-16 11:39:29,012 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n2024-11-16 11:39:29,012 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n2024-11-16 11:39:29,012 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n2024-11-16 11:39:29,012 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n2024-11-16 11:39:29,012 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:40:24,971 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:40:24,971 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:40:24,971 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:40:24,971 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:40:24,971 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:40:24,971 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 8 completed...\nTime Taken: 0h 0m 55s\n2024-11-16 11:40:24,978 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:40:24,978 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:40:24,978 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:40:24,978 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:40:24,978 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:40:24,978 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:44:00,111 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 35s\n2024-11-16 11:44:00,111 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 35s\n2024-11-16 11:44:00,111 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 35s\n2024-11-16 11:44:00,111 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 35s\n2024-11-16 11:44:00,111 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 35s\n2024-11-16 11:44:00,111 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 3m 35s\n2024-11-16 11:44:00,116 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:44:00,116 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:44:00,116 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:44:00,116 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:44:00,116 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:44:00,116 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:45:19,092 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:45:19,092 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:45:19,092 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:45:19,092 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:45:19,092 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:45:19,092 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:45:24,240 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.15937201103026216\t\n min train loss: 0.15937201103026216\t\n train acc epoch: 0.925748884639898\t\n max train acc: 0.925748884639898\t\n val acc epoch: 0.348\t\n equation acc epoch: 0.322\t\n max val acc: 0.348\t\n equation acc: 0.322\t\n2024-11-16 11:45:24,240 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.15937201103026216\t\n min train loss: 0.15937201103026216\t\n train acc epoch: 0.925748884639898\t\n max train acc: 0.925748884639898\t\n val acc epoch: 0.348\t\n equation acc epoch: 0.322\t\n max val acc: 0.348\t\n equation acc: 0.322\t\n2024-11-16 11:45:24,240 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.15937201103026216\t\n min train loss: 0.15937201103026216\t\n train acc epoch: 0.925748884639898\t\n max train acc: 0.925748884639898\t\n val acc epoch: 0.348\t\n equation acc epoch: 0.322\t\n max val acc: 0.348\t\n equation acc: 0.322\t\n2024-11-16 11:45:24,240 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.15937201103026216\t\n min train loss: 0.15937201103026216\t\n train acc epoch: 0.925748884639898\t\n max train acc: 0.925748884639898\t\n val acc epoch: 0.348\t\n equation acc epoch: 0.322\t\n max val acc: 0.348\t\n equation acc: 0.322\t\n2024-11-16 11:45:24,240 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.15937201103026216\t\n min train loss: 0.15937201103026216\t\n train acc epoch: 0.925748884639898\t\n max train acc: 0.925748884639898\t\n val acc epoch: 0.348\t\n equation acc epoch: 0.322\t\n max val acc: 0.348\t\n equation acc: 0.322\t\n2024-11-16 11:45:24,240 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 9\t\n best epoch: 9\t\n train loss epoch: 0.15937201103026216\t\n min train loss: 0.15937201103026216\t\n train acc epoch: 0.925748884639898\t\n max train acc: 0.925748884639898\t\n val acc epoch: 0.348\t\n equation acc epoch: 0.322\t\n max val acc: 0.348\t\n equation acc: 0.322\t\n2024-11-16 11:45:24,250 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:45:24,250 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:45:24,250 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:45:24,250 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:45:24,250 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:45:24,250 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 24s\n2024-11-16 11:45:34,233 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n2024-11-16 11:45:34,233 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n2024-11-16 11:45:34,233 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n2024-11-16 11:45:34,233 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n2024-11-16 11:45:34,233 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n2024-11-16 11:45:34,233 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n","output_type":"stream"},{"name":"stdout","text":"Completed 98 / 99...\r","output_type":"stream"},{"name":"stderr","text":"2024-11-16 11:46:29,134 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 54s\n2024-11-16 11:46:29,134 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 54s\n2024-11-16 11:46:29,134 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 54s\n2024-11-16 11:46:29,134 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 54s\n2024-11-16 11:46:29,134 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 54s\n2024-11-16 11:46:29,134 | DEBUG | 3377399906.py: 551 : <module>() ::\t Training for epoch 9 completed...\nTime Taken: 0h 0m 54s\n2024-11-16 11:46:29,141 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:46:29,141 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:46:29,141 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:46:29,141 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:46:29,141 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:46:29,141 | INFO | 3377399906.py: 561 : <module>() ::\t Computing Train Accuracy\n2024-11-16 11:49:22,430 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 53s\n2024-11-16 11:49:22,430 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 53s\n2024-11-16 11:49:22,430 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 53s\n2024-11-16 11:49:22,430 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 53s\n2024-11-16 11:49:22,430 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 53s\n2024-11-16 11:49:22,430 | DEBUG | 3377399906.py: 576 : <module>() ::\t Train Accuracy Computed...\nTime Taken: 0h 2m 53s\n2024-11-16 11:49:22,436 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:49:22,436 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:49:22,436 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:49:22,436 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:49:22,436 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:49:22,436 | INFO | 3377399906.py: 578 : <module>() ::\t Starting Validation\n2024-11-16 11:50:26,719 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:50:26,719 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:50:26,719 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:50:26,719 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:50:26,719 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:50:26,719 | INFO | 1049939482.py: 291 : save_checkpoint() ::\t Saving Checkpoint at : models/mawps_try1/model.pt\n2024-11-16 11:50:30,566 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.1511093715412749\t\n min train loss: 0.1511093715412749\t\n train acc epoch: 0.9394518801784576\t\n max train acc: 0.9394518801784576\t\n val acc epoch: 0.361\t\n equation acc epoch: 0.337\t\n max val acc: 0.361\t\n equation acc: 0.337\t\n2024-11-16 11:50:30,566 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.1511093715412749\t\n min train loss: 0.1511093715412749\t\n train acc epoch: 0.9394518801784576\t\n max train acc: 0.9394518801784576\t\n val acc epoch: 0.361\t\n equation acc epoch: 0.337\t\n max val acc: 0.361\t\n equation acc: 0.337\t\n2024-11-16 11:50:30,566 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.1511093715412749\t\n min train loss: 0.1511093715412749\t\n train acc epoch: 0.9394518801784576\t\n max train acc: 0.9394518801784576\t\n val acc epoch: 0.361\t\n equation acc epoch: 0.337\t\n max val acc: 0.361\t\n equation acc: 0.337\t\n2024-11-16 11:50:30,566 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.1511093715412749\t\n min train loss: 0.1511093715412749\t\n train acc epoch: 0.9394518801784576\t\n max train acc: 0.9394518801784576\t\n val acc epoch: 0.361\t\n equation acc epoch: 0.337\t\n max val acc: 0.361\t\n equation acc: 0.337\t\n2024-11-16 11:50:30,566 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.1511093715412749\t\n min train loss: 0.1511093715412749\t\n train acc epoch: 0.9394518801784576\t\n max train acc: 0.9394518801784576\t\n val acc epoch: 0.361\t\n equation acc epoch: 0.337\t\n max val acc: 0.361\t\n equation acc: 0.337\t\n2024-11-16 11:50:30,566 | INFO | 1049939482.py: 404 : print_log() ::\t \n Epoch: 10\t\n best epoch: 10\t\n train loss epoch: 0.1511093715412749\t\n min train loss: 0.1511093715412749\t\n train acc epoch: 0.9394518801784576\t\n max train acc: 0.9394518801784576\t\n val acc epoch: 0.361\t\n equation acc epoch: 0.337\t\n max val acc: 0.361\t\n equation acc: 0.337\t\n2024-11-16 11:50:30,573 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 8s\n2024-11-16 11:50:30,573 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 8s\n2024-11-16 11:50:30,573 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 8s\n2024-11-16 11:50:30,573 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 8s\n2024-11-16 11:50:30,573 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 8s\n2024-11-16 11:50:30,573 | DEBUG | 3377399906.py: 687 : <module>() ::\t Validation Completed...\nTime Taken: 0h 1m 8s\n2024-11-16 11:50:30,580 | INFO | 3377399906.py: 691 : <module>() ::\t Scores saved at ./out/val_results_mawps-asdiv-a_svamp.json\n2024-11-16 11:50:30,580 | INFO | 3377399906.py: 691 : <module>() ::\t Scores saved at ./out/val_results_mawps-asdiv-a_svamp.json\n2024-11-16 11:50:30,580 | INFO | 3377399906.py: 691 : <module>() ::\t Scores saved at ./out/val_results_mawps-asdiv-a_svamp.json\n2024-11-16 11:50:30,580 | INFO | 3377399906.py: 691 : <module>() ::\t Scores saved at ./out/val_results_mawps-asdiv-a_svamp.json\n2024-11-16 11:50:30,580 | INFO | 3377399906.py: 691 : <module>() ::\t Scores saved at ./out/val_results_mawps-asdiv-a_svamp.json\n2024-11-16 11:50:30,580 | INFO | 3377399906.py: 691 : <module>() ::\t Scores saved at ./out/val_results_mawps-asdiv-a_svamp.json\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"print(\"here\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T11:50:30.593296Z","iopub.execute_input":"2024-11-16T11:50:30.593681Z","iopub.status.idle":"2024-11-16T11:50:30.599963Z","shell.execute_reply.started":"2024-11-16T11:50:30.593642Z","shell.execute_reply":"2024-11-16T11:50:30.598816Z"}},"outputs":[{"name":"stdout","text":"here\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"def generate_full_question(question, numbers):\n    for number in numbers:\n        if \"NUM\" in question:  # Check if 'NUM' exists in the question\n            question = question.replace(\"NUM\", str(number), 1)\n        else:\n            break  # Stop if there are no more 'NUM' placeholders\n    return question\n\n\ndef convert_eqn(equation, numbers):\n    for i, num in enumerate(numbers):\n        placeholder = f\"N{i}\"\n        equation = equation.replace(placeholder, str(num))\n    return equation\n\n\n# Function to write evaluation results into a file\ndef write_to_file(filename, data):\n    with open(filename, 'w') as f:\n        for line in data:\n            f.write(line + '\\n')\n\n# Loop over the validation data and collect output for file\noutput_lines = []\nct = 1\nfor test_batch in test_pairs:\n    batch_graph = get_single_example_graph(test_batch[0], test_batch[1], test_batch[7], test_batch[4], test_batch[5])\n    test_res = evaluate_tree(config, test_batch[0], test_batch[1], generate_num_ids, embedding, encoder, predict, generate,\n                             merge, input_lang, output_lang, test_batch[4], test_batch[5], batch_graph, test_batch[7], beam_size=config.beam_size)\n    val_ac, equ_ac, _, _ = compute_prefix_tree_result(test_res, test_batch[2], output_lang, test_batch[4], test_batch[6])\n\n    numbers = test_batch[4]\n    ques = generate_full_question(stack_to_string(sentence_from_indexes(input_lang, test_batch[0])), numbers)\n    output_lines.append(f\"Question {ct}: {ques}\")\n    true_eqn = convert_eqn(stack_to_string(sentence_from_indexes(output_lang, test_batch[2])), numbers)\n    output_lines.append(f\"True Answer: {true_eqn}\")\n    decode_eqn = convert_eqn(stack_to_string(sentence_from_indexes(output_lang, test_res)), numbers)\n    output_lines.append(f\"Decoded Answer: {decode_eqn}\")\n    \n    result_comparison = \"Correct\" if true_eqn == decode_eqn else \"Incorrect\"\n    output_lines.append(f\"Predicted Result: {result_comparison}\")\n    output_lines.append(\"-\" * 160)\n    ct += 1\n    if(ct >= 5):\n        break\n\n# Write all collected lines to eval.txt\nwrite_to_file(\"gts_eval_robert.txt\", output_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T11:50:30.601964Z","iopub.execute_input":"2024-11-16T11:50:30.602455Z","iopub.status.idle":"2024-11-16T11:50:30.845351Z","shell.execute_reply.started":"2024-11-16T11:50:30.602397Z","shell.execute_reply":"2024-11-16T11:50:30.844225Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}