{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":164555,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":139994,"modelId":162605}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"cq01/mawps-asdiv-a_svamp\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T18:34:51.913799Z","iopub.execute_input":"2024-11-12T18:34:51.914151Z","iopub.status.idle":"2024-11-12T18:34:57.719197Z","shell.execute_reply.started":"2024-11-12T18:34:51.914114Z","shell.execute_reply":"2024-11-12T18:34:57.718303Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/761 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18076c5a6fb7429c943ed3571f46e2f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-52e35a24f615aa7b.parquet:   0%|          | 0.00/527k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fe44c3957294e138c645cde575c4817"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-1d9ca8ce89c3de29.parquet:   0%|          | 0.00/107k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fec32384aa24c17b10cfbb28d719412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3138 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a227eba30a400eb3a0efcfe016c157"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7279abaa7f424ca2abecb3ef039bc9"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install --upgrade pip\n\n!pip uninstall unsloth -y\n!pip uninstall torch torchvision -y\n!pip uninstall xformers -y\n\n!pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install torch==2.4.1 torchvision==0.19.1\n!pip install --upgrade xformers==0.0.28.post1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-11-12T18:41:45.342870Z","iopub.execute_input":"2024-11-12T18:41:45.343294Z","iopub.status.idle":"2024-11-12T18:41:47.196077Z","shell.execute_reply.started":"2024-11-12T18:41:45.343246Z","shell.execute_reply":"2024-11-12T18:41:47.195112Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for key, value in ds.items():\n    print(f\"Length of '{key}': {len(value)}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T18:41:47.198229Z","iopub.execute_input":"2024-11-12T18:41:47.198546Z","iopub.status.idle":"2024-11-12T18:41:47.205166Z","shell.execute_reply.started":"2024-11-12T18:41:47.198511Z","shell.execute_reply":"2024-11-12T18:41:47.203802Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Length of 'train': 3138\nLength of 'validation': 1000\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Check if a GPU is available\nprint(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2024-11-12T18:41:47.206340Z","iopub.execute_input":"2024-11-12T18:41:47.207108Z","iopub.status.idle":"2024-11-12T18:41:47.286285Z","shell.execute_reply.started":"2024-11-12T18:41:47.207064Z","shell.execute_reply":"2024-11-12T18:41:47.285289Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_full_question(example):\n    question = example[\"Question\"]\n    numbers = example[\"Numbers\"]\n    for i, num in enumerate(numbers):\n        placeholder = f\"number{i}\"\n        # Check if the number is an integer\n        if num.is_integer():\n            num_str = str(int(num))\n        else:\n            num_str = str(num)\n        question = question.replace(placeholder, num_str)\n    example[\"Full Question\"] = question\n    return example\n\n# Apply the transformation to the train and validation sets\nds = ds.map(generate_full_question)\n\n# Now you can access the transformed dataset with the new \"Full Question\" column\nprint(ds[\"train\"][23][\"Full Question\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-12T18:41:47.287429Z","iopub.execute_input":"2024-11-12T18:41:47.287710Z","iopub.status.idle":"2024-11-12T18:41:48.135442Z","shell.execute_reply.started":"2024-11-12T18:41:47.287681Z","shell.execute_reply":"2024-11-12T18:41:48.134565Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3138 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa64906a238b4dc2a197276f98115361"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a606eb2a41884264bca2f86e818336bb"}},"metadata":{}},{"name":"stdout","text":"A construction company ordered 0.16666666666666666 ton of concrete , 0.16666666666666666 ton of bricks , and 0.5 ton of stone . How many tons of material did the company order in all ?\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ds[\"train\"][0][\"Full Question\"])\nprint(ds[\"train\"][1][\"Full Question\"])\nprint(ds[\"train\"][2][\"Full Question\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-12T18:41:48.136534Z","iopub.execute_input":"2024-11-12T18:41:48.136798Z","iopub.status.idle":"2024-11-12T18:41:48.143176Z","shell.execute_reply.started":"2024-11-12T18:41:48.136767Z","shell.execute_reply":"2024-11-12T18:41:48.141845Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Bryan took a look at his books as well . If Bryan has 56 books in each of his 9 bookshelves , how many books does he have in total ?\nFor the fifth grade play , the chairs have been put into 27 rows with 16 chairs in each row . How many chairs have been put out for the play ?\nThere are 41 short trees and 44 tall trees currently in the park . Park workers will plant 57 short trees today . How many short trees will the park have when the workers are finished ?\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# local_model_path = \"unsloth/gemma-2-9b\"\nlocal_model_path = \"/kaggle/input/gemma-finetune/other/default/1/models/mathGemma-2-9b\"\n\ntokenizer = AutoTokenizer.from_pretrained(local_model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    local_model_path,\n    device_map=\"auto\",  \n)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T18:41:48.144303Z","iopub.execute_input":"2024-11-12T18:41:48.144581Z","iopub.status.idle":"2024-11-12T19:06:36.389821Z","shell.execute_reply.started":"2024-11-12T18:41:48.144551Z","shell.execute_reply":"2024-11-12T19:06:36.388981Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f343e7325164480b438b52bb90a53a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80ee3d6345c8456eb22445bb26731089"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff099b3d846b485683f909870684f43d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9579f5692ba34aea808bd905905d550f"}},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# def extract_number_eval(text):\n#     \"\"\"Extract the last number from text using regex.\"\"\"\n#     numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)\n#     return float(numbers[-1]) if numbers else None\n\ndef extract_number_eval(text):\n    # Extract the text after 'the answer is'\n    # match = re.search(r'the answer is\\s*(.*)', text, re.IGNORECASE)\n    match = re.search(r'### Response:\\s*the answer is\\s*(.*)', text, re.IGNORECASE | re.DOTALL)\n    if match:\n        number_text = match.group(1).strip()\n        # print('Number text is:', number_text)\n        number_text = re.sub(r'[^0-9\\.\\-\\+eE]', '', number_text)\n        # print('After removing non-digit characters, number_text is:', number_text)\n        if number_text.count('.') > 1:\n            parts = number_text.split('.', 1)\n            number_text = parts[0] + '.' + parts[1].replace('.', '')\n        try:\n            number = float(number_text)\n            return number\n        except ValueError:\n            numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)\n            return float(numbers[-1]) if numbers else None\n    else:\n        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)\n        return float(numbers[-1]) if numbers else None\n\ndef evaluate_accuracy(model, tokenizer, eval_dataset, num_samples=100, results_file=\"gemini_evaluation_results.txt\"):\n    \"\"\"Evaluate model accuracy on a small subset of validation data.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n\n    # Sample random indices\n    indices = np.random.choice(len(eval_dataset), num_samples, replace=False)\n    eval_subset = eval_dataset\n\n    evaluation_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nSolve the following math problem. Provide only the numerical answer in the format: \"the answer is [number]\" without any explanation.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n\n    # Open the results file for writing\n    with open(results_file, \"w\") as file:\n        file.write(\"Evaluation Results:\\n\\n\")\n\n        print(f\"\\nEvaluating accuracy on {num_samples} examples...\")\n        progress_bar = tqdm(eval_subset, total=num_samples)\n        for example in progress_bar:\n            try:\n                # Prepare input\n                prompt = evaluation_prompt.format(question=example[\"Full Question\"])\n                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n                # Generate prediction\n                with torch.no_grad():\n                    outputs = model.generate(\n                        input_ids=inputs[\"input_ids\"],\n                        max_new_tokens=32,\n                        use_cache=False,\n                        do_sample=False,\n                    )\n\n                # Decode output\n                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n                # Extract prediction and true answer\n                predicted_number = extract_number_eval(generated_text)\n                true_answer = float(example[\"Answer\"])\n\n                # Check if correct\n                is_correct = (predicted_number is not None and abs(predicted_number - true_answer) < 1e-6)\n                correct += int(is_correct)\n                total += 1\n\n                # Store result\n                result = {\n                    \"question\": example[\"Full Question\"],\n                    \"true_answer\": true_answer,\n                    \"predicted\": predicted_number,\n                    \"correct\": is_correct\n                }\n                results_line = (f\"Question: {result['question']}\\n\"\n                                f\"True Answer: {result['true_answer']}\\n\"\n                                f\"Predicted: {result['predicted']}\\n\"\n                                f\"Correct: {result['correct']}\\n\\n\")\n\n                # Write result to file\n                file.write(results_line)\n                \n                progress_bar.set_postfix(accuracy=f\"{(correct / total) * 100:.2f}%\")\n\n            except Exception as e:\n                error_message = f\"Error processing example: {e}\\n\"\n                print(error_message)\n                file.write(error_message)\n                continue\n\n        accuracy = correct / total if total > 0 else 0\n        accuracy_message = f\"\\nAccuracy on {num_samples} samples: {accuracy:.2%}\\n\"\n        print(accuracy_message)\n        file.write(accuracy_message)\n\n    return accuracy, results_line\n\n# def evaluate_accuracy(model, tokenizer, eval_dataset, num_samples=100):\n#     \"\"\"Evaluate model accuracy on a small subset of validation data.\"\"\"\n#     model.eval()\n#     correct = 0\n#     total = 0\n    \n#     # Sample random indices\n#     indices = np.random.choice(len(eval_dataset), num_samples, replace=False)\n#     eval_subset = eval_dataset\n    \n#     evaluation_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Solve the following math problem. Provide only the numerical answer in the format: \"the answer is [number]\" without any explanation.\n\n# ### Input:\n# {question}\n\n# ### Response:\n# \"\"\"\n    \n#     results = []\n    \n#     print(f\"\\nEvaluating accuracy on {num_samples} examples...\")\n#     for example in tqdm(eval_subset):\n#         try:\n#             # Prepare input\n#             prompt = evaluation_prompt.format(question=example[\"Full Question\"])\n#             inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n            \n#             # Generate prediction\n#             with torch.no_grad():\n#                 outputs = model.generate(\n#                     # **inputs,\n#                     input_ids=inputs[\"input_ids\"],\n#                     max_new_tokens=32,\n#                     use_cache=False,\n#                     do_sample=False,\n#                 )\n            \n#             # Decode output\n#             generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n#             # Extract prediction and true answer\n#             predicted_number = extract_number_eval(generated_text)\n#             true_answer = float(example[\"Answer\"])\n            \n#             # Check if correct\n#             is_correct = (predicted_number is not None and abs(predicted_number - true_answer) < 1e-6)\n#             correct += int(is_correct)\n#             total += 1\n            \n#             # Store result\n#             results.append({\n#                 \"question\": example[\"Full Question\"],\n#                 \"true_answer\": true_answer,\n#                 \"predicted\": predicted_number,\n#                 \"correct\": is_correct\n#             })\n            \n#         except Exception as e:\n#             print(f\"Error processing example: {e}\")\n#             continue\n    \n#     accuracy = correct / total if total > 0 else 0\n#     print(f\"Accuracy on {num_samples} samples: {accuracy:.2%}\")\n    \n#     return accuracy, results","metadata":{"execution":{"iopub.status.busy":"2024-11-12T19:06:36.391250Z","iopub.execute_input":"2024-11-12T19:06:36.391805Z","iopub.status.idle":"2024-11-12T19:06:36.412487Z","shell.execute_reply.started":"2024-11-12T19:06:36.391772Z","shell.execute_reply":"2024-11-12T19:06:36.411454Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#### Random test experiment\nimport re\n\nexample = ds[\"validation\"][1]\nevaluation_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nSolve the following math problem. Provide only the numerical answer in the format: \"the answer is [number]\" without any explanation.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n\n\nprompt = evaluation_prompt.format(question=example[\"Full Question\"])\n\n# inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\ninputs = tokenizer(\n        prompt, \n        return_tensors=\"pt\", \n        padding=True,\n        truncation=True,\n        max_length=512,  # Match with max_seq_length\n).to(model.device)\n\nmodel.eval()\n\noutputs = model.generate(\n    **inputs,\n    eos_token_id=tokenizer.eos_token_id,  \n    max_new_tokens=32,\n    use_cache=False,\n    do_sample=False,\n)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\npredicted_number = extract_number_eval(generated_text)\nprint(predicted_number)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T19:06:36.416120Z","iopub.execute_input":"2024-11-12T19:06:36.416426Z","iopub.status.idle":"2024-11-12T19:06:46.722944Z","shell.execute_reply.started":"2024-11-12T19:06:36.416394Z","shell.execute_reply":"2024-11-12T19:06:46.721970Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_30/2424970749.py\", line 30, in <module>\n    outputs = model.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1971, in generate\n    generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1510, in _prepare_generation_config\n    model_kwargs = generation_config.update(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 1279, in update\n    self.validate()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 751, in validate\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 328, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.'\nArguments: (<class 'UserWarning'>,)\n","output_type":"stream"},{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nSolve the following math problem. Provide only the numerical answer in the format: \"the answer is [number]\" without any explanation.\n\n### Input:\njulia played tag with 11 kids on monday . she played tag with 12 kids on tuesday . how many more kids did she play with on tuesday than on monday ?\n\n### Response:\nthe answer is 1.0\n\n1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"print(generated_text)\npredicted_number = extract_number_eval(generated_text)\nprint(predicted_number)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T19:06:46.724211Z","iopub.execute_input":"2024-11-12T19:06:46.724564Z","iopub.status.idle":"2024-11-12T19:06:46.729472Z","shell.execute_reply.started":"2024-11-12T19:06:46.724529Z","shell.execute_reply":"2024-11-12T19:06:46.728602Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nSolve the following math problem. Provide only the numerical answer in the format: \"the answer is [number]\" without any explanation.\n\n### Input:\njulia played tag with 11 kids on monday . she played tag with 12 kids on tuesday . how many more kids did she play with on tuesday than on monday ?\n\n### Response:\nthe answer is 1.0\n\n1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"\\nEvaluating model accuracy after fine-tuning...\")\ninitial_accuracy, initial_results = evaluate_accuracy(\n    model, tokenizer, ds[\"validation\"], num_samples=len(ds[\"validation\"])\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T19:06:46.730649Z","iopub.execute_input":"2024-11-12T19:06:46.730960Z","iopub.status.idle":"2024-11-13T03:20:30.316003Z","shell.execute_reply.started":"2024-11-12T19:06:46.730928Z","shell.execute_reply":"2024-11-13T03:20:30.315016Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\nEvaluating model accuracy after fine-tuning...\n\nEvaluating accuracy on 1000 examples...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [8:13:43<00:00, 29.62s/it, accuracy=66.90%] ","output_type":"stream"},{"name":"stdout","text":"\nAccuracy on 1000 samples: 66.90%\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}